{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîÑ Apache Iceberg Schema Evolution Tutorial\n",
    "\n",
    "Welcome to the comprehensive Schema Evolution tutorial! In this notebook, you'll learn:\n",
    "\n",
    "1. **Schema Evolution Fundamentals**\n",
    "2. **Adding Columns Safely**\n",
    "3. **Data Type Promotions**\n",
    "4. **Column Removal and Renaming**\n",
    "5. **Complex Schema Changes**\n",
    "6. **Real-World Evolution Scenarios**\n",
    "7. **Compatibility Testing**\n",
    "\n",
    "## üìã Prerequisites\n",
    "\n",
    "- Docker environment is running\n",
    "- Completed Project 1 (Core Concepts)\n",
    "- Understanding of basic Iceberg concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üöÄ Initialize Environment\n",
    "\n",
    "First, let's set up Spark with Iceberg for schema evolution experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è No existing Spark session to stop\n",
      "‚úÖ Spark with Iceberg initialized for Schema Evolution!\n",
      "Spark version: 3.5.0\n",
      "Warehouse location: file:///home/jovyan/work/warehouse\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "# Set Python path for Spark consistency\n",
    "os.environ['PYSPARK_PYTHON'] = '/opt/conda/bin/python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/opt/conda/bin/python'\n",
    "\n",
    "# Stop existing Spark session if any\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"üõë Stopped existing Spark session\")\n",
    "except:\n",
    "    print(\"‚ÑπÔ∏è No existing Spark session to stop\")\n",
    "\n",
    "# Create Spark session with Iceberg for Schema Evolution\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergSchemaEvolution\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.3\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.local.warehouse\", \"file:///home/jovyan/work/warehouse\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"‚úÖ Spark with Iceberg initialized for Schema Evolution!\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Warehouse location: {spark.conf.get('spark.sql.catalog.local.warehouse')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üèóÔ∏è Schema Evolution Fundamentals\n",
    "\n",
    "Let's start with understanding the basics of schema evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created schema evolution lab database\n"
     ]
    }
   ],
   "source": [
    "# Create database for schema evolution experiments\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS local.schema_lab\")\n",
    "print(\"‚úÖ Created schema evolution lab database\")\n",
    "\n",
    "# Helper function to display schema information\n",
    "def show_schema_info(table_name):\n",
    "    print(f\"\\nüìã Schema for {table_name}:\")\n",
    "    spark.sql(f\"DESCRIBE {table_name}\").show(truncate=False)\n",
    "    \n",
    "def show_schema_history(table_name):\n",
    "    print(f\"\\nüìö Schema history for {table_name}:\")\n",
    "    try:\n",
    "        spark.sql(f\"SELECT schema_id, schema FROM {table_name}.schemas\").show(truncate=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Schema history not available: {e}\")\n",
    "\n",
    "def count_records(table_name):\n",
    "    count = spark.sql(f\"SELECT COUNT(*) as count FROM {table_name}\").collect()[0]['count']\n",
    "    print(f\"üìä Total records in {table_name}: {count}\")\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üéØ Experiment 1: Adding Columns\n",
    "\n",
    "Let's create a user table and practice adding columns safely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created initial users table\n",
      "\n",
      "üìã Schema for local.schema_lab.users:\n",
      "+--------------+----------------+-------+\n",
      "|col_name      |data_type       |comment|\n",
      "+--------------+----------------+-------+\n",
      "|user_id       |bigint          |NULL   |\n",
      "|username      |string          |NULL   |\n",
      "|email         |string          |NULL   |\n",
      "|created_at    |timestamp       |NULL   |\n",
      "|              |                |       |\n",
      "|# Partitioning|                |       |\n",
      "|Part 0        |days(created_at)|       |\n",
      "+--------------+----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean slate - drop table if exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS local.schema_lab.users\")\n",
    "\n",
    "# Create initial user table with basic schema\n",
    "create_users_sql = \"\"\"\n",
    "CREATE TABLE local.schema_lab.users (\n",
    "    user_id bigint,\n",
    "    username string,\n",
    "    email string,\n",
    "    created_at timestamp\n",
    ") USING ICEBERG\n",
    "PARTITIONED BY (days(created_at))\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(create_users_sql)\n",
    "print(\"‚úÖ Created initial users table\")\n",
    "\n",
    "# Show initial schema\n",
    "show_schema_info(\"local.schema_lab.users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Initial user data:\n",
      "+-------+-----------+-------------------+-------------------+\n",
      "|user_id|   username|              email|         created_at|\n",
      "+-------+-----------+-------------------+-------------------+\n",
      "|   1001|    alice_w|  alice@example.com|2024-01-15 10:00:00|\n",
      "|   1002|  bob_smith|    bob@example.com|2024-01-15 11:00:00|\n",
      "|   1003|charlie_dev|charlie@example.com|2024-01-16 09:00:00|\n",
      "|   1004| diana_data|  diana@example.com|2024-01-16 14:00:00|\n",
      "+-------+-----------+-------------------+-------------------+\n",
      "\n",
      "‚úÖ Inserted initial user data\n",
      "üìä Total records in local.schema_lab.users: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert initial data\n",
    "initial_users = [\n",
    "    (1001, \"alice_w\", \"alice@example.com\", datetime(2024, 1, 15, 10, 0, 0)),\n",
    "    (1002, \"bob_smith\", \"bob@example.com\", datetime(2024, 1, 15, 11, 0, 0)),\n",
    "    (1003, \"charlie_dev\", \"charlie@example.com\", datetime(2024, 1, 16, 9, 0, 0)),\n",
    "    (1004, \"diana_data\", \"diana@example.com\", datetime(2024, 1, 16, 14, 0, 0))\n",
    "]\n",
    "\n",
    "columns = [\"user_id\", \"username\", \"email\", \"created_at\"]\n",
    "users_df = spark.createDataFrame(initial_users, columns)\n",
    "\n",
    "print(\"üìä Initial user data:\")\n",
    "users_df.show()\n",
    "\n",
    "# Insert into Iceberg table\n",
    "users_df.writeTo(\"local.schema_lab.users\").append()\n",
    "print(\"‚úÖ Inserted initial user data\")\n",
    "\n",
    "# Verify data\n",
    "count_records(\"local.schema_lab.users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Single Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Evolution 1: Adding profile columns...\n",
      "‚úÖ Added first_name column\n",
      "‚úÖ Added last_name column\n",
      "‚úÖ Added phone column\n",
      "\n",
      "üìã Schema for local.schema_lab.users:\n",
      "+--------------+----------------+-------+\n",
      "|col_name      |data_type       |comment|\n",
      "+--------------+----------------+-------+\n",
      "|user_id       |bigint          |NULL   |\n",
      "|username      |string          |NULL   |\n",
      "|email         |string          |NULL   |\n",
      "|created_at    |timestamp       |NULL   |\n",
      "|first_name    |string          |NULL   |\n",
      "|last_name     |string          |NULL   |\n",
      "|phone         |string          |NULL   |\n",
      "|              |                |       |\n",
      "|# Partitioning|                |       |\n",
      "|Part 0        |days(created_at)|       |\n",
      "+--------------+----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evolution 1: Add profile information columns\n",
    "print(\"üîÑ Evolution 1: Adding profile columns...\")\n",
    "\n",
    "# Add first_name column\n",
    "spark.sql(\"ALTER TABLE local.schema_lab.users ADD COLUMN first_name string\")\n",
    "print(\"‚úÖ Added first_name column\")\n",
    "\n",
    "# Add last_name column\n",
    "spark.sql(\"ALTER TABLE local.schema_lab.users ADD COLUMN last_name string\")\n",
    "print(\"‚úÖ Added last_name column\")\n",
    "\n",
    "# Add phone column\n",
    "spark.sql(\"ALTER TABLE local.schema_lab.users ADD COLUMN phone string\")\n",
    "print(\"‚úÖ Added phone column\")\n",
    "\n",
    "# Show updated schema\n",
    "show_schema_info(\"local.schema_lab.users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Data after adding columns (notice new columns are null):\n",
      "+-------+-----------+-------------------+-------------------+----------+---------+-----+\n",
      "|user_id|username   |email              |created_at         |first_name|last_name|phone|\n",
      "+-------+-----------+-------------------+-------------------+----------+---------+-----+\n",
      "|1001   |alice_w    |alice@example.com  |2024-01-15 10:00:00|NULL      |NULL     |NULL |\n",
      "|1002   |bob_smith  |bob@example.com    |2024-01-15 11:00:00|NULL      |NULL     |NULL |\n",
      "|1003   |charlie_dev|charlie@example.com|2024-01-16 09:00:00|NULL      |NULL     |NULL |\n",
      "|1004   |diana_data |diana@example.com  |2024-01-16 14:00:00|NULL      |NULL     |NULL |\n",
      "+-------+-----------+-------------------+-------------------+----------+---------+-----+\n",
      "\n",
      "\n",
      "üìä New user data with profile information:\n",
      "+-------+-----------+-----------------+-------------------+----------+---------+-----------+\n",
      "|user_id|   username|            email|         created_at|first_name|last_name|      phone|\n",
      "+-------+-----------+-----------------+-------------------+----------+---------+-----------+\n",
      "|   1005|eve_analyst|  eve@example.com|2024-01-17 10:00:00|       Eve|  Analyst|+1-555-0105|\n",
      "|   1006|frank_admin|frank@example.com|2024-01-17 11:00:00|     Frank|    Admin|+1-555-0106|\n",
      "+-------+-----------+-----------------+-------------------+----------+---------+-----------+\n",
      "\n",
      "‚úÖ Inserted new users with profile data\n",
      "\n",
      "üìä All users after schema evolution:\n",
      "+-------+-----------+-------------------+-------------------+----------+---------+-----------+\n",
      "|user_id|username   |email              |created_at         |first_name|last_name|phone      |\n",
      "+-------+-----------+-------------------+-------------------+----------+---------+-----------+\n",
      "|1001   |alice_w    |alice@example.com  |2024-01-15 10:00:00|NULL      |NULL     |NULL       |\n",
      "|1002   |bob_smith  |bob@example.com    |2024-01-15 11:00:00|NULL      |NULL     |NULL       |\n",
      "|1003   |charlie_dev|charlie@example.com|2024-01-16 09:00:00|NULL      |NULL     |NULL       |\n",
      "|1004   |diana_data |diana@example.com  |2024-01-16 14:00:00|NULL      |NULL     |NULL       |\n",
      "|1005   |eve_analyst|eve@example.com    |2024-01-17 10:00:00|Eve       |Analyst  |+1-555-0105|\n",
      "|1006   |frank_admin|frank@example.com  |2024-01-17 11:00:00|Frank     |Admin    |+1-555-0106|\n",
      "+-------+-----------+-------------------+-------------------+----------+---------+-----------+\n",
      "\n",
      "üìä Total records in local.schema_lab.users: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that old data is still accessible and new columns are null\n",
    "print(\"üìä Data after adding columns (notice new columns are null):\")\n",
    "spark.sql(\"SELECT * FROM local.schema_lab.users ORDER BY user_id\").show(truncate=False)\n",
    "\n",
    "# Insert new data with the new columns\n",
    "new_users_data = [\n",
    "    (1005, \"eve_analyst\", \"eve@example.com\", datetime(2024, 1, 17, 10, 0, 0), \"Eve\", \"Analyst\", \"+1-555-0105\"),\n",
    "    (1006, \"frank_admin\", \"frank@example.com\", datetime(2024, 1, 17, 11, 0, 0), \"Frank\", \"Admin\", \"+1-555-0106\")\n",
    "]\n",
    "\n",
    "new_columns = [\"user_id\", \"username\", \"email\", \"created_at\", \"first_name\", \"last_name\", \"phone\"]\n",
    "new_users_df = spark.createDataFrame(new_users_data, new_columns)\n",
    "\n",
    "print(\"\\nüìä New user data with profile information:\")\n",
    "new_users_df.show()\n",
    "\n",
    "# Insert new data\n",
    "new_users_df.writeTo(\"local.schema_lab.users\").append()\n",
    "print(\"‚úÖ Inserted new users with profile data\")\n",
    "\n",
    "# Show all data\n",
    "print(\"\\nüìä All users after schema evolution:\")\n",
    "spark.sql(\"SELECT * FROM local.schema_lab.users ORDER BY user_id\").show(truncate=False)\n",
    "\n",
    "count_records(\"local.schema_lab.users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Complex Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Evolution 2: Adding complex data types...\n",
      "‚úÖ Added preferences map column\n",
      "‚úÖ Added is_premium boolean column\n",
      "‚úÖ Added tags array column\n",
      "\n",
      "üìã Schema for local.schema_lab.users:\n",
      "+--------------+------------------+-------+\n",
      "|col_name      |data_type         |comment|\n",
      "+--------------+------------------+-------+\n",
      "|user_id       |bigint            |NULL   |\n",
      "|username      |string            |NULL   |\n",
      "|email         |string            |NULL   |\n",
      "|created_at    |timestamp         |NULL   |\n",
      "|first_name    |string            |NULL   |\n",
      "|last_name     |string            |NULL   |\n",
      "|phone         |string            |NULL   |\n",
      "|preferences   |map<string,string>|NULL   |\n",
      "|is_premium    |boolean           |NULL   |\n",
      "|tags          |array<string>     |NULL   |\n",
      "|              |                  |       |\n",
      "|# Partitioning|                  |       |\n",
      "|Part 0        |days(created_at)  |       |\n",
      "+--------------+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evolution 2: Add complex data types\n",
    "print(\"üîÑ Evolution 2: Adding complex data types...\")\n",
    "\n",
    "# Add preferences as a map\n",
    "spark.sql(\"ALTER TABLE local.schema_lab.users ADD COLUMN preferences map<string,string>\")\n",
    "print(\"‚úÖ Added preferences map column\")\n",
    "\n",
    "# Add is_premium boolean with default\n",
    "spark.sql(\"ALTER TABLE local.schema_lab.users ADD COLUMN is_premium boolean\")\n",
    "print(\"‚úÖ Added is_premium boolean column\")\n",
    "\n",
    "# Add tags as array\n",
    "spark.sql(\"ALTER TABLE local.schema_lab.users ADD COLUMN tags array<string>\")\n",
    "print(\"‚úÖ Added tags array column\")\n",
    "\n",
    "# Show updated schema\n",
    "show_schema_info(\"local.schema_lab.users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Inserting users with complex data types...\n",
      "‚úÖ Inserted users with complex data types\n",
      "\n",
      "üìä Users with complex data types:\n",
      "+-------+-------------+---------------------------------------------------------+----------+------------------------------------+\n",
      "|user_id|username     |preferences                                              |is_premium|tags                                |\n",
      "+-------+-------------+---------------------------------------------------------+----------+------------------------------------+\n",
      "|1007   |grace_premium|{theme -> dark, language -> en, notifications -> enabled}|true      |[premium, early-adopter, power-user]|\n",
      "|1008   |henry_basic  |{theme -> light, language -> es}                         |false     |[new-user]                          |\n",
      "+-------+-------------+---------------------------------------------------------+----------+------------------------------------+\n",
      "\n",
      "üìä Total records in local.schema_lab.users: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert data with complex types using SQL\n",
    "print(\"üìä Inserting users with complex data types...\")\n",
    "\n",
    "complex_users_sql = \"\"\"\n",
    "INSERT INTO local.schema_lab.users VALUES\n",
    "    (1007, 'grace_premium', 'grace@example.com', TIMESTAMP '2024-01-18 09:00:00', \n",
    "     'Grace', 'Premium', '+1-555-0107', \n",
    "     map('theme', 'dark', 'language', 'en', 'notifications', 'enabled'), \n",
    "     true, \n",
    "     array('premium', 'early-adopter', 'power-user')),\n",
    "    (1008, 'henry_basic', 'henry@example.com', TIMESTAMP '2024-01-18 10:00:00', \n",
    "     'Henry', 'Basic', '+1-555-0108', \n",
    "     map('theme', 'light', 'language', 'es'), \n",
    "     false, \n",
    "     array('new-user'))\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(complex_users_sql)\n",
    "print(\"‚úÖ Inserted users with complex data types\")\n",
    "\n",
    "# Query complex data\n",
    "print(\"\\nüìä Users with complex data types:\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT user_id, username, preferences, is_premium, tags \n",
    "FROM local.schema_lab.users \n",
    "WHERE preferences IS NOT NULL\n",
    "ORDER BY user_id\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "count_records(\"local.schema_lab.users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üìä Experiment 2: Data Type Evolution\n",
    "\n",
    "Let's practice safe data type promotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created metrics table with small data types\n",
      "\n",
      "üìã Schema for local.schema_lab.metrics:\n",
      "+--------------+-----------------+-------+\n",
      "|col_name      |data_type        |comment|\n",
      "+--------------+-----------------+-------+\n",
      "|metric_id     |int              |NULL   |\n",
      "|metric_name   |string           |NULL   |\n",
      "|value         |float            |NULL   |\n",
      "|count         |int              |NULL   |\n",
      "|recorded_at   |timestamp        |NULL   |\n",
      "|              |                 |       |\n",
      "|# Partitioning|                 |       |\n",
      "|Part 0        |days(recorded_at)|       |\n",
      "+--------------+-----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a metrics table for type evolution experiments\n",
    "spark.sql(\"DROP TABLE IF EXISTS local.schema_lab.metrics\")\n",
    "\n",
    "create_metrics_sql = \"\"\"\n",
    "CREATE TABLE local.schema_lab.metrics (\n",
    "    metric_id int,\n",
    "    metric_name string,\n",
    "    value float,\n",
    "    count int,\n",
    "    recorded_at timestamp\n",
    ") USING ICEBERG\n",
    "PARTITIONED BY (days(recorded_at))\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(create_metrics_sql)\n",
    "print(\"‚úÖ Created metrics table with small data types\")\n",
    "\n",
    "show_schema_info(\"local.schema_lab.metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Initial metrics data:\n",
      "+---------+---------------+-------+-----+-------------------+\n",
      "|metric_id|    metric_name|  value|count|        recorded_at|\n",
      "+---------+---------------+-------+-----+-------------------+\n",
      "|        1|     page_views| 1234.5| 5000|2024-01-15 10:00:00|\n",
      "|        2|     click_rate|  0.045| 2250|2024-01-15 11:00:00|\n",
      "|        3|conversion_rate|  0.023|  115|2024-01-16 09:00:00|\n",
      "|        4|        revenue|15678.9|   45|2024-01-16 14:00:00|\n",
      "+---------+---------------+-------+-----+-------------------+\n",
      "\n",
      "‚úÖ Inserted initial metrics data\n",
      "üìä Total records in local.schema_lab.metrics: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert initial metrics data\n",
    "initial_metrics = [\n",
    "    (1, \"page_views\", 1234.5, 5000, datetime(2024, 1, 15, 10, 0, 0)),\n",
    "    (2, \"click_rate\", 0.045, 2250, datetime(2024, 1, 15, 11, 0, 0)),\n",
    "    (3, \"conversion_rate\", 0.023, 115, datetime(2024, 1, 16, 9, 0, 0)),\n",
    "    (4, \"revenue\", 15678.90, 45, datetime(2024, 1, 16, 14, 0, 0))\n",
    "]\n",
    "\n",
    "metrics_columns = [\"metric_id\", \"metric_name\", \"value\", \"count\", \"recorded_at\"]\n",
    "metrics_df = spark.createDataFrame(initial_metrics, metrics_columns)\n",
    "\n",
    "print(\"üìä Initial metrics data:\")\n",
    "metrics_df.show()\n",
    "\n",
    "metrics_df.writeTo(\"local.schema_lab.metrics\").append()\n",
    "print(\"‚úÖ Inserted initial metrics data\")\n",
    "\n",
    "count_records(\"local.schema_lab.metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Safe Type Promotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Performing safe type promotions...\n",
      "‚úÖ Promoted metric_id: int ‚Üí bigint\n",
      "‚úÖ Promoted value: float ‚Üí double\n",
      "‚úÖ Promoted count: int ‚Üí bigint\n",
      "\n",
      "üìã Schema for local.schema_lab.metrics:\n",
      "+--------------+-----------------+-------+\n",
      "|col_name      |data_type        |comment|\n",
      "+--------------+-----------------+-------+\n",
      "|metric_id     |bigint           |NULL   |\n",
      "|metric_name   |string           |NULL   |\n",
      "|value         |double           |NULL   |\n",
      "|count         |bigint           |NULL   |\n",
      "|recorded_at   |timestamp        |NULL   |\n",
      "|              |                 |       |\n",
      "|# Partitioning|                 |       |\n",
      "|Part 0        |days(recorded_at)|       |\n",
      "+--------------+-----------------+-------+\n",
      "\n",
      "\n",
      "üìä Data after type promotions:\n",
      "+---------+---------------+--------------------+-----+-------------------+\n",
      "|metric_id|    metric_name|               value|count|        recorded_at|\n",
      "+---------+---------------+--------------------+-----+-------------------+\n",
      "|        1|     page_views|              1234.5| 5000|2024-01-15 10:00:00|\n",
      "|        2|     click_rate| 0.04500000178813934| 2250|2024-01-15 11:00:00|\n",
      "|        3|conversion_rate|0.023000000044703484|  115|2024-01-16 09:00:00|\n",
      "|        4|        revenue|     15678.900390625|   45|2024-01-16 14:00:00|\n",
      "+---------+---------------+--------------------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evolution: Promote data types to handle larger values\n",
    "print(\"üîÑ Performing safe type promotions...\")\n",
    "\n",
    "# int ‚Üí bigint (safe promotion)\n",
    "spark.sql(\"ALTER TABLE local.schema_lab.metrics ALTER COLUMN metric_id TYPE bigint\")\n",
    "print(\"‚úÖ Promoted metric_id: int ‚Üí bigint\")\n",
    "\n",
    "# float ‚Üí double (safe promotion)\n",
    "spark.sql(\"ALTER TABLE local.schema_lab.metrics ALTER COLUMN value TYPE double\")\n",
    "print(\"‚úÖ Promoted value: float ‚Üí double\")\n",
    "\n",
    "# int ‚Üí bigint (safe promotion)\n",
    "spark.sql(\"ALTER TABLE local.schema_lab.metrics ALTER COLUMN count TYPE bigint\")\n",
    "print(\"‚úÖ Promoted count: int ‚Üí bigint\")\n",
    "\n",
    "# Show updated schema\n",
    "show_schema_info(\"local.schema_lab.metrics\")\n",
    "\n",
    "# Verify data is still accessible\n",
    "print(\"\\nüìä Data after type promotions:\")\n",
    "spark.sql(\"SELECT * FROM local.schema_lab.metrics ORDER BY metric_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Added high-precision revenue column\n",
      "‚úÖ Inserted large values that would have overflowed original types\n",
      "\n",
      "üìä All metrics including large values:\n",
      "+----------+---------------+--------------------+-------------+-------------------+---------------+\n",
      "| metric_id|    metric_name|               value|        count|        recorded_at|revenue_precise|\n",
      "+----------+---------------+--------------------+-------------+-------------------+---------------+\n",
      "|         1|     page_views|              1234.5|         5000|2024-01-15 10:00:00|           NULL|\n",
      "|         2|     click_rate| 0.04500000178813934|         2250|2024-01-15 11:00:00|           NULL|\n",
      "|         3|conversion_rate|0.023000000044703484|          115|2024-01-16 09:00:00|           NULL|\n",
      "|         4|        revenue|     15678.900390625|           45|2024-01-16 14:00:00|           NULL|\n",
      "| 999999999|   large_metric|  9.99999999999999E8| 999999999999|2024-01-17 10:00:00|  1234567890.12|\n",
      "|1000000000|    huge_metric|               1.0E9|1000000000000|2024-01-17 11:00:00|  9999999999.99|\n",
      "+----------+---------------+--------------------+-------------+-------------------+---------------+\n",
      "\n",
      "üìä Total records in local.schema_lab.metrics: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add high-precision decimal column\n",
    "spark.sql(\"ALTER TABLE local.schema_lab.metrics ADD COLUMN revenue_precise decimal(15,2)\")\n",
    "print(\"‚úÖ Added high-precision revenue column\")\n",
    "\n",
    "# Insert data that would overflow the original types\n",
    "large_metrics_sql = \"\"\"\n",
    "INSERT INTO local.schema_lab.metrics VALUES\n",
    "    (999999999, 'large_metric', 999999999.999999, 999999999999, \n",
    "     TIMESTAMP '2024-01-17 10:00:00', 1234567890.12),\n",
    "    (1000000000, 'huge_metric', 1000000000.0, 1000000000000, \n",
    "     TIMESTAMP '2024-01-17 11:00:00', 9999999999.99)\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(large_metrics_sql)\n",
    "print(\"‚úÖ Inserted large values that would have overflowed original types\")\n",
    "\n",
    "print(\"\\nüìä All metrics including large values:\")\n",
    "spark.sql(\"SELECT * FROM local.schema_lab.metrics ORDER BY metric_id\").show()\n",
    "\n",
    "count_records(\"local.schema_lab.metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üîÑ Experiment 3: Column Removal and Renaming\n",
    "\n",
    "Let's practice removing and renaming columns safely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Current users schema:\n",
      "\n",
      "üìã Schema for local.schema_lab.users:\n",
      "+--------------+------------------+-------+\n",
      "|col_name      |data_type         |comment|\n",
      "+--------------+------------------+-------+\n",
      "|user_id       |bigint            |NULL   |\n",
      "|username      |string            |NULL   |\n",
      "|email         |string            |NULL   |\n",
      "|created_at    |timestamp         |NULL   |\n",
      "|first_name    |string            |NULL   |\n",
      "|last_name     |string            |NULL   |\n",
      "|phone         |string            |NULL   |\n",
      "|preferences   |map<string,string>|NULL   |\n",
      "|is_premium    |boolean           |NULL   |\n",
      "|tags          |array<string>     |NULL   |\n",
      "|              |                  |       |\n",
      "|# Partitioning|                  |       |\n",
      "|Part 0        |days(created_at)  |       |\n",
      "+--------------+------------------+-------+\n",
      "\n",
      "\n",
      "üìä Sample data before modifications:\n",
      "+-------+-------------+----------+---------+-----------+\n",
      "|user_id|     username|first_name|last_name|      phone|\n",
      "+-------+-------------+----------+---------+-----------+\n",
      "|   1005|  eve_analyst|       Eve|  Analyst|+1-555-0105|\n",
      "|   1006|  frank_admin|     Frank|    Admin|+1-555-0106|\n",
      "|   1007|grace_premium|     Grace|  Premium|+1-555-0107|\n",
      "+-------+-------------+----------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show current users schema before modifications\n",
    "print(\"üìã Current users schema:\")\n",
    "show_schema_info(\"local.schema_lab.users\")\n",
    "\n",
    "print(\"\\nüìä Sample data before modifications:\")\n",
    "spark.sql(\"SELECT user_id, username, first_name, last_name, phone FROM local.schema_lab.users LIMIT 3\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Renaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Renaming username to display_name...\n",
      "‚úÖ Renamed username ‚Üí display_name\n",
      "\n",
      "üìã Schema for local.schema_lab.users:\n",
      "+--------------+------------------+-------+\n",
      "|col_name      |data_type         |comment|\n",
      "+--------------+------------------+-------+\n",
      "|user_id       |bigint            |NULL   |\n",
      "|display_name  |string            |NULL   |\n",
      "|email         |string            |NULL   |\n",
      "|created_at    |timestamp         |NULL   |\n",
      "|first_name    |string            |NULL   |\n",
      "|last_name     |string            |NULL   |\n",
      "|phone         |string            |NULL   |\n",
      "|preferences   |map<string,string>|NULL   |\n",
      "|is_premium    |boolean           |NULL   |\n",
      "|tags          |array<string>     |NULL   |\n",
      "|              |                  |       |\n",
      "|# Partitioning|                  |       |\n",
      "|Part 0        |days(created_at)  |       |\n",
      "+--------------+------------------+-------+\n",
      "\n",
      "\n",
      "üìä Data with renamed column:\n",
      "+-------+-------------+----------+---------+\n",
      "|user_id| display_name|first_name|last_name|\n",
      "+-------+-------------+----------+---------+\n",
      "|   1001|      alice_w|      NULL|     NULL|\n",
      "|   1002|    bob_smith|      NULL|     NULL|\n",
      "|   1007|grace_premium|     Grace|  Premium|\n",
      "+-------+-------------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename username to display_name for better semantics\n",
    "print(\"üîÑ Renaming username to display_name...\")\n",
    "spark.sql(\"ALTER TABLE local.schema_lab.users RENAME COLUMN username TO display_name\")\n",
    "print(\"‚úÖ Renamed username ‚Üí display_name\")\n",
    "\n",
    "# Show updated schema\n",
    "show_schema_info(\"local.schema_lab.users\")\n",
    "\n",
    "# Verify data is accessible with new column name\n",
    "print(\"\\nüìä Data with renamed column:\")\n",
    "spark.sql(\"SELECT user_id, display_name, first_name, last_name FROM local.schema_lab.users LIMIT 3\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Removal (Privacy Compliance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Removing phone column for privacy compliance...\n",
      "‚úÖ Removed phone column\n",
      "\n",
      "üìã Schema for local.schema_lab.users:\n",
      "+--------------+------------------+-------+\n",
      "|col_name      |data_type         |comment|\n",
      "+--------------+------------------+-------+\n",
      "|user_id       |bigint            |NULL   |\n",
      "|display_name  |string            |NULL   |\n",
      "|email         |string            |NULL   |\n",
      "|created_at    |timestamp         |NULL   |\n",
      "|first_name    |string            |NULL   |\n",
      "|last_name     |string            |NULL   |\n",
      "|preferences   |map<string,string>|NULL   |\n",
      "|is_premium    |boolean           |NULL   |\n",
      "|tags          |array<string>     |NULL   |\n",
      "|              |                  |       |\n",
      "|# Partitioning|                  |       |\n",
      "|Part 0        |days(created_at)  |       |\n",
      "+--------------+------------------+-------+\n",
      "\n",
      "\n",
      "üìä Data after removing phone column:\n",
      "+-------+-------------+-----------------+-------------------+----------+---------+---------------------------------------------------------+----------+------------------------------------+\n",
      "|user_id|display_name |email            |created_at         |first_name|last_name|preferences                                              |is_premium|tags                                |\n",
      "+-------+-------------+-----------------+-------------------+----------+---------+---------------------------------------------------------+----------+------------------------------------+\n",
      "|1005   |eve_analyst  |eve@example.com  |2024-01-17 10:00:00|Eve       |Analyst  |NULL                                                     |NULL      |NULL                                |\n",
      "|1006   |frank_admin  |frank@example.com|2024-01-17 11:00:00|Frank     |Admin    |NULL                                                     |NULL      |NULL                                |\n",
      "|1007   |grace_premium|grace@example.com|2024-01-18 09:00:00|Grace     |Premium  |{theme -> dark, language -> en, notifications -> enabled}|true      |[premium, early-adopter, power-user]|\n",
      "|1008   |henry_basic  |henry@example.com|2024-01-18 10:00:00|Henry     |Basic    |{theme -> light, language -> es}                         |false     |[new-user]                          |\n",
      "|1001   |alice_w      |alice@example.com|2024-01-15 10:00:00|NULL      |NULL     |NULL                                                     |NULL      |NULL                                |\n",
      "+-------+-------------+-----------------+-------------------+----------+---------+---------------------------------------------------------+----------+------------------------------------+\n",
      "\n",
      "\n",
      "‚úÖ Expected error when querying dropped column: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `phone` cannot be resol...\n"
     ]
    }
   ],
   "source": [
    "# Remove phone column for privacy compliance (GDPR example)\n",
    "print(\"üîÑ Removing phone column for privacy compliance...\")\n",
    "spark.sql(\"ALTER TABLE local.schema_lab.users DROP COLUMN phone\")\n",
    "print(\"‚úÖ Removed phone column\")\n",
    "\n",
    "# Show updated schema\n",
    "show_schema_info(\"local.schema_lab.users\")\n",
    "\n",
    "# Verify data is still accessible (phone column should be gone)\n",
    "print(\"\\nüìä Data after removing phone column:\")\n",
    "spark.sql(\"SELECT * FROM local.schema_lab.users LIMIT 5\").show(truncate=False)\n",
    "\n",
    "# Try to query the dropped column (should fail)\n",
    "try:\n",
    "    spark.sql(\"SELECT phone FROM local.schema_lab.users LIMIT 1\").show()\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚úÖ Expected error when querying dropped column: {str(e)[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. üèóÔ∏è Experiment 4: Complex Schema Evolution\n",
    "\n",
    "Let's create a product catalog with complex nested structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created products table with simple schema\n",
      "‚úÖ Inserted initial products\n",
      "\n",
      "üìã Schema for local.schema_lab.products:\n",
      "+-----------------------+-------------+-------+\n",
      "|col_name               |data_type    |comment|\n",
      "+-----------------------+-------------+-------+\n",
      "|product_id             |string       |NULL   |\n",
      "|name                   |string       |NULL   |\n",
      "|price                  |decimal(10,2)|NULL   |\n",
      "|category               |string       |NULL   |\n",
      "|created_at             |timestamp    |NULL   |\n",
      "|# Partition Information|             |       |\n",
      "|# col_name             |data_type    |comment|\n",
      "|category               |string       |NULL   |\n",
      "+-----------------------+-------------+-------+\n",
      "\n",
      "\n",
      "üìä Initial products:\n",
      "+----------+------------------+-------+-----------+-------------------+\n",
      "|product_id|              name|  price|   category|         created_at|\n",
      "+----------+------------------+-------+-----------+-------------------+\n",
      "|  BOOK_001|Python Programming|  49.99|      books|2024-01-15 11:00:00|\n",
      "| CHAIR_001|      Office Chair| 299.99|  furniture|2024-01-16 09:00:00|\n",
      "|LAPTOP_001|     Gaming Laptop|1299.99|electronics|2024-01-15 10:00:00|\n",
      "+----------+------------------+-------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create products table with initial simple schema\n",
    "spark.sql(\"DROP TABLE IF EXISTS local.schema_lab.products\")\n",
    "\n",
    "create_products_sql = \"\"\"\n",
    "CREATE TABLE local.schema_lab.products (\n",
    "    product_id string,\n",
    "    name string,\n",
    "    price decimal(10,2),\n",
    "    category string,\n",
    "    created_at timestamp\n",
    ") USING ICEBERG\n",
    "PARTITIONED BY (category)\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(create_products_sql)\n",
    "print(\"‚úÖ Created products table with simple schema\")\n",
    "\n",
    "# Insert initial products\n",
    "initial_products_sql = \"\"\"\n",
    "INSERT INTO local.schema_lab.products VALUES\n",
    "    ('LAPTOP_001', 'Gaming Laptop', 1299.99, 'electronics', TIMESTAMP '2024-01-15 10:00:00'),\n",
    "    ('BOOK_001', 'Python Programming', 49.99, 'books', TIMESTAMP '2024-01-15 11:00:00'),\n",
    "    ('CHAIR_001', 'Office Chair', 299.99, 'furniture', TIMESTAMP '2024-01-16 09:00:00')\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(initial_products_sql)\n",
    "print(\"‚úÖ Inserted initial products\")\n",
    "\n",
    "show_schema_info(\"local.schema_lab.products\")\n",
    "print(\"\\nüìä Initial products:\")\n",
    "spark.sql(\"SELECT * FROM local.schema_lab.products\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Nested Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Adding attributes map for flexible properties...\n",
      "‚úÖ Added attributes map\n",
      "\n",
      "üîÑ Adding product variants array...\n",
      "‚úÖ Added variants array with struct elements\n",
      "\n",
      "üîÑ Adding metadata struct...\n",
      "‚úÖ Added metadata struct\n",
      "\n",
      "üìã Schema for local.schema_lab.products:\n",
      "+-----------------------+------------------------------------------------------------------------------------+-------+\n",
      "|col_name               |data_type                                                                           |comment|\n",
      "+-----------------------+------------------------------------------------------------------------------------+-------+\n",
      "|product_id             |string                                                                              |NULL   |\n",
      "|name                   |string                                                                              |NULL   |\n",
      "|price                  |decimal(10,2)                                                                       |NULL   |\n",
      "|category               |string                                                                              |NULL   |\n",
      "|created_at             |timestamp                                                                           |NULL   |\n",
      "|attributes             |map<string,string>                                                                  |NULL   |\n",
      "|variants               |array<struct<size:string,color:string,stock:int,price:decimal(10,2)>>               |NULL   |\n",
      "|metadata               |struct<brand:string,manufacturer:string,country:string,certifications:array<string>>|NULL   |\n",
      "|# Partition Information|                                                                                    |       |\n",
      "|# col_name             |data_type                                                                           |comment|\n",
      "|category               |string                                                                              |NULL   |\n",
      "+-----------------------+------------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evolution 1: Add flexible attributes map\n",
    "print(\"üîÑ Adding attributes map for flexible properties...\")\n",
    "spark.sql(\"ALTER TABLE local.schema_lab.products ADD COLUMN attributes map<string,string>\")\n",
    "print(\"‚úÖ Added attributes map\")\n",
    "\n",
    "# Evolution 2: Add product variants array\n",
    "print(\"\\nüîÑ Adding product variants array...\")\n",
    "variants_sql = \"\"\"\n",
    "ALTER TABLE local.schema_lab.products ADD COLUMN variants array<struct<\n",
    "    size: string,\n",
    "    color: string,\n",
    "    stock: int,\n",
    "    price: decimal(10,2)\n",
    ">>\n",
    "\"\"\"\n",
    "spark.sql(variants_sql)\n",
    "print(\"‚úÖ Added variants array with struct elements\")\n",
    "\n",
    "# Evolution 3: Add metadata struct\n",
    "print(\"\\nüîÑ Adding metadata struct...\")\n",
    "metadata_sql = \"\"\"\n",
    "ALTER TABLE local.schema_lab.products ADD COLUMN metadata struct<\n",
    "    brand: string,\n",
    "    manufacturer: string,\n",
    "    country: string,\n",
    "    certifications: array<string>\n",
    ">\n",
    "\"\"\"\n",
    "spark.sql(metadata_sql)\n",
    "print(\"‚úÖ Added metadata struct\")\n",
    "\n",
    "show_schema_info(\"local.schema_lab.products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Inserted products with complex nested data\n",
      "üìä Total records in local.schema_lab.products: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert products with complex nested data\n",
    "complex_products_sql = \"\"\"\n",
    "INSERT INTO local.schema_lab.products VALUES\n",
    "    ('LAPTOP_002', 'Professional Laptop', 1899.99, 'electronics', TIMESTAMP '2024-01-17 10:00:00',\n",
    "     map('processor', 'Intel i7', 'ram', '32GB', 'storage', '1TB SSD', 'warranty', '3 years'),\n",
    "     array(\n",
    "         struct('15-inch', 'Silver', 10, 1899.99),\n",
    "         struct('15-inch', 'Black', 8, 1899.99),\n",
    "         struct('17-inch', 'Silver', 5, 2099.99)\n",
    "     ),\n",
    "     struct('TechBrand', 'TechManufacturer', 'USA', array('ISO-9001', 'Energy Star'))\n",
    "    ),\n",
    "    ('BOOK_002', 'Data Science Handbook', 79.99, 'books', TIMESTAMP '2024-01-17 11:00:00',\n",
    "     map('pages', '650', 'language', 'English', 'edition', '3rd', 'format', 'Hardcover'),\n",
    "     array(\n",
    "         struct('Hardcover', 'Blue', 25, 79.99),\n",
    "         struct('Paperback', 'Blue', 50, 59.99),\n",
    "         struct('eBook', 'N/A', 9999, 39.99)\n",
    "     ),\n",
    "     struct('EduPublisher', 'EduPrint Co', 'UK', array('Educational Standard', 'Peer Reviewed'))\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(complex_products_sql)\n",
    "print(\"‚úÖ Inserted products with complex nested data\")\n",
    "\n",
    "count_records(\"local.schema_lab.products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Products with attributes:\n",
      "+----------+--------------------+---------+--------+\n",
      "|product_id|                name|processor|warranty|\n",
      "+----------+--------------------+---------+--------+\n",
      "|  BOOK_002|Data Science Hand...|     NULL|    NULL|\n",
      "|LAPTOP_002| Professional Laptop| Intel i7| 3 years|\n",
      "+----------+--------------------+---------+--------+\n",
      "\n",
      "\n",
      "üìä Product variants (flattened):\n",
      "+----------+--------------------+--------------------+\n",
      "|product_id|                name|             variant|\n",
      "+----------+--------------------+--------------------+\n",
      "|  BOOK_002|Data Science Hand...|{Hardcover, Blue,...|\n",
      "|  BOOK_002|Data Science Hand...|{Paperback, Blue,...|\n",
      "|  BOOK_002|Data Science Hand...|{eBook, N/A, 9999...|\n",
      "|LAPTOP_002| Professional Laptop|{15-inch, Silver,...|\n",
      "|LAPTOP_002| Professional Laptop|{15-inch, Black, ...|\n",
      "|LAPTOP_002| Professional Laptop|{17-inch, Silver,...|\n",
      "+----------+--------------------+--------------------+\n",
      "\n",
      "\n",
      "üìä Product metadata:\n",
      "+----------+---------------------+------------+-------+-------------------------------------+\n",
      "|product_id|name                 |brand       |country|certifications                       |\n",
      "+----------+---------------------+------------+-------+-------------------------------------+\n",
      "|BOOK_002  |Data Science Handbook|EduPublisher|UK     |[Educational Standard, Peer Reviewed]|\n",
      "|LAPTOP_002|Professional Laptop  |TechBrand   |USA    |[ISO-9001, Energy Star]              |\n",
      "+----------+---------------------+------------+-------+-------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query complex nested data\n",
    "print(\"üìä Products with attributes:\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT product_id, name, attributes['processor'] as processor, attributes['warranty'] as warranty\n",
    "FROM local.schema_lab.products \n",
    "WHERE attributes IS NOT NULL\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"\\nüìä Product variants (flattened):\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT product_id, name, explode(variants) as variant\n",
    "FROM local.schema_lab.products \n",
    "WHERE variants IS NOT NULL\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"\\nüìä Product metadata:\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT product_id, name, metadata.brand, metadata.country, metadata.certifications\n",
    "FROM local.schema_lab.products \n",
    "WHERE metadata IS NOT NULL\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. üß™ Compatibility Testing\n",
    "\n",
    "Let's test backward and forward compatibility of our schema changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∏ Available snapshots for users table:\n",
      "+-------------------+-----------------------+---------+\n",
      "|snapshot_id        |committed_at           |operation|\n",
      "+-------------------+-----------------------+---------+\n",
      "|4728977475871491922|2025-06-15 14:07:04.463|append   |\n",
      "|5453818090795943003|2025-06-15 14:07:19.846|append   |\n",
      "|499303692362199223 |2025-06-15 14:07:43.027|append   |\n",
      "+-------------------+-----------------------+---------+\n",
      "\n",
      "\n",
      "üéØ Testing with first snapshot ID: 4728977475871491922\n"
     ]
    }
   ],
   "source": [
    "# Get snapshots to test historical compatibility\n",
    "print(\"üì∏ Available snapshots for users table:\")\n",
    "users_snapshots = spark.sql(\"SELECT snapshot_id, committed_at, operation FROM local.schema_lab.users.snapshots ORDER BY committed_at\")\n",
    "users_snapshots.show(truncate=False)\n",
    "\n",
    "# Get the first snapshot (before schema evolution)\n",
    "first_snapshot = users_snapshots.collect()[0]['snapshot_id']\n",
    "print(f\"\\nüéØ Testing with first snapshot ID: {first_snapshot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è™ Backward Compatibility Test: Reading old data with new schema\n",
      "‚úÖ Successfully read old data with current reader\n",
      "üìä Old data columns: ['user_id', 'username', 'email', 'created_at']\n",
      "üìä Old data sample:\n",
      "+-------+---------+-----------------+-------------------+\n",
      "|user_id| username|            email|         created_at|\n",
      "+-------+---------+-----------------+-------------------+\n",
      "|   1001|  alice_w|alice@example.com|2024-01-15 10:00:00|\n",
      "|   1002|bob_smith|  bob@example.com|2024-01-15 11:00:00|\n",
      "+-------+---------+-----------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "üìä Records in old snapshot: 4\n"
     ]
    }
   ],
   "source": [
    "# Test backward compatibility: read old data with new schema reader\n",
    "print(\"‚è™ Backward Compatibility Test: Reading old data with new schema\")\n",
    "\n",
    "try:\n",
    "    old_data = spark.sql(f\"SELECT * FROM local.schema_lab.users VERSION AS OF {first_snapshot}\")\n",
    "    print(\"‚úÖ Successfully read old data with current reader\")\n",
    "    print(\"üìä Old data columns:\", old_data.columns)\n",
    "    print(\"üìä Old data sample:\")\n",
    "    old_data.show(2)\n",
    "    \n",
    "    # Count records in old snapshot\n",
    "    old_count = old_data.count()\n",
    "    print(f\"üìä Records in old snapshot: {old_count}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Backward compatibility issue: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è© Forward Compatibility Test: Compare old vs new data patterns\n",
      "\n",
      "üìä Compatible query (works with both old and new schema):\n",
      "+-------+-------------------+-------------------+\n",
      "|user_id|              email|         created_at|\n",
      "+-------+-------------------+-------------------+\n",
      "|   1001|  alice@example.com|2024-01-15 10:00:00|\n",
      "|   1002|    bob@example.com|2024-01-15 11:00:00|\n",
      "|   1003|charlie@example.com|2024-01-16 09:00:00|\n",
      "|   1004|  diana@example.com|2024-01-16 14:00:00|\n",
      "+-------+-------------------+-------------------+\n",
      "\n",
      "\n",
      "üìä New schema query (only works after evolution):\n",
      "+-------+-------------+----------+---------+----------+\n",
      "|user_id| display_name|first_name|last_name|is_premium|\n",
      "+-------+-------------+----------+---------+----------+\n",
      "|   1007|grace_premium|     Grace|  Premium|      true|\n",
      "|   1008|  henry_basic|     Henry|    Basic|     false|\n",
      "+-------+-------------+----------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test forward compatibility: compare data access patterns\n",
    "print(\"‚è© Forward Compatibility Test: Compare old vs new data patterns\")\n",
    "\n",
    "# Query that would work with both old and new schema\n",
    "compatible_query = \"\"\"\n",
    "SELECT user_id, email, created_at \n",
    "FROM local.schema_lab.users \n",
    "WHERE user_id < 1005\n",
    "ORDER BY user_id\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nüìä Compatible query (works with both old and new schema):\")\n",
    "spark.sql(compatible_query).show()\n",
    "\n",
    "# Query that only works with new schema\n",
    "new_schema_query = \"\"\"\n",
    "SELECT user_id, display_name, first_name, last_name, is_premium \n",
    "FROM local.schema_lab.users \n",
    "WHERE is_premium IS NOT NULL\n",
    "ORDER BY user_id\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nüìä New schema query (only works after evolution):\")\n",
    "spark.sql(new_schema_query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. üìä Performance Analysis\n",
    "\n",
    "Let's analyze the performance impact of schema evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Performance Analysis of Schema Evolution\n",
      "\n",
      "üìÅ Table files analysis:\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+-----------+------------+------------------+------------+\n",
      "|file_path                                                                                                                                   |file_format|record_count|file_size_in_bytes|file_size_kb|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+-----------+------------+------------------+------------+\n",
      "|file:/home/jovyan/work/warehouse/schema_lab/users/data/created_at_day=2024-01-18/00000-66-5785fe72-62cc-40a8-8f24-e5f25e2448d7-00001.parquet|PARQUET    |2           |3405              |3.33        |\n",
      "|file:/home/jovyan/work/warehouse/schema_lab/users/data/created_at_day=2024-01-17/00000-60-dad020b9-3266-4ab8-b193-36c5d9434310-00001.parquet|PARQUET    |2           |2116              |2.07        |\n",
      "|file:/home/jovyan/work/warehouse/schema_lab/users/data/created_at_day=2024-01-16/00000-28-78bdf321-74f3-485c-a705-c4246a4ad96f-00002.parquet|PARQUET    |2           |1350              |1.32        |\n",
      "|file:/home/jovyan/work/warehouse/schema_lab/users/data/created_at_day=2024-01-15/00000-28-78bdf321-74f3-485c-a705-c4246a4ad96f-00001.parquet|PARQUET    |2           |1319              |1.29        |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+-----------+------------+------------------+------------+\n",
      "\n",
      "\n",
      "üìä Storage summary:\n",
      "+-----------+-------------+----------------+-------------------+-------------+\n",
      "|total_files|total_records|total_size_bytes|avg_file_size_bytes|total_size_kb|\n",
      "+-----------+-------------+----------------+-------------------+-------------+\n",
      "|          4|            8|            8190|             2047.5|         8.00|\n",
      "+-----------+-------------+----------------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyze storage and performance impact\n",
    "print(\"üìà Performance Analysis of Schema Evolution\")\n",
    "\n",
    "# Check table files and sizes\n",
    "print(\"\\nüìÅ Table files analysis:\")\n",
    "files_info = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    file_path,\n",
    "    file_format,\n",
    "    record_count,\n",
    "    file_size_in_bytes,\n",
    "    ROUND(file_size_in_bytes / 1024.0, 2) as file_size_kb\n",
    "FROM local.schema_lab.users.files\n",
    "ORDER BY file_size_in_bytes DESC\n",
    "\"\"\")\n",
    "files_info.show(truncate=False)\n",
    "\n",
    "# Summary statistics\n",
    "summary_stats = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as total_files,\n",
    "    SUM(record_count) as total_records,\n",
    "    SUM(file_size_in_bytes) as total_size_bytes,\n",
    "    ROUND(AVG(file_size_in_bytes), 2) as avg_file_size_bytes,\n",
    "    ROUND(SUM(file_size_in_bytes) / 1024.0, 2) as total_size_kb\n",
    "FROM local.schema_lab.users.files\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüìä Storage summary:\")\n",
    "summary_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Query Performance Testing\n",
      "\n",
      "üéØ Testing column projection performance:\n",
      "Query 1 - Basic columns:\n",
      "Result count: 8\n",
      "\n",
      "Query 2 - All columns:\n",
      "Result count: 8\n",
      "\n",
      "Query 3 - Complex operations with null handling:\n",
      "+-------+-------------+----------------+--------------+---------+\n",
      "|user_id| display_name|first_name_clean|premium_status|tag_count|\n",
      "+-------+-------------+----------------+--------------+---------+\n",
      "|   1001|      alice_w|         Unknown|         false|        0|\n",
      "|   1002|    bob_smith|         Unknown|         false|        0|\n",
      "|   1003|  charlie_dev|         Unknown|         false|        0|\n",
      "|   1004|   diana_data|         Unknown|         false|        0|\n",
      "|   1005|  eve_analyst|             Eve|         false|        0|\n",
      "|   1006|  frank_admin|           Frank|         false|        0|\n",
      "|   1007|grace_premium|           Grace|          true|        3|\n",
      "|   1008|  henry_basic|           Henry|         false|        1|\n",
      "+-------+-------------+----------------+--------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test query performance with schema projection\n",
    "print(\"‚ö° Query Performance Testing\")\n",
    "\n",
    "# Query with column projection (only select needed columns)\n",
    "print(\"\\nüéØ Testing column projection performance:\")\n",
    "\n",
    "# Query 1: Select only basic columns (should be fast)\n",
    "basic_query = \"SELECT user_id, display_name, email FROM local.schema_lab.users\"\n",
    "print(\"Query 1 - Basic columns:\")\n",
    "result1 = spark.sql(basic_query)\n",
    "print(f\"Result count: {result1.count()}\")\n",
    "\n",
    "# Query 2: Select all columns (might be slower)\n",
    "all_columns_query = \"SELECT * FROM local.schema_lab.users\"\n",
    "print(\"\\nQuery 2 - All columns:\")\n",
    "result2 = spark.sql(all_columns_query)\n",
    "print(f\"Result count: {result2.count()}\")\n",
    "\n",
    "# Query 3: Complex column operations\n",
    "complex_query = \"\"\"\n",
    "SELECT \n",
    "    user_id,\n",
    "    display_name,\n",
    "    COALESCE(first_name, 'Unknown') as first_name_clean,\n",
    "    COALESCE(is_premium, false) as premium_status,\n",
    "    size(COALESCE(tags, array())) as tag_count\n",
    "FROM local.schema_lab.users\n",
    "\"\"\"\n",
    "print(\"\\nQuery 3 - Complex operations with null handling:\")\n",
    "result3 = spark.sql(complex_query)\n",
    "result3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. üéØ Schema Evolution Best Practices\n",
    "\n",
    "Let's demonstrate schema evolution best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Best Practice 1: Add nullable columns\n",
      "   - New columns are automatically nullable\n",
      "   - Historical data gets null values\n",
      "   - No data rewriting required\n",
      "\n",
      "‚úÖ Best Practice 2: Safe type promotions only\n",
      "   - int ‚Üí bigint ‚úÖ\n",
      "   - float ‚Üí double ‚úÖ\n",
      "   - decimal(p,s) ‚Üí decimal(p',s) where p' > p ‚úÖ\n",
      "   - string ‚Üí string (no change needed) ‚úÖ\n",
      "\n",
      "‚ùå Avoid these operations:\n",
      "   - bigint ‚Üí int (data loss possible)\n",
      "   - double ‚Üí float (precision loss)\n",
      "   - Adding required (non-null) columns\n",
      "   - Changing column semantics without renaming\n"
     ]
    }
   ],
   "source": [
    "# Best Practice 1: Always add columns as nullable initially\n",
    "print(\"‚úÖ Best Practice 1: Add nullable columns\")\n",
    "print(\"   - New columns are automatically nullable\")\n",
    "print(\"   - Historical data gets null values\")\n",
    "print(\"   - No data rewriting required\")\n",
    "\n",
    "# Best Practice 2: Use safe type promotions\n",
    "print(\"\\n‚úÖ Best Practice 2: Safe type promotions only\")\n",
    "safe_promotions = [\n",
    "    \"int ‚Üí bigint ‚úÖ\",\n",
    "    \"float ‚Üí double ‚úÖ\",\n",
    "    \"decimal(p,s) ‚Üí decimal(p',s) where p' > p ‚úÖ\",\n",
    "    \"string ‚Üí string (no change needed) ‚úÖ\"\n",
    "]\n",
    "for promotion in safe_promotions:\n",
    "    print(f\"   - {promotion}\")\n",
    "\n",
    "# Best Practice 3: Avoid these dangerous operations\n",
    "print(\"\\n‚ùå Avoid these operations:\")\n",
    "dangerous_ops = [\n",
    "    \"bigint ‚Üí int (data loss possible)\",\n",
    "    \"double ‚Üí float (precision loss)\",\n",
    "    \"Adding required (non-null) columns\",\n",
    "    \"Changing column semantics without renaming\"\n",
    "]\n",
    "for op in dangerous_ops:\n",
    "    print(f\"   - {op}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Validating schema evolution for local.schema_lab.users\n",
      "‚úÖ Current data accessible: 8 records\n",
      "‚úÖ Schema history available: 3 snapshots\n",
      "‚úÖ Historical data accessible: 4 records in first snapshot\n",
      "‚úÖ Metadata consistency: 4 data files tracked\n",
      "\n",
      "üéâ All validation tests passed for local.schema_lab.users!\n",
      "\n",
      "==================================================\n",
      "\n",
      "üîç Validating schema evolution for local.schema_lab.products\n",
      "‚úÖ Current data accessible: 5 records\n",
      "‚úÖ Schema history available: 2 snapshots\n",
      "‚úÖ Historical data accessible: 3 records in first snapshot\n",
      "‚úÖ Metadata consistency: 5 data files tracked\n",
      "\n",
      "üéâ All validation tests passed for local.schema_lab.products!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation function for schema evolution\n",
    "def validate_schema_evolution(table_name):\n",
    "    \"\"\"Comprehensive validation of schema evolution\"\"\"\n",
    "    print(f\"üîç Validating schema evolution for {table_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Test 1: Current data accessibility\n",
    "        current_count = spark.sql(f\"SELECT COUNT(*) as count FROM {table_name}\").collect()[0]['count']\n",
    "        print(f\"‚úÖ Current data accessible: {current_count} records\")\n",
    "        \n",
    "        # Test 2: Schema history availability\n",
    "        snapshots = spark.sql(f\"SELECT COUNT(*) as count FROM {table_name}.snapshots\").collect()[0]['count']\n",
    "        print(f\"‚úÖ Schema history available: {snapshots} snapshots\")\n",
    "        \n",
    "        # Test 3: Historical data accessibility\n",
    "        first_snapshot = spark.sql(f\"SELECT snapshot_id FROM {table_name}.snapshots ORDER BY committed_at LIMIT 1\").collect()[0]['snapshot_id']\n",
    "        historical_count = spark.sql(f\"SELECT COUNT(*) as count FROM {table_name} VERSION AS OF {first_snapshot}\").collect()[0]['count']\n",
    "        print(f\"‚úÖ Historical data accessible: {historical_count} records in first snapshot\")\n",
    "        \n",
    "        # Test 4: Metadata consistency\n",
    "        files_count = spark.sql(f\"SELECT COUNT(*) as count FROM {table_name}.files\").collect()[0]['count']\n",
    "        print(f\"‚úÖ Metadata consistency: {files_count} data files tracked\")\n",
    "        \n",
    "        print(f\"\\nüéâ All validation tests passed for {table_name}!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Validation failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Validate our evolved tables\n",
    "validate_schema_evolution(\"local.schema_lab.users\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "validate_schema_evolution(\"local.schema_lab.products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. üéâ Summary and Next Steps\n",
    "\n",
    "Congratulations! You've completed the Schema Evolution tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ Schema Evolution Tutorial Complete!\n",
      "\n",
      "‚úÖ What You've Mastered:\n",
      "   1. Added columns without data rewriting\n",
      "   2. Performed safe data type promotions\n",
      "   3. Renamed columns with backward compatibility\n",
      "   4. Removed columns for privacy compliance\n",
      "   5. Created complex nested data structures\n",
      "   6. Tested backward and forward compatibility\n",
      "   7. Analyzed performance impact of schema changes\n",
      "   8. Implemented schema evolution best practices\n",
      "\n",
      "üìä Final Statistics:\n",
      "   üìã users: 8 records, 3 snapshots\n",
      "   üìã metrics: 6 records, 2 snapshots\n",
      "   üìã products: 5 records, 2 snapshots\n",
      "\n",
      "üöÄ Next Steps:\n",
      "   ‚Üí Explore Project 3: Time Travel Features\n",
      "   ‚Üí Practice schema evolution in production scenarios\n",
      "   ‚Üí Learn about schema registries and governance\n",
      "   ‚Üí Study advanced Iceberg features\n",
      "\n",
      "üéØ Key Takeaways:\n",
      "   üí° Schema evolution in Iceberg is safe and efficient\n",
      "   üí° Historical data remains accessible after schema changes\n",
      "   üí° Complex data types enable flexible data modeling\n",
      "   üí° Proper validation ensures data integrity\n"
     ]
    }
   ],
   "source": [
    "# Final summary of what we accomplished\n",
    "print(\"üéâ Schema Evolution Tutorial Complete!\")\n",
    "print(\"\\n‚úÖ What You've Mastered:\")\n",
    "\n",
    "accomplishments = [\n",
    "    \"Added columns without data rewriting\",\n",
    "    \"Performed safe data type promotions\",\n",
    "    \"Renamed columns with backward compatibility\",\n",
    "    \"Removed columns for privacy compliance\",\n",
    "    \"Created complex nested data structures\",\n",
    "    \"Tested backward and forward compatibility\",\n",
    "    \"Analyzed performance impact of schema changes\",\n",
    "    \"Implemented schema evolution best practices\"\n",
    "]\n",
    "\n",
    "for i, accomplishment in enumerate(accomplishments, 1):\n",
    "    print(f\"   {i}. {accomplishment}\")\n",
    "\n",
    "print(\"\\nüìä Final Statistics:\")\n",
    "tables_created = ['users', 'metrics', 'products']\n",
    "for table in tables_created:\n",
    "    try:\n",
    "        count = spark.sql(f\"SELECT COUNT(*) as count FROM local.schema_lab.{table}\").collect()[0]['count']\n",
    "        snapshots = spark.sql(f\"SELECT COUNT(*) as count FROM local.schema_lab.{table}.snapshots\").collect()[0]['count']\n",
    "        print(f\"   üìã {table}: {count} records, {snapshots} snapshots\")\n",
    "    except:\n",
    "        print(f\"   üìã {table}: Not accessible\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "next_steps = [\n",
    "    \"Explore Project 3: Time Travel Features\",\n",
    "    \"Practice schema evolution in production scenarios\",\n",
    "    \"Learn about schema registries and governance\",\n",
    "    \"Study advanced Iceberg features\"\n",
    "]\n",
    "\n",
    "for step in next_steps:\n",
    "    print(f\"   ‚Üí {step}\")\n",
    "\n",
    "print(\"\\nüéØ Key Takeaways:\")\n",
    "takeaways = [\n",
    "    \"Schema evolution in Iceberg is safe and efficient\",\n",
    "    \"Historical data remains accessible after schema changes\",\n",
    "    \"Complex data types enable flexible data modeling\",\n",
    "    \"Proper validation ensures data integrity\"\n",
    "]\n",
    "\n",
    "for takeaway in takeaways:\n",
    "    print(f\"   üí° {takeaway}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
