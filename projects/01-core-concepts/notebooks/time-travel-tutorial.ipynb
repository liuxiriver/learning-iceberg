{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚è∞ Apache Iceberg Time Travel Tutorial\n",
    "\n",
    "Welcome to the comprehensive Time Travel tutorial! In this notebook, you'll learn:\n",
    "\n",
    "1. **Time Travel Fundamentals**\n",
    "2. **Reading Historical Data**\n",
    "3. **Snapshot Management**\n",
    "4. **Rollback Operations**\n",
    "5. **Schema Evolution with Time Travel**\n",
    "6. **Performance Considerations**\n",
    "7. **Real-world Use Cases**\n",
    "\n",
    "## üìã Prerequisites\n",
    "\n",
    "- Completed the basic Iceberg tutorial\n",
    "- Understanding of Iceberg table concepts\n",
    "- Basic knowledge of Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üöÄ Initialize Environment\n",
    "\n",
    "Set up Spark with Iceberg for time travel operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è No existing Spark session to stop\n",
      "‚úÖ Spark with Iceberg initialized successfully!\n",
      "Spark version: 3.5.0\n",
      "Python path: /opt/conda/bin/python\n",
      "Configured warehouse location: file:///home/jovyan/work/warehouse\n",
      "‚è∞ Time Travel Tutorial Environment Ready!\n",
      "üìÖ Current time: 2025-06-15 23:52:25\n",
      "‚úÖ Spark session initialized with Iceberg support\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "# Set Python path for Spark to ensure consistent Python version\n",
    "os.environ['PYSPARK_PYTHON'] = '/opt/conda/bin/python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/opt/conda/bin/python'\n",
    "\n",
    "# Stop existing Spark session if any\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"üõë Stopped existing Spark session\")\n",
    "except:\n",
    "    print(\"‚ÑπÔ∏è No existing Spark session to stop\")\n",
    "\n",
    "# Create Spark session with Iceberg and correct warehouse path\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergTutorial\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.3\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.local.warehouse\", \"file:///home/jovyan/work/warehouse\") \\\n",
    "    .config(\"spark.pyspark.python\", \"/opt/conda/bin/python\") \\\n",
    "    .config(\"spark.pyspark.driver.python\", \"/opt/conda/bin/python\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"‚úÖ Spark with Iceberg initialized successfully!\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Python path: {os.environ.get('PYSPARK_PYTHON', 'Not set')}\")\n",
    "\n",
    "# Verify the warehouse configuration\n",
    "warehouse_path = spark.conf.get(\"spark.sql.catalog.local.warehouse\")\n",
    "print(f\"Configured warehouse location: {warehouse_path}\")\n",
    "\n",
    "\n",
    "print(\"‚è∞ Time Travel Tutorial Environment Ready!\")\n",
    "print(f\"üìÖ Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"‚úÖ Spark session initialized with Iceberg support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üóÑÔ∏è Create Sample Data for Time Travel\n",
    "\n",
    "Create a sample table and insert data over time to demonstrate time travel capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Database 'local.time_travel_lab' created!\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create database\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS local.time_travel_lab\")\n",
    "print(\"‚úÖ Database 'local.time_travel_lab' created!\")\n",
    "\n",
    "# Show available databases\n",
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è Dropped existing table (if any)\n",
      "‚úÖ Created local.time_travel_lab.customer_orders table\n",
      "üìä Table schema:\n",
      "+--------------+----------------+-------+\n",
      "|      col_name|       data_type|comment|\n",
      "+--------------+----------------+-------+\n",
      "|      order_id|          bigint|   NULL|\n",
      "|   customer_id|          bigint|   NULL|\n",
      "|  product_name|          string|   NULL|\n",
      "|      quantity|             int|   NULL|\n",
      "|    unit_price|   decimal(10,2)|   NULL|\n",
      "|    order_date|            date|   NULL|\n",
      "|        status|          string|   NULL|\n",
      "|              |                |       |\n",
      "|# Partitioning|                |       |\n",
      "|        Part 0|days(order_date)|       |\n",
      "+--------------+----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop the table if it exists to avoid path conflicts\n",
    "try:\n",
    "    spark.sql(\"DROP TABLE IF EXISTS local.time_travel_lab.customer_orders\")\n",
    "    print(\"üóëÔ∏è Dropped existing table (if any)\")\n",
    "except:\n",
    "    print(\"‚ÑπÔ∏è No existing table to drop\")\n",
    "\n",
    "\n",
    "# Create sample table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE local.time_travel_lab.customer_orders (\n",
    "    order_id bigint,\n",
    "    customer_id bigint,\n",
    "    product_name string,\n",
    "    quantity int,\n",
    "    unit_price decimal(10,2),\n",
    "    order_date date,\n",
    "    status string\n",
    ") USING ICEBERG\n",
    "PARTITIONED BY (days(order_date))\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Created local.time_travel_lab.customer_orders table\")\n",
    "print(\"üìä Table schema:\")\n",
    "spark.sql(\"DESCRIBE local.time_travel_lab.customer_orders\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Day 1: Initial orders\n",
      "üì∏ Snapshot 1 ID: 319832172894815909\n",
      "üìÖ Snapshot 1 Time: 2025-06-15 23:54:37.954000\n",
      "\n",
      "üìä Current data:\n",
      "+--------+-----------+--------------+--------+----------+----------+-------+\n",
      "|order_id|customer_id|  product_name|quantity|unit_price|order_date| status|\n",
      "+--------+-----------+--------------+--------+----------+----------+-------+\n",
      "|    1001|        101|    Laptop Pro|       1|   1299.99|2024-01-15|pending|\n",
      "|    1002|        102|Wireless Mouse|       2|     29.99|2024-01-15|shipped|\n",
      "|    1003|        103|      Keyboard|       1|     89.99|2024-01-15|pending|\n",
      "+--------+-----------+--------------+--------+----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Insert initial data - Day 1\n",
    "print(\"üìÖ Day 1: Initial orders\")\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO local.time_travel_lab.customer_orders VALUES\n",
    "    (1001, 101, 'Laptop Pro', 1, 1299.99, DATE '2024-01-15', 'pending'),\n",
    "    (1002, 102, 'Wireless Mouse', 2, 29.99, DATE '2024-01-15', 'shipped'),\n",
    "    (1003, 103, 'Keyboard', 1, 89.99, DATE '2024-01-15', 'pending')\n",
    "\"\"\")\n",
    "\n",
    "# Store first snapshot info\n",
    "first_snapshot = spark.sql(\"SELECT snapshot_id, committed_at FROM local.time_travel_lab.customer_orders.snapshots ORDER BY committed_at LIMIT 1\").collect()[0]\n",
    "print(f\"üì∏ Snapshot 1 ID: {first_snapshot['snapshot_id']}\")\n",
    "print(f\"üìÖ Snapshot 1 Time: {first_snapshot['committed_at']}\")\n",
    "\n",
    "print(\"\\nüìä Current data:\")\n",
    "spark.sql(\"SELECT * FROM local.time_travel_lab.customer_orders ORDER BY order_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Day 2: Order status updates\n",
      "üì∏ Snapshot 2 ID: 3751811866144034316\n",
      "üìÖ Snapshot 2 Time: 2025-06-15 23:55:37.838000\n",
      "\n",
      "üìä Updated data:\n",
      "+--------+-----------+--------------+--------+----------+----------+---------+\n",
      "|order_id|customer_id|  product_name|quantity|unit_price|order_date|   status|\n",
      "+--------+-----------+--------------+--------+----------+----------+---------+\n",
      "|    1001|        101|    Laptop Pro|       1|   1299.99|2024-01-15|  shipped|\n",
      "|    1002|        102|Wireless Mouse|       2|     29.99|2024-01-15|delivered|\n",
      "|    1003|        103|      Keyboard|       1|     89.99|2024-01-15|  pending|\n",
      "+--------+-----------+--------------+--------+----------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Add a small delay to ensure different timestamps\n",
    "# time.sleep(2)\n",
    "\n",
    "# Update some orders - Day 2\n",
    "print(\"üìÖ Day 2: Order status updates\")\n",
    "spark.sql(\"\"\"\n",
    "UPDATE local.time_travel_lab.customer_orders \n",
    "SET status = 'shipped' \n",
    "WHERE order_id = 1001\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "UPDATE local.time_travel_lab.customer_orders \n",
    "SET status = 'delivered' \n",
    "WHERE order_id = 1002\n",
    "\"\"\")\n",
    "\n",
    "# Store second snapshot info\n",
    "second_snapshot = spark.sql(\"SELECT snapshot_id, committed_at FROM local.time_travel_lab.customer_orders.snapshots ORDER BY committed_at DESC LIMIT 1\").collect()[0]\n",
    "print(f\"üì∏ Snapshot 2 ID: {second_snapshot['snapshot_id']}\")\n",
    "print(f\"üìÖ Snapshot 2 Time: {second_snapshot['committed_at']}\")\n",
    "\n",
    "print(\"\\nüìä Updated data:\")\n",
    "spark.sql(\"SELECT * FROM local.time_travel_lab.customer_orders ORDER BY order_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Day 3: New orders added\n",
      "üì∏ Snapshot 3 ID: 1345330069139261156\n",
      "üìÖ Snapshot 3 Time: 2025-06-15 23:56:20.963000\n",
      "\n",
      "üìä Current data (latest):\n",
      "+--------+-----------+--------------+--------+----------+----------+---------+\n",
      "|order_id|customer_id|  product_name|quantity|unit_price|order_date|   status|\n",
      "+--------+-----------+--------------+--------+----------+----------+---------+\n",
      "|    1001|        101|    Laptop Pro|       1|   1299.99|2024-01-15|  shipped|\n",
      "|    1002|        102|Wireless Mouse|       2|     29.99|2024-01-15|delivered|\n",
      "|    1003|        103|      Keyboard|       1|     89.99|2024-01-15|  pending|\n",
      "|    1004|        104|    Monitor 4K|       1|    499.99|2024-01-17|  pending|\n",
      "|    1005|        105|     Webcam HD|       1|     79.99|2024-01-17|  pending|\n",
      "|    1006|        101|     USB-C Hub|       1|     49.99|2024-01-17|  shipped|\n",
      "+--------+-----------+--------------+--------+----------+----------+---------+\n",
      "\n",
      "\n",
      "üìà Total orders now: 6\n"
     ]
    }
   ],
   "source": [
    "time.sleep(2)\n",
    "\n",
    "# Add new orders - Day 3\n",
    "print(\"üìÖ Day 3: New orders added\")\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO local.time_travel_lab.customer_orders VALUES\n",
    "    (1004, 104, 'Monitor 4K', 1, 499.99, DATE '2024-01-17', 'pending'),\n",
    "    (1005, 105, 'Webcam HD', 1, 79.99, DATE '2024-01-17', 'pending'),\n",
    "    (1006, 101, 'USB-C Hub', 1, 49.99, DATE '2024-01-17', 'shipped')\n",
    "\"\"\")\n",
    "\n",
    "# Store third snapshot info\n",
    "third_snapshot = spark.sql(\"SELECT snapshot_id, committed_at FROM local.time_travel_lab.customer_orders.snapshots ORDER BY committed_at DESC LIMIT 1\").collect()[0]\n",
    "print(f\"üì∏ Snapshot 3 ID: {third_snapshot['snapshot_id']}\")\n",
    "print(f\"üìÖ Snapshot 3 Time: {third_snapshot['committed_at']}\")\n",
    "\n",
    "print(\"\\nüìä Current data (latest):\")\n",
    "spark.sql(\"SELECT * FROM local.time_travel_lab.customer_orders ORDER BY order_id\").show()\n",
    "\n",
    "print(f\"\\nüìà Total orders now: {spark.sql('SELECT COUNT(*) as count FROM local.time_travel_lab.customer_orders').collect()[0]['count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üì∏ Snapshot Management\n",
    "\n",
    "Learn how to view and manage table snapshots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∏ ALL SNAPSHOTS IN ORDER:\n",
      "+-------------------+-----------------------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|snapshot_id        |committed_at           |operation|summary                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "+-------------------+-----------------------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|319832172894815909 |2025-06-15 23:54:37.954|append   |{spark.app.id -> local-1750031545287, added-data-files -> 1, added-records -> 3, added-files-size -> 2161, changed-partition-count -> 1, total-records -> 3, total-files-size -> 2161, total-data-files -> 1, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}                                                                           |\n",
      "|2585198925291247008|2025-06-15 23:55:37.572|overwrite|{spark.app.id -> local-1750031545287, added-data-files -> 1, deleted-data-files -> 1, added-records -> 3, deleted-records -> 3, added-files-size -> 2217, removed-files-size -> 2161, changed-partition-count -> 1, total-records -> 3, total-files-size -> 2217, total-data-files -> 1, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}|\n",
      "|3751811866144034316|2025-06-15 23:55:37.838|overwrite|{spark.app.id -> local-1750031545287, added-data-files -> 1, deleted-data-files -> 1, added-records -> 3, deleted-records -> 3, added-files-size -> 2194, removed-files-size -> 2217, changed-partition-count -> 1, total-records -> 3, total-files-size -> 2194, total-data-files -> 1, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}|\n",
      "|1345330069139261156|2025-06-15 23:56:20.963|append   |{spark.app.id -> local-1750031545287, added-data-files -> 1, added-records -> 3, added-files-size -> 2141, changed-partition-count -> 1, total-records -> 6, total-files-size -> 4335, total-data-files -> 2, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}                                                                           |\n",
      "+-------------------+-----------------------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "\n",
      "üìä Total snapshots: 4\n",
      "üî¢ Snapshot IDs: [319832172894815909, 2585198925291247008, 3751811866144034316, 1345330069139261156]\n"
     ]
    }
   ],
   "source": [
    "# View all snapshots\n",
    "print(\"üì∏ ALL SNAPSHOTS IN ORDER:\")\n",
    "snapshots_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    snapshot_id,\n",
    "    committed_at,\n",
    "    operation,\n",
    "    summary\n",
    "FROM local.time_travel_lab.customer_orders.snapshots \n",
    "ORDER BY committed_at\n",
    "\"\"\")\n",
    "\n",
    "snapshots_df.show(truncate=False)\n",
    "\n",
    "print(f\"\\nüìä Total snapshots: {snapshots_df.count()}\")\n",
    "\n",
    "# Store snapshot IDs for later use\n",
    "all_snapshots = snapshots_df.collect()\n",
    "snapshot_ids = [row['snapshot_id'] for row in all_snapshots]\n",
    "print(f\"üî¢ Snapshot IDs: {snapshot_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã DETAILED TABLE HISTORY:\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|     made_current_at|        snapshot_id|          parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|2025-06-15 23:54:...| 319832172894815909|               NULL|               true|\n",
      "|2025-06-15 23:55:...|2585198925291247008| 319832172894815909|               true|\n",
      "|2025-06-15 23:55:...|3751811866144034316|2585198925291247008|               true|\n",
      "|2025-06-15 23:56:...|1345330069139261156|3751811866144034316|               true|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "\n",
      "\n",
      "üìÅ FILES PER SNAPSHOT:\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+------------------+------------+\n",
      "|file_path                                                                                                                                                  |file_size_in_bytes|record_count|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+------------------+------------+\n",
      "|file:/home/jovyan/work/warehouse/time_travel_lab/customer_orders/data/order_date_day=2024-01-17/00000-19-49c7586d-d2a7-4a77-a806-5c97b322e54c-00001.parquet|2141              |3           |\n",
      "|file:/home/jovyan/work/warehouse/time_travel_lab/customer_orders/data/order_date_day=2024-01-15/00000-13-33486d82-047b-4d72-9b7f-512a6c1ca5d5-00001.parquet|2194              |3           |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View table history with more details\n",
    "print(\"üìã DETAILED TABLE HISTORY:\")\n",
    "history_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    made_current_at,\n",
    "    snapshot_id,\n",
    "    parent_id,\n",
    "    is_current_ancestor\n",
    "FROM local.time_travel_lab.customer_orders.history\n",
    "ORDER BY made_current_at\n",
    "\"\"\")\n",
    "\n",
    "history_df.show()\n",
    "\n",
    "# View files for each snapshot\n",
    "print(\"\\nüìÅ FILES PER SNAPSHOT:\")\n",
    "files_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    file_path,\n",
    "    file_size_in_bytes,\n",
    "    record_count\n",
    "FROM local.time_travel_lab.customer_orders.files\n",
    "\"\"\")\n",
    "\n",
    "files_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ‚è∞ Time Travel Queries\n",
    "\n",
    "Learn different ways to query historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç METHOD 1: Query by Snapshot ID\n",
      "\\nüì∏ Data at first snapshot (original orders):\n",
      "+--------+-----------+--------------+--------+----------+----------+-------+\n",
      "|order_id|customer_id|  product_name|quantity|unit_price|order_date| status|\n",
      "+--------+-----------+--------------+--------+----------+----------+-------+\n",
      "|    1001|        101|    Laptop Pro|       1|   1299.99|2024-01-15|pending|\n",
      "|    1002|        102|Wireless Mouse|       2|     29.99|2024-01-15|shipped|\n",
      "|    1003|        103|      Keyboard|       1|     89.99|2024-01-15|pending|\n",
      "+--------+-----------+--------------+--------+----------+----------+-------+\n",
      "\n",
      "üìä Count at first snapshot: 3\n",
      "\\nüìä Current data for comparison:\n",
      "+--------+-----------+--------------+--------+----------+----------+---------+\n",
      "|order_id|customer_id|  product_name|quantity|unit_price|order_date|   status|\n",
      "+--------+-----------+--------------+--------+----------+----------+---------+\n",
      "|    1001|        101|    Laptop Pro|       1|   1299.99|2024-01-15|  shipped|\n",
      "|    1002|        102|Wireless Mouse|       2|     29.99|2024-01-15|delivered|\n",
      "|    1003|        103|      Keyboard|       1|     89.99|2024-01-15|  pending|\n",
      "|    1004|        104|    Monitor 4K|       1|    499.99|2024-01-17|  pending|\n",
      "|    1005|        105|     Webcam HD|       1|     79.99|2024-01-17|  pending|\n",
      "|    1006|        101|     USB-C Hub|       1|     49.99|2024-01-17|  shipped|\n",
      "+--------+-----------+--------------+--------+----------+----------+---------+\n",
      "\n",
      "üìä Current count: 6\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Query by Snapshot ID\n",
    "print(\"üîç METHOD 1: Query by Snapshot ID\")\n",
    "print(\"\\\\nüì∏ Data at first snapshot (original orders):\")\n",
    "\n",
    "if len(snapshot_ids) > 0:\n",
    "    first_snapshot_query = f\"\"\"\n",
    "    SELECT * FROM local.time_travel_lab.customer_orders\n",
    "    VERSION AS OF {snapshot_ids[0]}\n",
    "    ORDER BY order_id\n",
    "    \"\"\"\n",
    "    spark.sql(first_snapshot_query).show()\n",
    "    \n",
    "    print(f\"üìä Count at first snapshot: {spark.sql(f'SELECT COUNT(*) as count FROM local.time_travel_lab.customer_orders VERSION AS OF {snapshot_ids[0]}').collect()[0]['count']}\")\n",
    "\n",
    "# Compare with current data\n",
    "print(\"\\\\nüìä Current data for comparison:\")\n",
    "spark.sql(\"SELECT * FROM local.time_travel_lab.customer_orders ORDER BY order_id\").show()\n",
    "print(f\"üìä Current count: {spark.sql('SELECT COUNT(*) as count FROM local.time_travel_lab.customer_orders').collect()[0]['count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç METHOD 2: Query by Timestamp\n",
      "\\nüìÖ Data as of timestamp: 2025-06-15 23:55:37.572000\n",
      "+--------+-----------+--------------+--------+----------+----------+-------+\n",
      "|order_id|customer_id|  product_name|quantity|unit_price|order_date| status|\n",
      "+--------+-----------+--------------+--------+----------+----------+-------+\n",
      "|    1001|        101|    Laptop Pro|       1|   1299.99|2024-01-15|shipped|\n",
      "|    1002|        102|Wireless Mouse|       2|     29.99|2024-01-15|shipped|\n",
      "|    1003|        103|      Keyboard|       1|     89.99|2024-01-15|pending|\n",
      "+--------+-----------+--------------+--------+----------+----------+-------+\n",
      "\n",
      "üìä Count at that timestamp: 3\n",
      "\\nüîç METHOD 3: Query with system functions\n",
      "\\nüìÖ Data from 5 minutes ago (if available):\n",
      "+--------+-----------+--------------+--------+----------+----------+---------+\n",
      "|order_id|customer_id|  product_name|quantity|unit_price|order_date|   status|\n",
      "+--------+-----------+--------------+--------+----------+----------+---------+\n",
      "|    1001|        101|    Laptop Pro|       1|   1299.99|2024-01-15|  shipped|\n",
      "|    1002|        102|Wireless Mouse|       2|     29.99|2024-01-15|delivered|\n",
      "|    1003|        103|      Keyboard|       1|     89.99|2024-01-15|  pending|\n",
      "|    1004|        104|    Monitor 4K|       1|    499.99|2024-01-17|  pending|\n",
      "|    1005|        105|     Webcam HD|       1|     79.99|2024-01-17|  pending|\n",
      "|    1006|        101|     USB-C Hub|       1|     49.99|2024-01-17|  shipped|\n",
      "+--------+-----------+--------------+--------+----------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Query by Timestamp\n",
    "print(\"üîç METHOD 2: Query by Timestamp\")\n",
    "\n",
    "# Get timestamp from second snapshot\n",
    "if len(all_snapshots) >= 2:\n",
    "    second_timestamp = all_snapshots[1]['committed_at']\n",
    "    print(f\"\\\\nüìÖ Data as of timestamp: {second_timestamp}\")\n",
    "    \n",
    "    timestamp_query = f\"\"\"\n",
    "    SELECT * FROM local.time_travel_lab.customer_orders\n",
    "    TIMESTAMP AS OF '{second_timestamp}'\n",
    "    ORDER BY order_id\n",
    "    \"\"\"\n",
    "    spark.sql(timestamp_query).show()\n",
    "    \n",
    "    print(f\"üìä Count at that timestamp: {spark.sql(f'''SELECT COUNT(*) as count FROM local.time_travel_lab.customer_orders TIMESTAMP AS OF '{second_timestamp}' ''').collect()[0]['count']}\")\n",
    "\n",
    "# Method 3: Query with relative time\n",
    "print(\"\\\\nüîç METHOD 3: Query with system functions\")\n",
    "print(\"\\\\nüìÖ Data from 5 minutes ago (if available):\")\n",
    "\n",
    "# This would work in a real scenario with longer time gaps\n",
    "relative_time_query = \"\"\"\n",
    "SELECT * FROM local.time_travel_lab.customer_orders\n",
    "TIMESTAMP AS OF CURRENT_TIMESTAMP() - INTERVAL 5 MINUTES\n",
    "ORDER BY order_id\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    spark.sql(relative_time_query).show()\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Note: {str(e)}\")\n",
    "    print(\"üí° This is expected in our demo due to short time intervals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üîÑ Rollback Operations\n",
    "\n",
    "Learn how to rollback tables to previous states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä CURRENT STATE (before rollback):\n",
      "+--------+-----------+--------------+--------+----------+----------+-------+--------------+\n",
      "|order_id|customer_id|  product_name|quantity|unit_price|order_date| status|customer_email|\n",
      "+--------+-----------+--------------+--------+----------+----------+-------+--------------+\n",
      "|    1001|        101|    Laptop Pro|       1|   1299.99|2024-01-15|shipped|          NULL|\n",
      "|    1002|        102|Wireless Mouse|       2|     29.99|2024-01-15|shipped|          NULL|\n",
      "|    1003|        103|      Keyboard|       1|     89.99|2024-01-15|pending|          NULL|\n",
      "+--------+-----------+--------------+--------+----------+----------+-------+--------------+\n",
      "\n",
      "Current record count: 3\n",
      "\\nüì∏ Available snapshots for rollback:\n",
      "+-------------------+-----------------------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|snapshot_id        |committed_at           |operation|summary                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "+-------------------+-----------------------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|319832172894815909 |2025-06-15 23:54:37.954|append   |{spark.app.id -> local-1750031545287, added-data-files -> 1, added-records -> 3, added-files-size -> 2161, changed-partition-count -> 1, total-records -> 3, total-files-size -> 2161, total-data-files -> 1, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}                                                                           |\n",
      "|2585198925291247008|2025-06-15 23:55:37.572|overwrite|{spark.app.id -> local-1750031545287, added-data-files -> 1, deleted-data-files -> 1, added-records -> 3, deleted-records -> 3, added-files-size -> 2217, removed-files-size -> 2161, changed-partition-count -> 1, total-records -> 3, total-files-size -> 2217, total-data-files -> 1, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}|\n",
      "|3751811866144034316|2025-06-15 23:55:37.838|overwrite|{spark.app.id -> local-1750031545287, added-data-files -> 1, deleted-data-files -> 1, added-records -> 3, deleted-records -> 3, added-files-size -> 2194, removed-files-size -> 2217, changed-partition-count -> 1, total-records -> 3, total-files-size -> 2194, total-data-files -> 1, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}|\n",
      "|1345330069139261156|2025-06-15 23:56:20.963|append   |{spark.app.id -> local-1750031545287, added-data-files -> 1, added-records -> 3, added-files-size -> 2141, changed-partition-count -> 1, total-records -> 6, total-files-size -> 4335, total-data-files -> 2, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}                                                                           |\n",
      "|5109776364082806215|2025-06-16 00:22:42.094|append   |{spark.app.id -> local-1750031545287, added-data-files -> 1, added-records -> 1, added-files-size -> 2382, changed-partition-count -> 1, total-records -> 7, total-files-size -> 6717, total-data-files -> 3, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}                                                                           |\n",
      "|2052505312059808573|2025-06-16 00:23:24.061|overwrite|{spark.app.id -> local-1750031545287, added-data-files -> 2, deleted-data-files -> 3, added-records -> 3, deleted-records -> 7, added-files-size -> 4619, removed-files-size -> 6717, changed-partition-count -> 3, total-records -> 3, total-files-size -> 4619, total-data-files -> 2, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}|\n",
      "|5207708375428732091|2025-06-16 00:26:05.353|overwrite|{spark.app.id -> local-1750031545287, changed-partition-count -> 0, total-records -> 3, total-files-size -> 4619, total-data-files -> 2, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}                                                                                                                                                |\n",
      "|2744569615811412771|2025-06-16 00:27:58.335|overwrite|{spark.app.id -> local-1750031545287, changed-partition-count -> 0, total-records -> 3, total-files-size -> 4619, total-data-files -> 2, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}                                                                                                                                                |\n",
      "+-------------------+-----------------------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "üîÑ Rolling back to snapshot: 2585198925291247008\n",
      "‚úÖ Rollback completed!\n",
      "\\nüìä DATA AFTER ROLLBACK:\n",
      "+--------+-----------+--------------+--------+----------+----------+-------+--------------+\n",
      "|order_id|customer_id|  product_name|quantity|unit_price|order_date| status|customer_email|\n",
      "+--------+-----------+--------------+--------+----------+----------+-------+--------------+\n",
      "|    1001|        101|    Laptop Pro|       1|   1299.99|2024-01-15|shipped|          NULL|\n",
      "|    1002|        102|Wireless Mouse|       2|     29.99|2024-01-15|shipped|          NULL|\n",
      "|    1003|        103|      Keyboard|       1|     89.99|2024-01-15|pending|          NULL|\n",
      "+--------+-----------+--------------+--------+----------+----------+-------+--------------+\n",
      "\n",
      "Record count after rollback: 3\n",
      "\\nüí° Notice:\n",
      "- The new orders (1004-1006) from Day 3 are gone\n",
      "- We're back to the state after Day 2 updates\n",
      "- Order 1001 status is 'shipped' (not 'pending')\n",
      "- Order 1002 status is 'delivered' (not 'shipped')\n"
     ]
    }
   ],
   "source": [
    "# Show current state before rollback\n",
    "print(\"üìä CURRENT STATE (before rollback):\")\n",
    "current_data = spark.sql(\"SELECT * FROM local.time_travel_lab.customer_orders ORDER BY order_id\")\n",
    "current_data.show()\n",
    "print(f\"Current record count: {current_data.count()}\")\n",
    "\n",
    "# Show available snapshots\n",
    "print(\"\\\\nüì∏ Available snapshots for rollback:\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    snapshot_id,\n",
    "    committed_at,\n",
    "    operation,\n",
    "    summary\n",
    "FROM local.time_travel_lab.customer_orders.snapshots \n",
    "ORDER BY committed_at\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# Rollback to the second snapshot (after updates but before new orders)\n",
    "if len(snapshot_ids) >= 2:\n",
    "    rollback_snapshot_id = snapshot_ids[1]  # Second snapshot\n",
    "    print(f\"üîÑ Rolling back to snapshot: {rollback_snapshot_id}\")\n",
    "    \n",
    "    rollback_query = f\"\"\"\n",
    "    CALL local.system.rollback_to_snapshot('local.time_travel_lab.customer_orders', {rollback_snapshot_id})\n",
    "    \"\"\"\n",
    "    \n",
    "    spark.sql(rollback_query)\n",
    "    print(\"‚úÖ Rollback completed!\")\n",
    "    \n",
    "    # Show data after rollback\n",
    "    print(\"\\\\nüìä DATA AFTER ROLLBACK:\")\n",
    "    rollback_data = spark.sql(\"SELECT * FROM local.time_travel_lab.customer_orders ORDER BY order_id\")\n",
    "    rollback_data.show()\n",
    "    print(f\"Record count after rollback: {rollback_data.count()}\")\n",
    "    \n",
    "    # Compare with what we expected\n",
    "    print(\"\\\\nüí° Notice:\")\n",
    "    print(\"- The new orders (1004-1006) from Day 3 are gone\")\n",
    "    print(\"- We're back to the state after Day 2 updates\")\n",
    "    print(\"- Order 1001 status is 'shipped' (not 'pending')\")\n",
    "    print(\"- Order 1002 status is 'delivered' (not 'shipped')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∏ SNAPSHOTS AFTER ROLLBACK:\n",
      "+-------------------+-----------------------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|snapshot_id        |committed_at           |operation|summary                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "+-------------------+-----------------------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|319832172894815909 |2025-06-15 23:54:37.954|append   |{spark.app.id -> local-1750031545287, added-data-files -> 1, added-records -> 3, added-files-size -> 2161, changed-partition-count -> 1, total-records -> 3, total-files-size -> 2161, total-data-files -> 1, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}                                                                           |\n",
      "|2585198925291247008|2025-06-15 23:55:37.572|overwrite|{spark.app.id -> local-1750031545287, added-data-files -> 1, deleted-data-files -> 1, added-records -> 3, deleted-records -> 3, added-files-size -> 2217, removed-files-size -> 2161, changed-partition-count -> 1, total-records -> 3, total-files-size -> 2217, total-data-files -> 1, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}|\n",
      "|3751811866144034316|2025-06-15 23:55:37.838|overwrite|{spark.app.id -> local-1750031545287, added-data-files -> 1, deleted-data-files -> 1, added-records -> 3, deleted-records -> 3, added-files-size -> 2194, removed-files-size -> 2217, changed-partition-count -> 1, total-records -> 3, total-files-size -> 2194, total-data-files -> 1, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}|\n",
      "|1345330069139261156|2025-06-15 23:56:20.963|append   |{spark.app.id -> local-1750031545287, added-data-files -> 1, added-records -> 3, added-files-size -> 2141, changed-partition-count -> 1, total-records -> 6, total-files-size -> 4335, total-data-files -> 2, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}                                                                           |\n",
      "|5109776364082806215|2025-06-16 00:22:42.094|append   |{spark.app.id -> local-1750031545287, added-data-files -> 1, added-records -> 1, added-files-size -> 2382, changed-partition-count -> 1, total-records -> 7, total-files-size -> 6717, total-data-files -> 3, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}                                                                           |\n",
      "|2052505312059808573|2025-06-16 00:23:24.061|overwrite|{spark.app.id -> local-1750031545287, added-data-files -> 2, deleted-data-files -> 3, added-records -> 3, deleted-records -> 7, added-files-size -> 4619, removed-files-size -> 6717, changed-partition-count -> 3, total-records -> 3, total-files-size -> 4619, total-data-files -> 2, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}|\n",
      "|5207708375428732091|2025-06-16 00:26:05.353|overwrite|{spark.app.id -> local-1750031545287, changed-partition-count -> 0, total-records -> 3, total-files-size -> 4619, total-data-files -> 2, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}                                                                                                                                                |\n",
      "|2744569615811412771|2025-06-16 00:27:58.335|overwrite|{spark.app.id -> local-1750031545287, changed-partition-count -> 0, total-records -> 3, total-files-size -> 4619, total-data-files -> 2, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}                                                                                                                                                |\n",
      "+-------------------+-----------------------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "\\nüí° Key Points about Rollback:\n",
      "‚úì Rollback creates a new snapshot\n",
      "‚úì Original snapshots are still preserved\n",
      "‚úì You can still time-travel to any historical state\n",
      "‚úì Rollback is metadata operation - very fast\n",
      "‚úì No data files are actually deleted\n"
     ]
    }
   ],
   "source": [
    "# View snapshots after rollback\n",
    "print(\"üì∏ SNAPSHOTS AFTER ROLLBACK:\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    snapshot_id,\n",
    "    committed_at,\n",
    "    operation,\n",
    "    summary\n",
    "FROM local.time_travel_lab.customer_orders.snapshots \n",
    "ORDER BY committed_at\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "print(\"\\\\nüí° Key Points about Rollback:\")\n",
    "print(\"‚úì Rollback creates a new snapshot\")\n",
    "print(\"‚úì Original snapshots are still preserved\") \n",
    "print(\"‚úì You can still time-travel to any historical state\")\n",
    "print(\"‚úì Rollback is metadata operation - very fast\")\n",
    "print(\"‚úì No data files are actually deleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. üîß Schema Evolution with Time Travel\n",
    "\n",
    "Learn how time travel works with schema changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß SCHEMA EVOLUTION DEMO\n",
      "\\nüìã Current schema:\n",
      "+--------------+----------------+-------+\n",
      "|      col_name|       data_type|comment|\n",
      "+--------------+----------------+-------+\n",
      "|      order_id|          bigint|   NULL|\n",
      "|   customer_id|          bigint|   NULL|\n",
      "|  product_name|          string|   NULL|\n",
      "|      quantity|             int|   NULL|\n",
      "|    unit_price|   decimal(10,2)|   NULL|\n",
      "|    order_date|            date|   NULL|\n",
      "|        status|          string|   NULL|\n",
      "|              |                |       |\n",
      "|# Partitioning|                |       |\n",
      "|        Part 0|days(order_date)|       |\n",
      "+--------------+----------------+-------+\n",
      "\n",
      "\\n‚ûï Adding new column 'customer_email':\n",
      "\\nüìã New schema:\n",
      "+--------------+----------------+-------+\n",
      "|      col_name|       data_type|comment|\n",
      "+--------------+----------------+-------+\n",
      "|      order_id|          bigint|   NULL|\n",
      "|   customer_id|          bigint|   NULL|\n",
      "|  product_name|          string|   NULL|\n",
      "|      quantity|             int|   NULL|\n",
      "|    unit_price|   decimal(10,2)|   NULL|\n",
      "|    order_date|            date|   NULL|\n",
      "|        status|          string|   NULL|\n",
      "|customer_email|          string|   NULL|\n",
      "|              |                |       |\n",
      "|# Partitioning|                |       |\n",
      "|        Part 0|days(order_date)|       |\n",
      "+--------------+----------------+-------+\n",
      "\n",
      "\\nüìù Inserting data with new column:\n",
      "\\nüìä Current data with new column:\n",
      "+--------+-----------+--------------+--------+----------+----------+---------+---------------------+\n",
      "|order_id|customer_id|product_name  |quantity|unit_price|order_date|status   |customer_email       |\n",
      "+--------+-----------+--------------+--------+----------+----------+---------+---------------------+\n",
      "|1001    |101        |Laptop Pro    |1       |1299.99   |2024-01-15|shipped  |NULL                 |\n",
      "|1002    |102        |Wireless Mouse|2       |29.99     |2024-01-15|delivered|NULL                 |\n",
      "|1003    |103        |Keyboard      |1       |89.99     |2024-01-15|pending  |NULL                 |\n",
      "|1004    |104        |Monitor 4K    |1       |499.99    |2024-01-17|pending  |NULL                 |\n",
      "|1005    |105        |Webcam HD     |1       |79.99     |2024-01-17|pending  |NULL                 |\n",
      "|1006    |101        |USB-C Hub     |1       |49.99     |2024-01-17|shipped  |NULL                 |\n",
      "|1007    |106        |Headphones    |1       |199.99    |2024-01-18|pending  |customer106@email.com|\n",
      "+--------+-----------+--------------+--------+----------+----------+---------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a new column to demonstrate schema evolution\n",
    "print(\"üîß SCHEMA EVOLUTION DEMO\")\n",
    "print(\"\\\\nüìã Current schema:\")\n",
    "spark.sql(\"DESCRIBE local.time_travel_lab.customer_orders\").show()\n",
    "\n",
    "# Add a new column\n",
    "print(\"\\\\n‚ûï Adding new column 'customer_email':\")\n",
    "spark.sql(\"ALTER TABLE local.time_travel_lab.customer_orders ADD COLUMN customer_email string\")\n",
    "\n",
    "print(\"\\\\nüìã New schema:\")\n",
    "spark.sql(\"DESCRIBE local.time_travel_lab.customer_orders\").show()\n",
    "\n",
    "# Insert data with the new column\n",
    "print(\"\\\\nüìù Inserting data with new column:\")\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO local.time_travel_lab.customer_orders VALUES\n",
    "    (1007, 106, 'Headphones', 1, 199.99, DATE '2024-01-18', 'pending', 'customer106@email.com')\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\\\nüìä Current data with new column:\")\n",
    "spark.sql(\"SELECT * FROM local.time_travel_lab.customer_orders ORDER BY order_id\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è∞ TIME TRAVEL WITH SCHEMA EVOLUTION\n",
      "\\nüì∏ Querying old snapshot 319832172894815909 (before schema change):\n",
      "+--------+-----------+--------------+--------+----------+----------+-------+\n",
      "|order_id|customer_id|  product_name|quantity|unit_price|order_date| status|\n",
      "+--------+-----------+--------------+--------+----------+----------+-------+\n",
      "|    1001|        101|    Laptop Pro|       1|   1299.99|2024-01-15|pending|\n",
      "|    1002|        102|Wireless Mouse|       2|     29.99|2024-01-15|shipped|\n",
      "|    1003|        103|      Keyboard|       1|     89.99|2024-01-15|pending|\n",
      "+--------+-----------+--------------+--------+----------+----------+-------+\n",
      "\n",
      "\\nüí° Notice:\n",
      "‚úì Old snapshots show NULL for new columns\n",
      "‚úì Schema evolution is backward compatible\n",
      "‚úì You can query any historical snapshot regardless of schema changes\n",
      "\\nüìã SCHEMA EVOLUTION HISTORY:\n",
      "1. Original: order_id, customer_id, product_name, quantity, unit_price, order_date, status\n",
      "2. Current: + customer_email (added later)\n",
      "\\n‚úÖ Time travel works seamlessly across schema versions!\n"
     ]
    }
   ],
   "source": [
    "# Time travel with schema evolution\n",
    "print(\"‚è∞ TIME TRAVEL WITH SCHEMA EVOLUTION\")\n",
    "\n",
    "# Query old snapshot - schema compatibility\n",
    "if len(snapshot_ids) > 0:\n",
    "    print(f\"\\\\nüì∏ Querying old snapshot {snapshot_ids[0]} (before schema change):\")\n",
    "    old_data_query = f\"\"\"\n",
    "    SELECT * FROM local.time_travel_lab.customer_orders\n",
    "    VERSION AS OF {snapshot_ids[0]}\n",
    "    ORDER BY order_id\n",
    "    \"\"\"\n",
    "    old_data = spark.sql(old_data_query) \n",
    "    old_data.show()\n",
    "    \n",
    "    print(\"\\\\nüí° Notice:\")\n",
    "    print(\"‚úì Old snapshots show NULL for new columns\")\n",
    "    print(\"‚úì Schema evolution is backward compatible\")\n",
    "    print(\"‚úì You can query any historical snapshot regardless of schema changes\")\n",
    "\n",
    "# Show the evolution of the schema over time\n",
    "print(\"\\\\nüìã SCHEMA EVOLUTION HISTORY:\")\n",
    "print(\"1. Original: order_id, customer_id, product_name, quantity, unit_price, order_date, status\")\n",
    "print(\"2. Current: + customer_email (added later)\")\n",
    "print(\"\\\\n‚úÖ Time travel works seamlessly across schema versions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. üìä Analytical Use Cases\n",
    "\n",
    "Real-world analytical scenarios using time travel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä USE CASE 1: Point-in-time Reporting\n",
      "\\nüìÖ Generate daily snapshots for reporting:\n",
      "\\nüìä Daily Report 1 - 2025-06-15 23:54:37.954000\n",
      "+------------+-------------+----------------+---------------+\n",
      "|total_orders|total_revenue|unique_customers|avg_order_value|\n",
      "+------------+-------------+----------------+---------------+\n",
      "|           3|      1449.96|               3|     483.320000|\n",
      "+------------+-------------+----------------+---------------+\n",
      "\n",
      "\\nüìä Daily Report 2 - 2025-06-15 23:55:37.572000\n",
      "+------------+-------------+----------------+---------------+\n",
      "|total_orders|total_revenue|unique_customers|avg_order_value|\n",
      "+------------+-------------+----------------+---------------+\n",
      "|           3|      1449.96|               3|     483.320000|\n",
      "+------------+-------------+----------------+---------------+\n",
      "\n",
      "\\nüìä Daily Report 3 - 2025-06-15 23:55:37.838000\n",
      "+------------+-------------+----------------+---------------+\n",
      "|total_orders|total_revenue|unique_customers|avg_order_value|\n",
      "+------------+-------------+----------------+---------------+\n",
      "|           3|      1449.96|               3|     483.320000|\n",
      "+------------+-------------+----------------+---------------+\n",
      "\n",
      "\\nüí° Benefits:\n",
      "‚úì Consistent reporting across time\n",
      "‚úì Audit trail for financial reports\n",
      "‚úì Compare metrics across different time periods\n"
     ]
    }
   ],
   "source": [
    "# Use Case 1: Point-in-time reporting\n",
    "print(\"üìä USE CASE 1: Point-in-time Reporting\")\n",
    "print(\"\\\\nüìÖ Generate daily snapshots for reporting:\")\n",
    "\n",
    "# Simulate daily reporting snapshots\n",
    "for i, snapshot in enumerate(all_snapshots[:3], 1):\n",
    "    snapshot_id = snapshot['snapshot_id']\n",
    "    snapshot_time = snapshot['committed_at']\n",
    "    \n",
    "    print(f\"\\\\nüìä Daily Report {i} - {snapshot_time}\")\n",
    "    \n",
    "    daily_report = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_orders,\n",
    "        SUM(quantity * unit_price) as total_revenue,\n",
    "        COUNT(DISTINCT customer_id) as unique_customers,\n",
    "        AVG(quantity * unit_price) as avg_order_value\n",
    "    FROM local.time_travel_lab.customer_orders\n",
    "    VERSION AS OF {snapshot_id}\n",
    "    \"\"\")\n",
    "    \n",
    "    daily_report.show()\n",
    "\n",
    "print(\"\\\\nüí° Benefits:\")\n",
    "print(\"‚úì Consistent reporting across time\")\n",
    "print(\"‚úì Audit trail for financial reports\") \n",
    "print(\"‚úì Compare metrics across different time periods\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä USE CASE 2: Change Data Capture Analysis\n",
      "\\nüîç Analyzing what changed between snapshots:\n",
      "\\nüì∏ Comparing Snapshot 319832172894815909 vs 2585198925291247008\n",
      "\\nüîÑ Status changes between snapshots:\n",
      "+--------+----------+----------+--------------+\n",
      "|order_id|old_status|new_status|   change_type|\n",
      "+--------+----------+----------+--------------+\n",
      "|    1001|   pending|   shipped|Status Changed|\n",
      "+--------+----------+----------+--------------+\n",
      "\n",
      "\\nüí° CDC Use Cases:\n",
      "‚úì Track data lineage and audit changes\n",
      "‚úì Build change streams for downstream systems\n",
      "‚úì Identify data quality issues\n",
      "‚úì Monitor business process changes\n"
     ]
    }
   ],
   "source": [
    "# Use Case 2: Change Data Capture (CDC) Analysis\n",
    "print(\"üìä USE CASE 2: Change Data Capture Analysis\")\n",
    "print(\"\\\\nüîç Analyzing what changed between snapshots:\")\n",
    "\n",
    "if len(all_snapshots) >= 2:\n",
    "    snapshot1_id = all_snapshots[0]['snapshot_id']\n",
    "    snapshot2_id = all_snapshots[1]['snapshot_id']\n",
    "    \n",
    "    print(f\"\\\\nüì∏ Comparing Snapshot {snapshot1_id} vs {snapshot2_id}\")\n",
    "    \n",
    "    # Data at first snapshot\n",
    "    data1 = spark.sql(f\"\"\"\n",
    "    SELECT order_id, status, 'snapshot1' as source\n",
    "    FROM local.time_travel_lab.customer_orders\n",
    "    VERSION AS OF {snapshot1_id}\n",
    "    \"\"\")\n",
    "    \n",
    "    # Data at second snapshot  \n",
    "    data2 = spark.sql(f\"\"\"\n",
    "    SELECT order_id, status, 'snapshot2' as source\n",
    "    FROM local.time_travel_lab.customer_orders\n",
    "    VERSION AS OF {snapshot2_id}\n",
    "    \"\"\")\n",
    "    \n",
    "    # Find changes\n",
    "    print(\"\\\\nüîÑ Status changes between snapshots:\")\n",
    "    changes = spark.sql(f\"\"\"\n",
    "    WITH snapshot1 AS (\n",
    "        SELECT order_id, status as status1\n",
    "        FROM local.time_travel_lab.customer_orders\n",
    "        VERSION AS OF {snapshot1_id}\n",
    "    ),\n",
    "    snapshot2 AS (\n",
    "        SELECT order_id, status as status2\n",
    "        FROM local.time_travel_lab.customer_orders\n",
    "        VERSION AS OF {snapshot2_id}\n",
    "    )\n",
    "    SELECT \n",
    "        s1.order_id,\n",
    "        s1.status1 as old_status,\n",
    "        s2.status2 as new_status,\n",
    "        'Status Changed' as change_type\n",
    "    FROM snapshot1 s1\n",
    "    JOIN snapshot2 s2 ON s1.order_id = s2.order_id\n",
    "    WHERE s1.status1 != s2.status2\n",
    "    \"\"\")\n",
    "    \n",
    "    changes.show()\n",
    "\n",
    "print(\"\\\\nüí° CDC Use Cases:\")\n",
    "print(\"‚úì Track data lineage and audit changes\")\n",
    "print(\"‚úì Build change streams for downstream systems\")\n",
    "print(\"‚úì Identify data quality issues\")\n",
    "print(\"‚úì Monitor business process changes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä USE CASE 3: Data Recovery and Debugging\n",
      "\\nüîß Simulate an accidental data corruption and recovery:\n",
      "\\n‚ö†Ô∏è Simulating accidental data deletion:\n",
      "Current data before 'accident':\n",
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|    3|\n",
      "+-----+\n",
      "\n",
      "\\nüí• After accidental deletion:\n",
      "+--------+-----------+--------------+--------+----------+----------+---------+--------------+\n",
      "|order_id|customer_id|  product_name|quantity|unit_price|order_date|   status|customer_email|\n",
      "+--------+-----------+--------------+--------+----------+----------+---------+--------------+\n",
      "|    1001|        101|    Laptop Pro|       1|   1299.99|2024-01-15|  shipped|          NULL|\n",
      "|    1002|        102|Wireless Mouse|       2|     29.99|2024-01-15|delivered|          NULL|\n",
      "|    1006|        101|     USB-C Hub|       1|     49.99|2024-01-17|  shipped|          NULL|\n",
      "+--------+-----------+--------------+--------+----------+----------+---------+--------------+\n",
      "\n",
      "Records remaining: 3\n",
      "\\nüö® EMERGENCY RECOVERY PROCEDURE:\n",
      "1. Identify the problem\n",
      "2. Find the last good snapshot\n",
      "3. Rollback to restore data\n",
      "\\nüì∏ Available snapshots for recovery:\n",
      "+-------------------+--------------------+---------+\n",
      "|        snapshot_id|        committed_at|operation|\n",
      "+-------------------+--------------------+---------+\n",
      "|2744569615811412771|2025-06-16 00:27:...|overwrite|\n",
      "|5207708375428732091|2025-06-16 00:26:...|overwrite|\n",
      "|2052505312059808573|2025-06-16 00:23:...|overwrite|\n",
      "|5109776364082806215|2025-06-16 00:22:...|   append|\n",
      "|1345330069139261156|2025-06-15 23:56:...|   append|\n",
      "|3751811866144034316|2025-06-15 23:55:...|overwrite|\n",
      "|2585198925291247008|2025-06-15 23:55:...|overwrite|\n",
      "| 319832172894815909|2025-06-15 23:54:...|   append|\n",
      "+-------------------+--------------------+---------+\n",
      "\n",
      "\\nüîÑ Recovering to snapshot: 5207708375428732091\n",
      "\\n‚úÖ DATA RECOVERED!\n",
      "+--------+-----------+--------------+--------+----------+----------+---------+--------------+\n",
      "|order_id|customer_id|  product_name|quantity|unit_price|order_date|   status|customer_email|\n",
      "+--------+-----------+--------------+--------+----------+----------+---------+--------------+\n",
      "|    1001|        101|    Laptop Pro|       1|   1299.99|2024-01-15|  shipped|          NULL|\n",
      "|    1002|        102|Wireless Mouse|       2|     29.99|2024-01-15|delivered|          NULL|\n",
      "|    1006|        101|     USB-C Hub|       1|     49.99|2024-01-17|  shipped|          NULL|\n",
      "+--------+-----------+--------------+--------+----------+----------+---------+--------------+\n",
      "\n",
      "Records after recovery: 3\n",
      "\\nüí° Recovery Benefits:\n",
      "‚úì Instant recovery from any point in time\n",
      "‚úì No need for external backups\n",
      "‚úì Minimal downtime\n",
      "‚úì Granular recovery options\n"
     ]
    }
   ],
   "source": [
    "# Use Case 3: Data Recovery and Debugging\n",
    "print(\"üìä USE CASE 3: Data Recovery and Debugging\")\n",
    "print(\"\\\\nüîß Simulate an accidental data corruption and recovery:\")\n",
    "\n",
    "# Simulate accidental deletion\n",
    "print(\"\\\\n‚ö†Ô∏è Simulating accidental data deletion:\")\n",
    "print(\"Current data before 'accident':\")\n",
    "spark.sql(\"SELECT COUNT(*) as count FROM local.time_travel_lab.customer_orders\").show()\n",
    "\n",
    "# Accidentally delete all pending orders\n",
    "spark.sql(\"DELETE FROM local.time_travel_lab.customer_orders WHERE status = 'pending'\")\n",
    "\n",
    "print(\"\\\\nüí• After accidental deletion:\")\n",
    "remaining_data = spark.sql(\"SELECT * FROM local.time_travel_lab.customer_orders ORDER BY order_id\")\n",
    "remaining_data.show()\n",
    "print(f\"Records remaining: {remaining_data.count()}\")\n",
    "\n",
    "# Recovery using time travel\n",
    "print(\"\\\\nüö® EMERGENCY RECOVERY PROCEDURE:\")\n",
    "print(\"1. Identify the problem\")\n",
    "print(\"2. Find the last good snapshot\")\n",
    "print(\"3. Rollback to restore data\")\n",
    "\n",
    "# Find the snapshot before deletion\n",
    "recovery_snapshots = spark.sql(\"\"\"\n",
    "SELECT snapshot_id, committed_at, operation\n",
    "FROM local.time_travel_lab.customer_orders.snapshots \n",
    "ORDER BY committed_at DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\\\nüì∏ Available snapshots for recovery:\")\n",
    "recovery_snapshots.show()\n",
    "\n",
    "# Get the snapshot before the DELETE operation\n",
    "snapshots_list = recovery_snapshots.collect()\n",
    "if len(snapshots_list) >= 2:\n",
    "    # Second most recent (before the DELETE)\n",
    "    recovery_snapshot_id = snapshots_list[1]['snapshot_id']\n",
    "    print(f\"\\\\nüîÑ Recovering to snapshot: {recovery_snapshot_id}\")\n",
    "    \n",
    "    # Rollback to recover\n",
    "    spark.sql(f\"CALL local.system.rollback_to_snapshot('local.time_travel_lab.customer_orders', {recovery_snapshot_id})\")    \n",
    "    print(\"\\\\n‚úÖ DATA RECOVERED!\")\n",
    "    recovered_data = spark.sql(\"SELECT * FROM local.time_travel_lab.customer_orders ORDER BY order_id\")\n",
    "    recovered_data.show()\n",
    "    print(f\"Records after recovery: {recovered_data.count()}\")\n",
    "\n",
    "print(\"\\\\nüí° Recovery Benefits:\")\n",
    "print(\"‚úì Instant recovery from any point in time\")\n",
    "print(\"‚úì No need for external backups\")\n",
    "print(\"‚úì Minimal downtime\")\n",
    "print(\"‚úì Granular recovery options\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ‚ö° Performance Considerations\n",
    "\n",
    "Best practices for time travel performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° TIME TRAVEL PERFORMANCE CONSIDERATIONS\n",
      "\\nüì∏ 1. SNAPSHOT MANAGEMENT:\n",
      "Current snapshot count:\n",
      "Total snapshots: 6\n",
      "\\nüí° Snapshot Retention Best Practices:\n",
      "‚úì Keep only necessary snapshots for time travel\n",
      "‚úì Use expire_snapshots procedure to clean old snapshots\n",
      "‚úì Balance between history needs and metadata overhead\n",
      "‚úì Consider business and compliance requirements\n",
      "‚úì Monitor metadata size vs data size ratio\n",
      "\\nüöÄ 2. QUERY PERFORMANCE TIPS:\n",
      "‚úì Use snapshot IDs instead of timestamps when possible\n",
      "‚úì Snapshot queries are faster than timestamp queries\n",
      "‚úì Combine time travel with partition pruning\n",
      "‚úì Use projection pushdown to reduce data scanning\n",
      "‚úì Cache frequently accessed historical snapshots\n",
      "\\nüíæ 3. STORAGE IMPACT:\n",
      "+-----------+--------------------+--------------------+\n",
      "|total_files|       total_size_mb|    avg_file_size_mb|\n",
      "+-----------+--------------------+--------------------+\n",
      "|          2|0.004405021667480469|0.002202510833740...|\n",
      "+-----------+--------------------+--------------------+\n",
      "\n",
      "Storage optimization tips:\n",
      "‚úì Iceberg stores only changed data, not full copies\n",
      "‚úì Metadata overhead is minimal compared to data size\n",
      "‚úì File-level deduplication reduces storage costs\n",
      "‚úì Compaction helps optimize file sizes over time\n"
     ]
    }
   ],
   "source": [
    "# Performance Analysis\n",
    "print(\"‚ö° TIME TRAVEL PERFORMANCE CONSIDERATIONS\")\n",
    "\n",
    "# 1. Snapshot Retention\n",
    "print(\"\\\\nüì∏ 1. SNAPSHOT MANAGEMENT:\")\n",
    "print(\"Current snapshot count:\")\n",
    "snapshot_count = spark.sql(\"SELECT COUNT(*) as count FROM local.time_travel_lab.customer_orders.snapshots\").collect()[0]['count']\n",
    "print(f\"Total snapshots: {snapshot_count}\")\n",
    "\n",
    "print(\"\\\\nüí° Snapshot Retention Best Practices:\")\n",
    "retention_tips = [\n",
    "    \"Keep only necessary snapshots for time travel\",\n",
    "    \"Use expire_snapshots procedure to clean old snapshots\", \n",
    "    \"Balance between history needs and metadata overhead\",\n",
    "    \"Consider business and compliance requirements\",\n",
    "    \"Monitor metadata size vs data size ratio\"\n",
    "]\n",
    "\n",
    "for tip in retention_tips:\n",
    "    print(f\"‚úì {tip}\")\n",
    "\n",
    "# 2. Query Performance Tips\n",
    "print(\"\\\\nüöÄ 2. QUERY PERFORMANCE TIPS:\")\n",
    "performance_tips = [\n",
    "    \"Use snapshot IDs instead of timestamps when possible\",\n",
    "    \"Snapshot queries are faster than timestamp queries\",\n",
    "    \"Combine time travel with partition pruning\",\n",
    "    \"Use projection pushdown to reduce data scanning\",\n",
    "    \"Cache frequently accessed historical snapshots\"\n",
    "]\n",
    "\n",
    "for tip in performance_tips:\n",
    "    print(f\"‚úì {tip}\")\n",
    "\n",
    "# 3. Storage Impact\n",
    "print(\"\\\\nüíæ 3. STORAGE IMPACT:\")\n",
    "files_info = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as total_files,\n",
    "    SUM(file_size_in_bytes) / 1024 / 1024 as total_size_mb,\n",
    "    AVG(file_size_in_bytes) / 1024 / 1024 as avg_file_size_mb\n",
    "FROM local.time_travel_lab.customer_orders.files\n",
    "\"\"\")\n",
    "\n",
    "files_info.show()\n",
    "\n",
    "print(\"Storage optimization tips:\")\n",
    "storage_tips = [\n",
    "    \"Iceberg stores only changed data, not full copies\",\n",
    "    \"Metadata overhead is minimal compared to data size\",\n",
    "    \"File-level deduplication reduces storage costs\",\n",
    "    \"Compaction helps optimize file sizes over time\"\n",
    "]\n",
    "\n",
    "for tip in storage_tips:\n",
    "    print(f\"‚úì {tip}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ SNAPSHOT CLEANUP DEMONSTRATION\n",
      "\\nüì∏ Before cleanup - All snapshots:\n",
      "+-------------------+--------------------+---------+\n",
      "|        snapshot_id|        committed_at|operation|\n",
      "+-------------------+--------------------+---------+\n",
      "| 319832172894815909|2025-06-15 23:54:...|   append|\n",
      "|2585198925291247008|2025-06-15 23:55:...|overwrite|\n",
      "|3751811866144034316|2025-06-15 23:55:...|overwrite|\n",
      "|1345330069139261156|2025-06-15 23:56:...|   append|\n",
      "|5109776364082806215|2025-06-16 00:22:...|   append|\n",
      "|2052505312059808573|2025-06-16 00:23:...|overwrite|\n",
      "+-------------------+--------------------+---------+\n",
      "\n",
      "\\nüßπ Snapshot Cleanup Commands (for reference):\n",
      "-- Expire snapshots older than 7 days:\n",
      "CALL spark_catalog.system.expire_snapshots('local.time_travel_lab.customer_orders', TIMESTAMP '2024-01-08 00:00:00')\n",
      "\n",
      "-- Keep only last 5 snapshots:\n",
      "CALL spark_catalog.system.expire_snapshots('local.time_travel_lab.customer_orders', retain_last => 5)\n",
      "\n",
      "-- Orphan file cleanup:\n",
      "CALL spark_catalog.system.remove_orphan_files('local.time_travel_lab.customer_orders')\n",
      "\\n‚ö†Ô∏è Note: In this demo, we keep all snapshots for learning purposes\n",
      "\\nüí° Production Recommendations:\n",
      "‚úì Set up automated snapshot cleanup jobs\n",
      "‚úì Define retention policies based on business needs\n",
      "‚úì Monitor storage growth and costs\n",
      "‚úì Test recovery procedures regularly\n",
      "‚úì Document time travel usage patterns\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate snapshot cleanup (expire old snapshots)\n",
    "print(\"üßπ SNAPSHOT CLEANUP DEMONSTRATION\")\n",
    "print(\"\\\\nüì∏ Before cleanup - All snapshots:\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    snapshot_id,\n",
    "    committed_at,\n",
    "    operation\n",
    "FROM local.time_travel_lab.customer_orders.snapshots \n",
    "ORDER BY committed_at\n",
    "\"\"\").show()\n",
    "\n",
    "# In production, you would expire old snapshots like this:\n",
    "print(\"\\\\nüßπ Snapshot Cleanup Commands (for reference):\")\n",
    "cleanup_commands = [\n",
    "    \"-- Expire snapshots older than 7 days:\",\n",
    "    \"CALL spark_catalog.system.expire_snapshots('local.time_travel_lab.customer_orders', TIMESTAMP '2024-01-08 00:00:00')\",\n",
    "    \"\",\n",
    "    \"-- Keep only last 5 snapshots:\",\n",
    "    \"CALL spark_catalog.system.expire_snapshots('local.time_travel_lab.customer_orders', retain_last => 5)\",\n",
    "    \"\",\n",
    "    \"-- Orphan file cleanup:\",\n",
    "    \"CALL spark_catalog.system.remove_orphan_files('local.time_travel_lab.customer_orders')\"\n",
    "]\n",
    "\n",
    "for cmd in cleanup_commands:\n",
    "    print(cmd)\n",
    "\n",
    "print(\"\\\\n‚ö†Ô∏è Note: In this demo, we keep all snapshots for learning purposes\")\n",
    "print(\"\\\\nüí° Production Recommendations:\")\n",
    "prod_recommendations = [\n",
    "    \"Set up automated snapshot cleanup jobs\",\n",
    "    \"Define retention policies based on business needs\",\n",
    "    \"Monitor storage growth and costs\",\n",
    "    \"Test recovery procedures regularly\",\n",
    "    \"Document time travel usage patterns\"\n",
    "]\n",
    "\n",
    "for rec in prod_recommendations:\n",
    "    print(f\"‚úì {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. üéâ Summary and Best Practices\n",
    "\n",
    "Time travel tutorial summary and key takeaways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üéâ TIME TRAVEL TUTORIAL COMPLETE!**\n",
    "\n",
    "‚úÖ What You've Learned:  \n",
    "   1. Time travel fundamentals and snapshot concepts\n",
    "   2. Multiple ways to query historical data (snapshot ID, timestamp)\n",
    "   3. Rollback operations for data recovery\n",
    "   4. Schema evolution compatibility with time travel\n",
    "   5. Real-world analytical use cases\n",
    "   6. Performance optimization techniques\n",
    "   7. Snapshot management and cleanup procedures  \n",
    "üí° TIME TRAVEL BEST PRACTICES:  \n",
    "üîç Querying:  \n",
    "   ‚Ä¢ Use snapshot IDs for better performance when possible\n",
    "   ‚Ä¢ Combine time travel with partition pruning\n",
    "   ‚Ä¢ Cache frequently accessed historical data\n",
    "   ‚Ä¢ Use projection pushdown to minimize data scanning  \n",
    "üóÑÔ∏è Snapshot Management:  \n",
    "   ‚Ä¢ Define clear retention policies\n",
    "   ‚Ä¢ Automate snapshot cleanup procedures\n",
    "   ‚Ä¢ Monitor metadata growth over time\n",
    "   ‚Ä¢ Balance history needs with storage costs  \n",
    "üö® Recovery Planning:  \n",
    "   ‚Ä¢ Document recovery procedures\n",
    "   ‚Ä¢ Test rollback operations regularly\n",
    "   ‚Ä¢ Monitor table history for anomalies\n",
    "   ‚Ä¢ Establish RTO/RPO requirements  \n",
    "‚ö° Performance:  \n",
    "   ‚Ä¢ Keep metadata size reasonable\n",
    "   ‚Ä¢ Use appropriate file sizes\n",
    "   ‚Ä¢ Consider compaction strategies\n",
    "   ‚Ä¢ Monitor query performance over time  \n",
    "üöÄ Next Steps:  \n",
    "   ‚Üí Practice with larger datasets\n",
    "   ‚Üí Implement automated retention policies\n",
    "   ‚Üí Explore advanced analytical patterns\n",
    "   ‚Üí Integrate with your data pipeline  \n",
    "üéØ Key Takeaway:  \n",
    "   Time travel in Iceberg provides powerful capabilities for\n",
    "   data recovery, auditing, and historical analysis with\n",
    "   minimal performance and storage overhead!  \n",
    "üßπ Cleaning up demo environment...  \n",
    "‚úÖ Tutorial complete! Environment ready for your experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
