{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary\n",
    "print(\"üéâ TIME TRAVEL TUTORIAL COMPLETE!\")\n",
    "print(\"\\\\n‚úÖ What You've Learned:\")\n",
    "\n",
    "accomplishments = [\n",
    "    \"Time travel fundamentals and snapshot concepts\",\n",
    "    \"Multiple ways to query historical data (snapshot ID, timestamp)\",\n",
    "    \"Rollback operations for data recovery\",\n",
    "    \"Schema evolution compatibility with time travel\",\n",
    "    \"Real-world analytical use cases\",\n",
    "    \"Performance optimization techniques\",\n",
    "    \"Snapshot management and cleanup procedures\"\n",
    "]\n",
    "\n",
    "for i, accomplishment in enumerate(accomplishments, 1):\n",
    "    print(f\"   {i}. {accomplishment}\")\n",
    "\n",
    "print(\"\\\\nüí° TIME TRAVEL BEST PRACTICES:\")\n",
    "\n",
    "best_practices = {\n",
    "    \"üîç Querying\": [\n",
    "        \"Use snapshot IDs for better performance when possible\",\n",
    "        \"Combine time travel with partition pruning\",\n",
    "        \"Cache frequently accessed historical data\",\n",
    "        \"Use projection pushdown to minimize data scanning\"\n",
    "    ],\n",
    "    \"üóÑÔ∏è Snapshot Management\": [\n",
    "        \"Define clear retention policies\",\n",
    "        \"Automate snapshot cleanup procedures\", \n",
    "        \"Monitor metadata growth over time\",\n",
    "        \"Balance history needs with storage costs\"\n",
    "    ],\n",
    "    \"üö® Recovery Planning\": [\n",
    "        \"Document recovery procedures\",\n",
    "        \"Test rollback operations regularly\",\n",
    "        \"Monitor table history for anomalies\",\n",
    "        \"Establish RTO/RPO requirements\"\n",
    "    ],\n",
    "    \"‚ö° Performance\": [\n",
    "        \"Keep metadata size reasonable\",\n",
    "        \"Use appropriate file sizes\",\n",
    "        \"Consider compaction strategies\",\n",
    "        \"Monitor query performance over time\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, practices in best_practices.items():\n",
    "    print(f\"\\\\n{category}:\")\n",
    "    for practice in practices:\n",
    "        print(f\"   ‚Ä¢ {practice}\")\n",
    "\n",
    "print(\"\\\\nüöÄ Next Steps:\")\n",
    "next_steps = [\n",
    "    \"Practice with larger datasets\",\n",
    "    \"Implement automated retention policies\",\n",
    "    \"Explore advanced analytical patterns\",\n",
    "    \"Integrate with your data pipeline\"\n",
    "]\n",
    "\n",
    "for step in next_steps:\n",
    "    print(f\"   ‚Üí {step}\")\n",
    "\n",
    "print(\"\\\\nüéØ Key Takeaway:\")\n",
    "print(\"   Time travel in Iceberg provides powerful capabilities for\")\n",
    "print(\"   data recovery, auditing, and historical analysis with\")\n",
    "print(\"   minimal performance and storage overhead!\")\n",
    "\n",
    "# Clean up\n",
    "print(\"\\\\nüßπ Cleaning up demo environment...\")\n",
    "print(\"‚úÖ Tutorial complete! Environment ready for your experiments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. üéâ Summary and Best Practices\n",
    "\n",
    "Time travel tutorial summary and key takeaways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate snapshot cleanup (expire old snapshots)\n",
    "print(\"üßπ SNAPSHOT CLEANUP DEMONSTRATION\")\n",
    "print(\"\\\\nüì∏ Before cleanup - All snapshots:\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    snapshot_id,\n",
    "    committed_at,\n",
    "    operation\n",
    "FROM time_travel_lab.customer_orders.snapshots \n",
    "ORDER BY committed_at\n",
    "\"\"\").show()\n",
    "\n",
    "# In production, you would expire old snapshots like this:\n",
    "print(\"\\\\nüßπ Snapshot Cleanup Commands (for reference):\")\n",
    "cleanup_commands = [\n",
    "    \"-- Expire snapshots older than 7 days:\",\n",
    "    \"CALL spark_catalog.system.expire_snapshots('time_travel_lab.customer_orders', TIMESTAMP '2024-01-08 00:00:00')\",\n",
    "    \"\",\n",
    "    \"-- Keep only last 5 snapshots:\",\n",
    "    \"CALL spark_catalog.system.expire_snapshots('time_travel_lab.customer_orders', retain_last => 5)\",\n",
    "    \"\",\n",
    "    \"-- Orphan file cleanup:\",\n",
    "    \"CALL spark_catalog.system.remove_orphan_files('time_travel_lab.customer_orders')\"\n",
    "]\n",
    "\n",
    "for cmd in cleanup_commands:\n",
    "    print(cmd)\n",
    "\n",
    "print(\"\\\\n‚ö†Ô∏è Note: In this demo, we keep all snapshots for learning purposes\")\n",
    "print(\"\\\\nüí° Production Recommendations:\")\n",
    "prod_recommendations = [\n",
    "    \"Set up automated snapshot cleanup jobs\",\n",
    "    \"Define retention policies based on business needs\",\n",
    "    \"Monitor storage growth and costs\",\n",
    "    \"Test recovery procedures regularly\",\n",
    "    \"Document time travel usage patterns\"\n",
    "]\n",
    "\n",
    "for rec in prod_recommendations:\n",
    "    print(f\"‚úì {rec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Analysis\n",
    "print(\"‚ö° TIME TRAVEL PERFORMANCE CONSIDERATIONS\")\n",
    "\n",
    "# 1. Snapshot Retention\n",
    "print(\"\\\\nüì∏ 1. SNAPSHOT MANAGEMENT:\")\n",
    "print(\"Current snapshot count:\")\n",
    "snapshot_count = spark.sql(\"SELECT COUNT(*) as count FROM time_travel_lab.customer_orders.snapshots\").collect()[0]['count']\n",
    "print(f\"Total snapshots: {snapshot_count}\")\n",
    "\n",
    "print(\"\\\\nüí° Snapshot Retention Best Practices:\")\n",
    "retention_tips = [\n",
    "    \"Keep only necessary snapshots for time travel\",\n",
    "    \"Use expire_snapshots procedure to clean old snapshots\", \n",
    "    \"Balance between history needs and metadata overhead\",\n",
    "    \"Consider business and compliance requirements\",\n",
    "    \"Monitor metadata size vs data size ratio\"\n",
    "]\n",
    "\n",
    "for tip in retention_tips:\n",
    "    print(f\"‚úì {tip}\")\n",
    "\n",
    "# 2. Query Performance Tips\n",
    "print(\"\\\\nüöÄ 2. QUERY PERFORMANCE TIPS:\")\n",
    "performance_tips = [\n",
    "    \"Use snapshot IDs instead of timestamps when possible\",\n",
    "    \"Snapshot queries are faster than timestamp queries\",\n",
    "    \"Combine time travel with partition pruning\",\n",
    "    \"Use projection pushdown to reduce data scanning\",\n",
    "    \"Cache frequently accessed historical snapshots\"\n",
    "]\n",
    "\n",
    "for tip in performance_tips:\n",
    "    print(f\"‚úì {tip}\")\n",
    "\n",
    "# 3. Storage Impact\n",
    "print(\"\\\\nüíæ 3. STORAGE IMPACT:\")\n",
    "files_info = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as total_files,\n",
    "    SUM(file_size_in_bytes) / 1024 / 1024 as total_size_mb,\n",
    "    AVG(file_size_in_bytes) / 1024 / 1024 as avg_file_size_mb\n",
    "FROM time_travel_lab.customer_orders.files\n",
    "\"\"\")\n",
    "\n",
    "files_info.show()\n",
    "\n",
    "print(\"Storage optimization tips:\")\n",
    "storage_tips = [\n",
    "    \"Iceberg stores only changed data, not full copies\",\n",
    "    \"Metadata overhead is minimal compared to data size\",\n",
    "    \"File-level deduplication reduces storage costs\",\n",
    "    \"Compaction helps optimize file sizes over time\"\n",
    "]\n",
    "\n",
    "for tip in storage_tips:\n",
    "    print(f\"‚úì {tip}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ‚ö° Performance Considerations\n",
    "\n",
    "Best practices for time travel performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Case 3: Data Recovery and Debugging\n",
    "print(\"üìä USE CASE 3: Data Recovery and Debugging\")\n",
    "print(\"\\\\nüîß Simulate an accidental data corruption and recovery:\")\n",
    "\n",
    "# Simulate accidental deletion\n",
    "print(\"\\\\n‚ö†Ô∏è Simulating accidental data deletion:\")\n",
    "print(\"Current data before 'accident':\")\n",
    "spark.sql(\"SELECT COUNT(*) as count FROM customer_orders\").show()\n",
    "\n",
    "# Accidentally delete all pending orders\n",
    "spark.sql(\"DELETE FROM customer_orders WHERE status = 'pending'\")\n",
    "\n",
    "print(\"\\\\nüí• After accidental deletion:\")\n",
    "remaining_data = spark.sql(\"SELECT * FROM customer_orders ORDER BY order_id\")\n",
    "remaining_data.show()\n",
    "print(f\"Records remaining: {remaining_data.count()}\")\n",
    "\n",
    "# Recovery using time travel\n",
    "print(\"\\\\nüö® EMERGENCY RECOVERY PROCEDURE:\")\n",
    "print(\"1. Identify the problem\")\n",
    "print(\"2. Find the last good snapshot\")\n",
    "print(\"3. Rollback to restore data\")\n",
    "\n",
    "# Find the snapshot before deletion\n",
    "recovery_snapshots = spark.sql(\"\"\"\n",
    "SELECT snapshot_id, committed_at, operation\n",
    "FROM time_travel_lab.customer_orders.snapshots \n",
    "ORDER BY committed_at DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\\\nüì∏ Available snapshots for recovery:\")\n",
    "recovery_snapshots.show()\n",
    "\n",
    "# Get the snapshot before the DELETE operation\n",
    "snapshots_list = recovery_snapshots.collect()\n",
    "if len(snapshots_list) >= 2:\n",
    "    # Second most recent (before the DELETE)\n",
    "    recovery_snapshot_id = snapshots_list[1]['snapshot_id']\n",
    "    print(f\"\\\\nüîÑ Recovering to snapshot: {recovery_snapshot_id}\")\n",
    "    \n",
    "    # Rollback to recover\n",
    "    spark.sql(f\"CALL spark_catalog.system.rollback_to_snapshot('time_travel_lab.customer_orders', {recovery_snapshot_id})\")\n",
    "    \n",
    "    print(\"\\\\n‚úÖ DATA RECOVERED!\")\n",
    "    recovered_data = spark.sql(\"SELECT * FROM customer_orders ORDER BY order_id\")\n",
    "    recovered_data.show()\n",
    "    print(f\"Records after recovery: {recovered_data.count()}\")\n",
    "\n",
    "print(\"\\\\nüí° Recovery Benefits:\")\n",
    "print(\"‚úì Instant recovery from any point in time\")\n",
    "print(\"‚úì No need for external backups\")\n",
    "print(\"‚úì Minimal downtime\")\n",
    "print(\"‚úì Granular recovery options\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Case 2: Change Data Capture (CDC) Analysis\n",
    "print(\"üìä USE CASE 2: Change Data Capture Analysis\")\n",
    "print(\"\\\\nüîç Analyzing what changed between snapshots:\")\n",
    "\n",
    "if len(all_snapshots) >= 2:\n",
    "    snapshot1_id = all_snapshots[0]['snapshot_id']\n",
    "    snapshot2_id = all_snapshots[1]['snapshot_id']\n",
    "    \n",
    "    print(f\"\\\\nüì∏ Comparing Snapshot {snapshot1_id} vs {snapshot2_id}\")\n",
    "    \n",
    "    # Data at first snapshot\n",
    "    data1 = spark.sql(f\"\"\"\n",
    "    SELECT order_id, status, 'snapshot1' as source\n",
    "    FROM time_travel_lab.customer_orders\n",
    "    VERSION AS OF {snapshot1_id}\n",
    "    \"\"\")\n",
    "    \n",
    "    # Data at second snapshot  \n",
    "    data2 = spark.sql(f\"\"\"\n",
    "    SELECT order_id, status, 'snapshot2' as source\n",
    "    FROM time_travel_lab.customer_orders\n",
    "    VERSION AS OF {snapshot2_id}\n",
    "    \"\"\")\n",
    "    \n",
    "    # Find changes\n",
    "    print(\"\\\\nüîÑ Status changes between snapshots:\")\n",
    "    changes = spark.sql(f\"\"\"\n",
    "    WITH snapshot1 AS (\n",
    "        SELECT order_id, status as status1\n",
    "        FROM time_travel_lab.customer_orders\n",
    "        VERSION AS OF {snapshot1_id}\n",
    "    ),\n",
    "    snapshot2 AS (\n",
    "        SELECT order_id, status as status2\n",
    "        FROM time_travel_lab.customer_orders\n",
    "        VERSION AS OF {snapshot2_id}\n",
    "    )\n",
    "    SELECT \n",
    "        s1.order_id,\n",
    "        s1.status1 as old_status,\n",
    "        s2.status2 as new_status,\n",
    "        'Status Changed' as change_type\n",
    "    FROM snapshot1 s1\n",
    "    JOIN snapshot2 s2 ON s1.order_id = s2.order_id\n",
    "    WHERE s1.status1 != s2.status2\n",
    "    \"\"\")\n",
    "    \n",
    "    changes.show()\n",
    "\n",
    "print(\"\\\\nüí° CDC Use Cases:\")\n",
    "print(\"‚úì Track data lineage and audit changes\")\n",
    "print(\"‚úì Build change streams for downstream systems\")\n",
    "print(\"‚úì Identify data quality issues\")\n",
    "print(\"‚úì Monitor business process changes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Case 1: Point-in-time reporting\n",
    "print(\"üìä USE CASE 1: Point-in-time Reporting\")\n",
    "print(\"\\\\nüìÖ Generate daily snapshots for reporting:\")\n",
    "\n",
    "# Simulate daily reporting snapshots\n",
    "for i, snapshot in enumerate(all_snapshots[:3], 1):\n",
    "    snapshot_id = snapshot['snapshot_id']\n",
    "    snapshot_time = snapshot['committed_at']\n",
    "    \n",
    "    print(f\"\\\\nüìä Daily Report {i} - {snapshot_time}\")\n",
    "    \n",
    "    daily_report = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_orders,\n",
    "        SUM(quantity * unit_price) as total_revenue,\n",
    "        COUNT(DISTINCT customer_id) as unique_customers,\n",
    "        AVG(quantity * unit_price) as avg_order_value\n",
    "    FROM time_travel_lab.customer_orders\n",
    "    VERSION AS OF {snapshot_id}\n",
    "    \"\"\")\n",
    "    \n",
    "    daily_report.show()\n",
    "\n",
    "print(\"\\\\nüí° Benefits:\")\n",
    "print(\"‚úì Consistent reporting across time\")\n",
    "print(\"‚úì Audit trail for financial reports\") \n",
    "print(\"‚úì Compare metrics across different time periods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. üìä Analytical Use Cases\n",
    "\n",
    "Real-world analytical scenarios using time travel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time travel with schema evolution\n",
    "print(\"‚è∞ TIME TRAVEL WITH SCHEMA EVOLUTION\")\n",
    "\n",
    "# Query old snapshot - schema compatibility\n",
    "if len(snapshot_ids) > 0:\n",
    "    print(f\"\\\\nüì∏ Querying old snapshot {snapshot_ids[0]} (before schema change):\")\n",
    "    old_data_query = f\"\"\"\n",
    "    SELECT * FROM time_travel_lab.customer_orders\n",
    "    VERSION AS OF {snapshot_ids[0]}\n",
    "    ORDER BY order_id\n",
    "    \"\"\"\n",
    "    old_data = spark.sql(old_data_query) \n",
    "    old_data.show()\n",
    "    \n",
    "    print(\"\\\\nüí° Notice:\")\n",
    "    print(\"‚úì Old snapshots show NULL for new columns\")\n",
    "    print(\"‚úì Schema evolution is backward compatible\")\n",
    "    print(\"‚úì You can query any historical snapshot regardless of schema changes\")\n",
    "\n",
    "# Show the evolution of the schema over time\n",
    "print(\"\\\\nüìã SCHEMA EVOLUTION HISTORY:\")\n",
    "print(\"1. Original: order_id, customer_id, product_name, quantity, unit_price, order_date, status\")\n",
    "print(\"2. Current: + customer_email (added later)\")\n",
    "print(\"\\\\n‚úÖ Time travel works seamlessly across schema versions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column to demonstrate schema evolution\n",
    "print(\"üîß SCHEMA EVOLUTION DEMO\")\n",
    "print(\"\\\\nüìã Current schema:\")\n",
    "spark.sql(\"DESCRIBE customer_orders\").show()\n",
    "\n",
    "# Add a new column\n",
    "print(\"\\\\n‚ûï Adding new column 'customer_email':\")\n",
    "spark.sql(\"ALTER TABLE customer_orders ADD COLUMN customer_email string\")\n",
    "\n",
    "print(\"\\\\nüìã New schema:\")\n",
    "spark.sql(\"DESCRIBE customer_orders\").show()\n",
    "\n",
    "# Insert data with the new column\n",
    "print(\"\\\\nüìù Inserting data with new column:\")\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO customer_orders VALUES\n",
    "    (1007, 106, 'Headphones', 1, 199.99, DATE '2024-01-18', 'pending', 'customer106@email.com')\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\\\nüìä Current data with new column:\")\n",
    "spark.sql(\"SELECT * FROM customer_orders ORDER BY order_id\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. üîß Schema Evolution with Time Travel\n",
    "\n",
    "Learn how time travel works with schema changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View snapshots after rollback\n",
    "print(\"üì∏ SNAPSHOTS AFTER ROLLBACK:\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    snapshot_id,\n",
    "    committed_at,\n",
    "    operation,\n",
    "    summary\n",
    "FROM time_travel_lab.customer_orders.snapshots \n",
    "ORDER BY committed_at\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "print(\"\\\\nüí° Key Points about Rollback:\")\n",
    "print(\"‚úì Rollback creates a new snapshot\")\n",
    "print(\"‚úì Original snapshots are still preserved\") \n",
    "print(\"‚úì You can still time-travel to any historical state\")\n",
    "print(\"‚úì Rollback is metadata operation - very fast\")\n",
    "print(\"‚úì No data files are actually deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rollback to the second snapshot (after updates but before new orders)\n",
    "if len(snapshot_ids) >= 2:\n",
    "    rollback_snapshot_id = snapshot_ids[1]  # Second snapshot\n",
    "    print(f\"üîÑ Rolling back to snapshot: {rollback_snapshot_id}\")\n",
    "    \n",
    "    rollback_query = f\"\"\"\n",
    "    CALL spark_catalog.system.rollback_to_snapshot('time_travel_lab.customer_orders', {rollback_snapshot_id})\n",
    "    \"\"\"\n",
    "    \n",
    "    spark.sql(rollback_query)\n",
    "    print(\"‚úÖ Rollback completed!\")\n",
    "    \n",
    "    # Show data after rollback\n",
    "    print(\"\\\\nüìä DATA AFTER ROLLBACK:\")\n",
    "    rollback_data = spark.sql(\"SELECT * FROM customer_orders ORDER BY order_id\")\n",
    "    rollback_data.show()\n",
    "    print(f\"Record count after rollback: {rollback_data.count()}\")\n",
    "    \n",
    "    # Compare with what we expected\n",
    "    print(\"\\\\nüí° Notice:\")\n",
    "    print(\"- The new orders (1004-1006) from Day 3 are gone\")\n",
    "    print(\"- We're back to the state after Day 2 updates\")\n",
    "    print(\"- Order 1001 status is 'shipped' (not 'pending')\")\n",
    "    print(\"- Order 1002 status is 'delivered' (not 'shipped')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show current state before rollback\n",
    "print(\"üìä CURRENT STATE (before rollback):\")\n",
    "current_data = spark.sql(\"SELECT * FROM customer_orders ORDER BY order_id\")\n",
    "current_data.show()\n",
    "print(f\"Current record count: {current_data.count()}\")\n",
    "\n",
    "# Show available snapshots\n",
    "print(\"\\\\nüì∏ Available snapshots for rollback:\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    snapshot_id,\n",
    "    committed_at,\n",
    "    operation,\n",
    "    summary\n",
    "FROM time_travel_lab.customer_orders.snapshots \n",
    "ORDER BY committed_at\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üîÑ Rollback Operations\n",
    "\n",
    "Learn how to rollback tables to previous states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Query by Timestamp\n",
    "print(\"üîç METHOD 2: Query by Timestamp\")\n",
    "\n",
    "# Get timestamp from second snapshot\n",
    "if len(all_snapshots) >= 2:\n",
    "    second_timestamp = all_snapshots[1]['committed_at']\n",
    "    print(f\"\\\\nüìÖ Data as of timestamp: {second_timestamp}\")\n",
    "    \n",
    "    timestamp_query = f\"\"\"\n",
    "    SELECT * FROM time_travel_lab.customer_orders\n",
    "    TIMESTAMP AS OF '{second_timestamp}'\n",
    "    ORDER BY order_id\n",
    "    \"\"\"\n",
    "    spark.sql(timestamp_query).show()\n",
    "    \n",
    "    print(f\"üìä Count at that timestamp: {spark.sql(f'''SELECT COUNT(*) as count FROM time_travel_lab.customer_orders TIMESTAMP AS OF '{second_timestamp}' ''').collect()[0]['count']}\")\n",
    "\n",
    "# Method 3: Query with relative time\n",
    "print(\"\\\\nüîç METHOD 3: Query with system functions\")\n",
    "print(\"\\\\nüìÖ Data from 5 minutes ago (if available):\")\n",
    "\n",
    "# This would work in a real scenario with longer time gaps\n",
    "relative_time_query = \"\"\"\n",
    "SELECT * FROM time_travel_lab.customer_orders\n",
    "TIMESTAMP AS OF CURRENT_TIMESTAMP() - INTERVAL 5 MINUTES\n",
    "ORDER BY order_id\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    spark.sql(relative_time_query).show()\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Note: {str(e)}\")\n",
    "    print(\"üí° This is expected in our demo due to short time intervals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Query by Snapshot ID\n",
    "print(\"üîç METHOD 1: Query by Snapshot ID\")\n",
    "print(\"\\\\nüì∏ Data at first snapshot (original orders):\")\n",
    "\n",
    "if len(snapshot_ids) > 0:\n",
    "    first_snapshot_query = f\"\"\"\n",
    "    SELECT * FROM time_travel_lab.customer_orders\n",
    "    VERSION AS OF {snapshot_ids[0]}\n",
    "    ORDER BY order_id\n",
    "    \"\"\"\n",
    "    spark.sql(first_snapshot_query).show()\n",
    "    \n",
    "    print(f\"üìä Count at first snapshot: {spark.sql(f'SELECT COUNT(*) as count FROM time_travel_lab.customer_orders VERSION AS OF {snapshot_ids[0]}').collect()[0]['count']}\")\n",
    "\n",
    "# Compare with current data\n",
    "print(\"\\\\nüìä Current data for comparison:\")\n",
    "spark.sql(\"SELECT * FROM customer_orders ORDER BY order_id\").show()\n",
    "print(f\"üìä Current count: {spark.sql('SELECT COUNT(*) as count FROM customer_orders').collect()[0]['count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ‚è∞ Time Travel Queries\n",
    "\n",
    "Learn different ways to query historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View table history with more details\n",
    "print(\"üìã DETAILED TABLE HISTORY:\")\n",
    "history_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    made_current_at,\n",
    "    snapshot_id,\n",
    "    parent_id,\n",
    "    is_current_ancestor\n",
    "FROM time_travel_lab.customer_orders.history\n",
    "ORDER BY made_current_at\n",
    "\"\"\")\n",
    "\n",
    "history_df.show()\n",
    "\n",
    "# View files for each snapshot\n",
    "print(\"\\nüìÅ FILES PER SNAPSHOT:\")\n",
    "files_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    snapshot_id,\n",
    "    file_path,\n",
    "    file_size_in_bytes,\n",
    "    record_count\n",
    "FROM time_travel_lab.customer_orders.files\n",
    "\"\"\")\n",
    "\n",
    "files_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View all snapshots\n",
    "print(\"üì∏ ALL SNAPSHOTS IN ORDER:\")\n",
    "snapshots_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    snapshot_id,\n",
    "    committed_at,\n",
    "    operation,\n",
    "    summary\n",
    "FROM time_travel_lab.customer_orders.snapshots \n",
    "ORDER BY committed_at\n",
    "\"\"\")\n",
    "\n",
    "snapshots_df.show(truncate=False)\n",
    "\n",
    "print(f\"\\nüìä Total snapshots: {snapshots_df.count()}\")\n",
    "\n",
    "# Store snapshot IDs for later use\n",
    "all_snapshots = snapshots_df.collect()\n",
    "snapshot_ids = [row['snapshot_id'] for row in all_snapshots]\n",
    "print(f\"üî¢ Snapshot IDs: {snapshot_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üì∏ Snapshot Management\n",
    "\n",
    "Learn how to view and manage table snapshots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(2)\n",
    "\n",
    "# Add new orders - Day 3\n",
    "print(\"üìÖ Day 3: New orders added\")\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO customer_orders VALUES\n",
    "    (1004, 104, 'Monitor 4K', 1, 499.99, DATE '2024-01-17', 'pending'),\n",
    "    (1005, 105, 'Webcam HD', 1, 79.99, DATE '2024-01-17', 'pending'),\n",
    "    (1006, 101, 'USB-C Hub', 1, 49.99, DATE '2024-01-17', 'shipped')\n",
    "\"\"\")\n",
    "\n",
    "# Store third snapshot info\n",
    "third_snapshot = spark.sql(\"SELECT snapshot_id, committed_at FROM time_travel_lab.customer_orders.snapshots ORDER BY committed_at DESC LIMIT 1\").collect()[0]\n",
    "print(f\"üì∏ Snapshot 3 ID: {third_snapshot['snapshot_id']}\")\n",
    "print(f\"üìÖ Snapshot 3 Time: {third_snapshot['committed_at']}\")\n",
    "\n",
    "print(\"\\nüìä Current data (latest):\")\n",
    "spark.sql(\"SELECT * FROM customer_orders ORDER BY order_id\").show()\n",
    "\n",
    "print(f\"\\nüìà Total orders now: {spark.sql('SELECT COUNT(*) as count FROM customer_orders').collect()[0]['count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a small delay to ensure different timestamps\n",
    "time.sleep(2)\n",
    "\n",
    "# Update some orders - Day 2\n",
    "print(\"üìÖ Day 2: Order status updates\")\n",
    "spark.sql(\"\"\"\n",
    "UPDATE customer_orders \n",
    "SET status = 'shipped' \n",
    "WHERE order_id = 1001\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "UPDATE customer_orders \n",
    "SET status = 'delivered' \n",
    "WHERE order_id = 1002\n",
    "\"\"\")\n",
    "\n",
    "# Store second snapshot info\n",
    "second_snapshot = spark.sql(\"SELECT snapshot_id, committed_at FROM time_travel_lab.customer_orders.snapshots ORDER BY committed_at DESC LIMIT 1\").collect()[0]\n",
    "print(f\"üì∏ Snapshot 2 ID: {second_snapshot['snapshot_id']}\")\n",
    "print(f\"üìÖ Snapshot 2 Time: {second_snapshot['committed_at']}\")\n",
    "\n",
    "print(\"\\nüìä Updated data:\")\n",
    "spark.sql(\"SELECT * FROM customer_orders ORDER BY order_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert initial data - Day 1\n",
    "print(\"üìÖ Day 1: Initial orders\")\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO customer_orders VALUES\n",
    "    (1001, 101, 'Laptop Pro', 1, 1299.99, DATE '2024-01-15', 'pending'),\n",
    "    (1002, 102, 'Wireless Mouse', 2, 29.99, DATE '2024-01-15', 'shipped'),\n",
    "    (1003, 103, 'Keyboard', 1, 89.99, DATE '2024-01-15', 'pending')\n",
    "\"\"\")\n",
    "\n",
    "# Store first snapshot info\n",
    "first_snapshot = spark.sql(\"SELECT snapshot_id, committed_at FROM time_travel_lab.customer_orders.snapshots ORDER BY committed_at LIMIT 1\").collect()[0]\n",
    "print(f\"üì∏ Snapshot 1 ID: {first_snapshot['snapshot_id']}\")\n",
    "print(f\"üìÖ Snapshot 1 Time: {first_snapshot['committed_at']}\")\n",
    "\n",
    "print(\"\\nüìä Current data:\")\n",
    "spark.sql(\"SELECT * FROM customer_orders ORDER BY order_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create database for time travel demo\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS time_travel_lab\")\n",
    "spark.sql(\"USE time_travel_lab\")\n",
    "\n",
    "# Drop table if exists (for clean demo)\n",
    "spark.sql(\"DROP TABLE IF EXISTS customer_orders\")\n",
    "\n",
    "# Create sample table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE customer_orders (\n",
    "    order_id bigint,\n",
    "    customer_id bigint,\n",
    "    product_name string,\n",
    "    quantity int,\n",
    "    unit_price decimal(10,2),\n",
    "    order_date date,\n",
    "    status string\n",
    ") USING ICEBERG\n",
    "PARTITIONED BY (days(order_date))\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Created customer_orders table\")\n",
    "print(\"üìä Table schema:\")\n",
    "spark.sql(\"DESCRIBE customer_orders\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üóÑÔ∏è Create Sample Data for Time Travel\n",
    "\n",
    "Create a sample table and insert data over time to demonstrate time travel capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "# Set Python path for Spark consistency\n",
    "os.environ['PYSPARK_PYTHON'] = '/opt/conda/bin/python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/opt/conda/bin/python'\n",
    "\n",
    "# Initialize Spark with Iceberg\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergTimeTravel\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.warehouse\", \"/opt/spark/work-dir/warehouse\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to reduce noise\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"‚è∞ Time Travel Tutorial Environment Ready!\")\n",
    "print(f\"üìÖ Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"‚úÖ Spark session initialized with Iceberg support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚è∞ Apache Iceberg Time Travel Tutorial\n",
    "\n",
    "Welcome to the comprehensive Time Travel tutorial! In this notebook, you'll learn:\n",
    "\n",
    "1. **Time Travel Fundamentals**\n",
    "2. **Reading Historical Data**\n",
    "3. **Snapshot Management**\n",
    "4. **Rollback Operations**\n",
    "5. **Schema Evolution with Time Travel**\n",
    "6. **Performance Considerations**\n",
    "7. **Real-world Use Cases**\n",
    "\n",
    "## üìã Prerequisites\n",
    "\n",
    "- Completed the basic Iceberg tutorial\n",
    "- Understanding of Iceberg table concepts\n",
    "- Basic knowledge of Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üöÄ Initialize Environment\n",
    "\n",
    "Set up Spark with Iceberg for time travel operations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
