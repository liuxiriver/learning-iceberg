{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚òÅÔ∏è Apache Iceberg Cloud Integration Tutorial\n",
    "\n",
    "Welcome to the comprehensive Cloud Integration tutorial! In this notebook, you'll learn:\n",
    "\n",
    "1. **Cloud Storage Integration**\n",
    "2. **AWS S3 + Glue Configuration**\n",
    "3. **Azure ADLS + Synapse Setup**\n",
    "4. **GCP Cloud Storage + BigQuery**\n",
    "5. **Multi-Cloud Strategies**\n",
    "6. **Security and Permissions**\n",
    "7. **Performance Optimization**\n",
    "\n",
    "## üìã Prerequisites\n",
    "\n",
    "- Basic understanding of Iceberg concepts\n",
    "- Cloud account access (AWS/Azure/GCP)\n",
    "- Understanding of cloud storage concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üöÄ Initialize Environment\n",
    "\n",
    "Set up Spark with cloud-ready Iceberg configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "import json\n",
    "\n",
    "# Set Python path for Spark consistency\n",
    "os.environ['PYSPARK_PYTHON'] = '/opt/conda/bin/python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/opt/conda/bin/python'\n",
    "\n",
    "print(\"‚òÅÔ∏è Cloud Integration Tutorial Environment Setup\")\n",
    "print(\"‚ÑπÔ∏è This tutorial demonstrates cloud integration patterns\")\n",
    "print(\"‚ÑπÔ∏è Some examples require actual cloud credentials to execute\")\n",
    "print(\"‚ÑπÔ∏è We'll provide configuration examples and best practices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üîß Cloud Configuration Patterns\n",
    "\n",
    "Learn the different ways to configure Iceberg for cloud environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud Configuration Examples (for reference)\n",
    "print(\"‚òÅÔ∏è CLOUD CONFIGURATION PATTERNS\")\n",
    "print(\"\\nüìù Configuration patterns for different cloud providers:\")\n",
    "\n",
    "# Helper function to show configuration\n",
    "def show_cloud_config(provider, config_dict):\n",
    "    print(f\"\\nüåê {provider} Configuration:\")\n",
    "    print(\"```python\")\n",
    "    for key, value in config_dict.items():\n",
    "        print(f\".config('{key}', '{value}')\")\n",
    "    print(\"```\")\n",
    "\n",
    "# AWS S3 Configuration\n",
    "aws_config = {\n",
    "    \"spark.sql.catalog.prod\": \"org.apache.iceberg.spark.SparkCatalog\",\n",
    "    \"spark.sql.catalog.prod.type\": \"glue\",\n",
    "    \"spark.sql.catalog.prod.warehouse\": \"s3a://my-iceberg-warehouse/\",\n",
    "    \"spark.sql.catalog.prod.io-impl\": \"org.apache.iceberg.aws.s3.S3FileIO\",\n",
    "    \"spark.hadoop.fs.s3a.access.key\": \"YOUR_ACCESS_KEY\",\n",
    "    \"spark.hadoop.fs.s3a.secret.key\": \"YOUR_SECRET_KEY\",\n",
    "    \"spark.hadoop.fs.s3a.region\": \"us-west-2\"\n",
    "}\n",
    "\n",
    "show_cloud_config(\"AWS S3 + Glue\", aws_config)\n",
    "\n",
    "# Azure ADLS Configuration\n",
    "azure_config = {\n",
    "    \"spark.sql.catalog.prod\": \"org.apache.iceberg.spark.SparkCatalog\",\n",
    "    \"spark.sql.catalog.prod.type\": \"hadoop\",\n",
    "    \"spark.sql.catalog.prod.warehouse\": \"abfss://container@account.dfs.core.windows.net/warehouse\",\n",
    "    \"spark.hadoop.fs.azure.account.key.account.dfs.core.windows.net\": \"YOUR_ACCOUNT_KEY\",\n",
    "    \"spark.hadoop.fs.azure.account.auth.type.account.dfs.core.windows.net\": \"SharedKey\"\n",
    "}\n",
    "\n",
    "show_cloud_config(\"Azure ADLS Gen2\", azure_config)\n",
    "\n",
    "# GCP Cloud Storage Configuration\n",
    "gcp_config = {\n",
    "    \"spark.sql.catalog.prod\": \"org.apache.iceberg.spark.SparkCatalog\",\n",
    "    \"spark.sql.catalog.prod.type\": \"hadoop\",\n",
    "    \"spark.sql.catalog.prod.warehouse\": \"gs://my-iceberg-bucket/warehouse\",\n",
    "    \"spark.hadoop.google.cloud.auth.service.account.json.keyfile\": \"/path/to/service-account.json\",\n",
    "    \"spark.hadoop.fs.gs.impl\": \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\"\n",
    "}\n",
    "\n",
    "show_cloud_config(\"GCP Cloud Storage\", gcp_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üõ°Ô∏è Security and Authentication\n",
    "\n",
    "Learn cloud security best practices for Iceberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Security Best Practices\n",
    "print(\"üõ°Ô∏è CLOUD SECURITY BEST PRACTICES\")\n",
    "print(\"\\nüîê Authentication Methods:\")\n",
    "\n",
    "security_patterns = {\n",
    "    \"AWS\": {\n",
    "        \"IAM Roles (Recommended)\": [\n",
    "            \"Use IAM roles instead of access keys\",\n",
    "            \"Assign minimal required permissions\",\n",
    "            \"Enable CloudTrail for audit logging\",\n",
    "            \"Use KMS for encryption at rest\"\n",
    "        ],\n",
    "        \"S3 Permissions\": [\n",
    "            \"s3:GetObject, s3:PutObject for data files\",\n",
    "            \"s3:ListBucket for warehouse bucket\",\n",
    "            \"s3:DeleteObject for maintenance operations\"\n",
    "        ],\n",
    "        \"Glue Permissions\": [\n",
    "            \"glue:GetDatabase, glue:GetTable\",\n",
    "            \"glue:CreateTable, glue:UpdateTable\",\n",
    "            \"glue:DeleteTable (for maintenance)\"\n",
    "        ]\n",
    "    },\n",
    "    \"Azure\": {\n",
    "        \"Managed Identity (Recommended)\": [\n",
    "            \"Use system or user-assigned managed identities\",\n",
    "            \"Avoid storing connection strings in code\",\n",
    "            \"Enable Azure Monitor for logging\",\n",
    "            \"Use Azure Key Vault for secrets\"\n",
    "        ],\n",
    "        \"ADLS Permissions\": [\n",
    "            \"Storage Blob Data Contributor role\",\n",
    "            \"Storage Blob Data Reader for read-only access\",\n",
    "            \"Storage Account Contributor for management\"\n",
    "        ]\n",
    "    },\n",
    "    \"GCP\": {\n",
    "        \"Service Accounts (Recommended)\": [\n",
    "            \"Use service accounts with minimal permissions\",\n",
    "            \"Enable audit logging with Cloud Logging\",\n",
    "            \"Use Cloud KMS for encryption\",\n",
    "            \"Store keys in Secret Manager\"\n",
    "        ],\n",
    "        \"Cloud Storage Permissions\": [\n",
    "            \"Storage Object Admin for full access\",\n",
    "            \"Storage Object Viewer for read-only\",\n",
    "            \"Storage Legacy Bucket Reader for metadata\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "for cloud, categories in security_patterns.items():\n",
    "    print(f\"\\nüåê {cloud} Security:\")\n",
    "    for category, items in categories.items():\n",
    "        print(f\"  üìã {category}:\")\n",
    "        for item in items:\n",
    "            print(f\"    ‚Ä¢ {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üèóÔ∏è AWS S3 + Glue Integration\n",
    "\n",
    "Deep dive into AWS cloud integration patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Integration Examples\n",
    "print(\"üèóÔ∏è AWS S3 + GLUE INTEGRATION\")\n",
    "print(\"\\nüìù Complete AWS setup example:\")\n",
    "\n",
    "# AWS Spark Session Configuration\n",
    "aws_spark_config = \"\"\"\n",
    "# Complete AWS Spark Session Setup\n",
    "spark = SparkSession.builder \\\\\n",
    "    .appName(\"IcebergAWS\") \\\\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.3,\"\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"software.amazon.awssdk:bundle:2.20.18\") \\\\\n",
    "    .config(\"spark.sql.extensions\", \n",
    "            \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\\\n",
    "    .config(\"spark.sql.catalog.glue_catalog\", \n",
    "            \"org.apache.iceberg.spark.SparkCatalog\") \\\\\n",
    "    .config(\"spark.sql.catalog.glue_catalog.type\", \"glue\") \\\\\n",
    "    .config(\"spark.sql.catalog.glue_catalog.warehouse\", \n",
    "            \"s3a://my-iceberg-warehouse/\") \\\\\n",
    "    .config(\"spark.sql.catalog.glue_catalog.io-impl\", \n",
    "            \"org.apache.iceberg.aws.s3.S3FileIO\") \\\\\n",
    "    .config(\"spark.hadoop.fs.s3a.region\", \"us-west-2\") \\\\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \n",
    "            \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\\\n",
    "    .getOrCreate()\n",
    "\"\"\"\n",
    "\n",
    "print(aws_spark_config)\n",
    "\n",
    "# AWS IAM Policy Example\n",
    "print(\"\\nüîê AWS IAM Policy Example:\")\n",
    "iam_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:PutObject\",\n",
    "                \"s3:DeleteObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::my-iceberg-warehouse\",\n",
    "                \"arn:aws:s3:::my-iceberg-warehouse/*\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"glue:GetDatabase\",\n",
    "                \"glue:GetTable\",\n",
    "                \"glue:GetTables\",\n",
    "                \"glue:CreateTable\",\n",
    "                \"glue:UpdateTable\",\n",
    "                \"glue:DeleteTable\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(json.dumps(iam_policy, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Example Operations\n",
    "print(\"üöÄ AWS EXAMPLE OPERATIONS\")\n",
    "print(\"\\nüìù Example SQL operations with AWS Glue catalog:\")\n",
    "\n",
    "aws_operations = [\n",
    "    {\n",
    "        \"operation\": \"Create Database\",\n",
    "        \"sql\": \"CREATE DATABASE IF NOT EXISTS glue_catalog.sales_db\"\n",
    "    },\n",
    "    {\n",
    "        \"operation\": \"Create Table\",\n",
    "        \"sql\": \"\"\"\n",
    "CREATE TABLE glue_catalog.sales_db.orders (\n",
    "    order_id bigint,\n",
    "    customer_id bigint,\n",
    "    product_name string,\n",
    "    quantity int,\n",
    "    price decimal(10,2),\n",
    "    order_date date\n",
    ") USING ICEBERG\n",
    "PARTITIONED BY (bucket(16, customer_id), days(order_date))\n",
    "LOCATION 's3a://my-iceberg-warehouse/sales_db/orders'\n",
    "\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"operation\": \"Insert Data\",\n",
    "        \"sql\": \"\"\"\n",
    "INSERT INTO glue_catalog.sales_db.orders VALUES\n",
    "    (1001, 5001, 'Laptop', 1, 999.99, DATE '2024-01-15'),\n",
    "    (1002, 5002, 'Mouse', 2, 29.99, DATE '2024-01-16')\n",
    "\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"operation\": \"Query with Partition Pruning\",\n",
    "        \"sql\": \"\"\"\n",
    "SELECT * FROM glue_catalog.sales_db.orders \n",
    "WHERE order_date >= DATE '2024-01-01' \n",
    "AND customer_id = 5001\n",
    "\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for op in aws_operations:\n",
    "    print(f\"\\nüìã {op['operation']}:\")\n",
    "    print(f\"```sql\\n{op['sql'].strip()}\\n```\")\n",
    "\n",
    "print(\"\\nüí° Benefits of AWS Glue Integration:\")\n",
    "aws_benefits = [\n",
    "    \"Centralized metadata catalog\",\n",
    "    \"Integration with AWS analytics services\",\n",
    "    \"Automatic schema discovery\",\n",
    "    \"Built-in data governance features\",\n",
    "    \"Support for multiple compute engines\"\n",
    "]\n",
    "\n",
    "for benefit in aws_benefits:\n",
    "    print(f\"  ‚úÖ {benefit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üíô Azure ADLS + Synapse Integration\n",
    "\n",
    "Explore Azure cloud integration patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure Integration Examples\n",
    "print(\"üíô AZURE ADLS + SYNAPSE INTEGRATION\")\n",
    "print(\"\\nüìù Complete Azure setup example:\")\n",
    "\n",
    "# Azure Spark Session Configuration\n",
    "azure_spark_config = \"\"\"\n",
    "# Complete Azure Spark Session Setup\n",
    "spark = SparkSession.builder \\\\\n",
    "    .appName(\"IcebergAzure\") \\\\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.3,\"\n",
    "            \"org.apache.hadoop:hadoop-azure:3.3.4\") \\\\\n",
    "    .config(\"spark.sql.extensions\", \n",
    "            \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\\\n",
    "    .config(\"spark.sql.catalog.azure_catalog\", \n",
    "            \"org.apache.iceberg.spark.SparkCatalog\") \\\\\n",
    "    .config(\"spark.sql.catalog.azure_catalog.type\", \"hadoop\") \\\\\n",
    "    .config(\"spark.sql.catalog.azure_catalog.warehouse\", \n",
    "            \"abfss://warehouse@mystorageaccount.dfs.core.windows.net/iceberg\") \\\\\n",
    "    .config(\"spark.hadoop.fs.azure.account.auth.type.mystorageaccount.dfs.core.windows.net\", \n",
    "            \"OAuth\") \\\\\n",
    "    .config(\"spark.hadoop.fs.azure.account.oauth.provider.type.mystorageaccount.dfs.core.windows.net\", \n",
    "            \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\") \\\\\n",
    "    .config(\"spark.hadoop.fs.azure.account.oauth2.client.id.mystorageaccount.dfs.core.windows.net\", \n",
    "            \"your-client-id\") \\\\\n",
    "    .config(\"spark.hadoop.fs.azure.account.oauth2.client.secret.mystorageaccount.dfs.core.windows.net\", \n",
    "            \"your-client-secret\") \\\\\n",
    "    .config(\"spark.hadoop.fs.azure.account.oauth2.client.endpoint.mystorageaccount.dfs.core.windows.net\", \n",
    "            \"https://login.microsoftonline.com/your-tenant-id/oauth2/token\") \\\\\n",
    "    .getOrCreate()\n",
    "\"\"\"\n",
    "\n",
    "print(azure_spark_config)\n",
    "\n",
    "print(\"\\nüîê Azure Service Principal Setup:\")\n",
    "azure_setup_steps = [\n",
    "    \"1. Create Azure Service Principal in Azure AD\",\n",
    "    \"2. Assign 'Storage Blob Data Contributor' role to storage account\",\n",
    "    \"3. Note down: Client ID, Client Secret, Tenant ID\",\n",
    "    \"4. Create ADLS Gen2 storage account with hierarchical namespace\",\n",
    "    \"5. Configure Synapse workspace with storage account\"\n",
    "]\n",
    "\n",
    "for step in azure_setup_steps:\n",
    "    print(f\"  {step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure Example Operations\n",
    "print(\"üöÄ AZURE EXAMPLE OPERATIONS\")\n",
    "print(\"\\nüìù Example operations with Azure ADLS:\")\n",
    "\n",
    "azure_operations = [\n",
    "    {\n",
    "        \"operation\": \"Create Database\",\n",
    "        \"sql\": \"CREATE DATABASE IF NOT EXISTS azure_catalog.analytics_db\"\n",
    "    },\n",
    "    {\n",
    "        \"operation\": \"Create Table with ADLS Location\",\n",
    "        \"sql\": \"\"\"\n",
    "CREATE TABLE azure_catalog.analytics_db.customer_events (\n",
    "    event_id bigint,\n",
    "    customer_id bigint,\n",
    "    event_type string,\n",
    "    event_time timestamp,\n",
    "    properties map<string, string>\n",
    ") USING ICEBERG\n",
    "PARTITIONED BY (days(event_time), bucket(32, customer_id))\n",
    "LOCATION 'abfss://warehouse@mystorageaccount.dfs.core.windows.net/analytics_db/customer_events'\n",
    "\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"operation\": \"Synapse Analytics Integration\",\n",
    "        \"sql\": \"\"\"\n",
    "-- Query from Synapse SQL Pool\n",
    "SELECT \n",
    "    event_type,\n",
    "    COUNT(*) as event_count,\n",
    "    COUNT(DISTINCT customer_id) as unique_customers\n",
    "FROM azure_catalog.analytics_db.customer_events\n",
    "WHERE event_time >= CURRENT_DATE - INTERVAL 30 DAYS\n",
    "GROUP BY event_type\n",
    "\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for op in azure_operations:\n",
    "    print(f\"\\nüìã {op['operation']}:\")\n",
    "    print(f\"```sql\\n{op['sql'].strip()}\\n```\")\n",
    "\n",
    "print(\"\\nüí° Benefits of Azure Integration:\")\n",
    "azure_benefits = [\n",
    "    \"Native integration with Synapse Analytics\",\n",
    "    \"Azure Active Directory authentication\",\n",
    "    \"Integration with Power BI for visualization\",\n",
    "    \"Azure Monitor for observability\",\n",
    "    \"Cost-effective storage with lifecycle policies\"\n",
    "]\n",
    "\n",
    "for benefit in azure_benefits:\n",
    "    print(f\"  ‚úÖ {benefit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. üåê GCP Cloud Storage + BigQuery Integration\n",
    "\n",
    "Learn Google Cloud Platform integration patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCP Integration Examples\n",
    "print(\"üåê GCP CLOUD STORAGE + BIGQUERY INTEGRATION\")\n",
    "print(\"\\nüìù Complete GCP setup example:\")\n",
    "\n",
    "# GCP Spark Session Configuration\n",
    "gcp_spark_config = \"\"\"\n",
    "# Complete GCP Spark Session Setup\n",
    "spark = SparkSession.builder \\\\\n",
    "    .appName(\"IcebergGCP\") \\\\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.3,\"\n",
    "            \"com.google.cloud.bigdataoss:gcs-connector:hadoop3-2.2.11\") \\\\\n",
    "    .config(\"spark.sql.extensions\", \n",
    "            \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\\\n",
    "    .config(\"spark.sql.catalog.gcp_catalog\", \n",
    "            \"org.apache.iceberg.spark.SparkCatalog\") \\\\\n",
    "    .config(\"spark.sql.catalog.gcp_catalog.type\", \"hadoop\") \\\\\n",
    "    .config(\"spark.sql.catalog.gcp_catalog.warehouse\", \n",
    "            \"gs://my-iceberg-bucket/warehouse\") \\\\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \n",
    "            \"/path/to/service-account-key.json\") \\\\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \n",
    "            \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\\\n",
    "    .config(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\", \n",
    "            \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\\\n",
    "    .getOrCreate()\n",
    "\"\"\"\n",
    "\n",
    "print(gcp_spark_config)\n",
    "\n",
    "print(\"\\nüîê GCP Service Account Setup:\")\n",
    "gcp_setup_steps = [\n",
    "    \"1. Create GCP Service Account in IAM & Admin\",\n",
    "    \"2. Assign roles: Storage Admin, BigQuery Admin\",\n",
    "    \"3. Generate and download JSON key file\",\n",
    "    \"4. Create Cloud Storage bucket for warehouse\",\n",
    "    \"5. Enable BigQuery API for your project\"\n",
    "]\n",
    "\n",
    "for step in gcp_setup_steps:\n",
    "    print(f\"  {step}\")\n",
    "\n",
    "# GCP IAM Roles\n",
    "print(\"\\nüîê Required GCP IAM Roles:\")\n",
    "gcp_roles = {\n",
    "    \"Storage Object Admin\": \"Full control over Cloud Storage objects\",\n",
    "    \"Storage Legacy Bucket Reader\": \"List buckets and read bucket metadata\",\n",
    "    \"BigQuery Data Editor\": \"Create and modify BigQuery datasets/tables\",\n",
    "    \"BigQuery Job User\": \"Run BigQuery jobs\"\n",
    "}\n",
    "\n",
    "for role, description in gcp_roles.items():\n",
    "    print(f\"  üìã {role}: {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCP Example Operations\n",
    "print(\"üöÄ GCP EXAMPLE OPERATIONS\")\n",
    "print(\"\\nüìù Example operations with GCP Cloud Storage:\")\n",
    "\n",
    "gcp_operations = [\n",
    "    {\n",
    "        \"operation\": \"Create Database\",\n",
    "        \"sql\": \"CREATE DATABASE IF NOT EXISTS gcp_catalog.data_lake_db\"\n",
    "    },\n",
    "    {\n",
    "        \"operation\": \"Create Table with GCS Location\",\n",
    "        \"sql\": \"\"\"\n",
    "CREATE TABLE gcp_catalog.data_lake_db.web_analytics (\n",
    "    session_id string,\n",
    "    user_id bigint,\n",
    "    page_url string,\n",
    "    event_time timestamp,\n",
    "    user_agent string,\n",
    "    geo_location struct<country: string, city: string>\n",
    ") USING ICEBERG\n",
    "PARTITIONED BY (days(event_time), bucket(64, user_id))\n",
    "LOCATION 'gs://my-iceberg-bucket/warehouse/data_lake_db/web_analytics'\n",
    "\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"operation\": \"BigQuery External Table\",\n",
    "        \"description\": \"Create BigQuery external table to query Iceberg data\",\n",
    "        \"sql\": \"\"\"\n",
    "-- BigQuery SQL to create external table\n",
    "CREATE OR REPLACE EXTERNAL TABLE `project.dataset.web_analytics_external`\n",
    "OPTIONS (\n",
    "  format = 'ICEBERG',\n",
    "  uris = ['gs://my-iceberg-bucket/warehouse/data_lake_db/web_analytics']\n",
    ")\n",
    "\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for op in gcp_operations:\n",
    "    print(f\"\\nüìã {op['operation']}:\")\n",
    "    if 'description' in op:\n",
    "        print(f\"    {op['description']}\")\n",
    "    print(f\"```sql\\n{op['sql'].strip()}\\n```\")\n",
    "\n",
    "print(\"\\nüí° Benefits of GCP Integration:\")\n",
    "gcp_benefits = [\n",
    "    \"Integration with BigQuery for serverless analytics\",\n",
    "    \"Cost-effective Cloud Storage with lifecycle management\",\n",
    "    \"Integration with Dataflow for stream processing\",\n",
    "    \"Cloud Monitoring for observability\",\n",
    "    \"Data Studio for visualization\"\n",
    "]\n",
    "\n",
    "for benefit in gcp_benefits:\n",
    "    print(f\"  ‚úÖ {benefit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. üîÑ Multi-Cloud Strategies\n",
    "\n",
    "Learn patterns for multi-cloud and hybrid deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Cloud Patterns\n",
    "print(\"üîÑ MULTI-CLOUD STRATEGIES\")\n",
    "print(\"\\nüåê Multi-cloud deployment patterns:\")\n",
    "\n",
    "multicloud_patterns = {\n",
    "    \"üè¢ Hybrid Cloud\": {\n",
    "        \"description\": \"On-premises + one cloud provider\",\n",
    "        \"use_cases\": [\n",
    "            \"Gradual cloud migration\",\n",
    "            \"Data residency requirements\",\n",
    "            \"Cost optimization\"\n",
    "        ],\n",
    "        \"challenges\": [\n",
    "            \"Network latency\",\n",
    "            \"Data transfer costs\",\n",
    "            \"Security complexity\"\n",
    "        ]\n",
    "    },\n",
    "    \"‚òÅÔ∏è Multi-Cloud\": {\n",
    "        \"description\": \"Multiple cloud providers\",\n",
    "        \"use_cases\": [\n",
    "            \"Vendor lock-in avoidance\",\n",
    "            \"Best-of-breed services\",\n",
    "            \"Disaster recovery\"\n",
    "        ],\n",
    "        \"challenges\": [\n",
    "            \"Increased complexity\",\n",
    "            \"Data consistency\",\n",
    "            \"Cross-cloud networking\"\n",
    "        ]\n",
    "    },\n",
    "    \"üåç Data Distribution\": {\n",
    "        \"description\": \"Geographically distributed data\",\n",
    "        \"use_cases\": [\n",
    "            \"Global applications\",\n",
    "            \"Compliance requirements\",\n",
    "            \"Performance optimization\"\n",
    "        ],\n",
    "        \"challenges\": [\n",
    "            \"Data governance\",\n",
    "            \"Consistency models\",\n",
    "            \"Cross-region costs\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "for pattern, details in multicloud_patterns.items():\n",
    "    print(f\"\\n{pattern}: {details['description']}\")\n",
    "    print(\"  üìà Use Cases:\")\n",
    "    for use_case in details['use_cases']:\n",
    "        print(f\"    ‚Ä¢ {use_case}\")\n",
    "    print(\"  ‚ö†Ô∏è Challenges:\")\n",
    "    for challenge in details['challenges']:\n",
    "        print(f\"    ‚Ä¢ {challenge}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Cloud Configuration Example\n",
    "print(\"üîß MULTI-CLOUD CONFIGURATION EXAMPLE\")\n",
    "print(\"\\nüìù Federated catalog setup across clouds:\")\n",
    "\n",
    "multicloud_config = \"\"\"\n",
    "# Multi-Cloud Spark Session with Multiple Catalogs\n",
    "spark = SparkSession.builder \\\\\n",
    "    .appName(\"IcebergMultiCloud\") \\\\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.3,\"\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"org.apache.hadoop:hadoop-azure:3.3.4,\"\n",
    "            \"com.google.cloud.bigdataoss:gcs-connector:hadoop3-2.2.11\") \\\\\n",
    "    .config(\"spark.sql.extensions\", \n",
    "            \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\\\n",
    "    \\\\\n",
    "    # AWS Catalog\n",
    "    .config(\"spark.sql.catalog.aws\", \"org.apache.iceberg.spark.SparkCatalog\") \\\\\n",
    "    .config(\"spark.sql.catalog.aws.type\", \"glue\") \\\\\n",
    "    .config(\"spark.sql.catalog.aws.warehouse\", \"s3a://aws-warehouse/\") \\\\\n",
    "    \\\\\n",
    "    # Azure Catalog\n",
    "    .config(\"spark.sql.catalog.azure\", \"org.apache.iceberg.spark.SparkCatalog\") \\\\\n",
    "    .config(\"spark.sql.catalog.azure.type\", \"hadoop\") \\\\\n",
    "    .config(\"spark.sql.catalog.azure.warehouse\", \n",
    "            \"abfss://warehouse@account.dfs.core.windows.net/\") \\\\\n",
    "    \\\\\n",
    "    # GCP Catalog\n",
    "    .config(\"spark.sql.catalog.gcp\", \"org.apache.iceberg.spark.SparkCatalog\") \\\\\n",
    "    .config(\"spark.sql.catalog.gcp.type\", \"hadoop\") \\\\\n",
    "    .config(\"spark.sql.catalog.gcp.warehouse\", \"gs://gcp-warehouse/\") \\\\\n",
    "    .getOrCreate()\n",
    "\"\"\"\n",
    "\n",
    "print(multicloud_config)\n",
    "\n",
    "print(\"\\nüìä Multi-cloud query examples:\")\n",
    "multicloud_queries = [\n",
    "    {\n",
    "        \"operation\": \"Cross-cloud data federation\",\n",
    "        \"sql\": \"\"\"\n",
    "-- Join data across clouds\n",
    "SELECT \n",
    "    a.customer_id,\n",
    "    a.order_total,\n",
    "    b.event_count\n",
    "FROM aws.sales.orders a\n",
    "JOIN azure.analytics.customer_events b\n",
    "  ON a.customer_id = b.customer_id\n",
    "WHERE a.order_date >= CURRENT_DATE - INTERVAL 30 DAYS\n",
    "\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"operation\": \"Data migration between clouds\",\n",
    "        \"sql\": \"\"\"\n",
    "-- Migrate data from AWS to GCP\n",
    "CREATE TABLE gcp.migration.orders AS\n",
    "SELECT * FROM aws.sales.orders\n",
    "WHERE order_date >= DATE '2024-01-01'\n",
    "\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for query in multicloud_queries:\n",
    "    print(f\"\\nüìã {query['operation']}:\")\n",
    "    print(f\"```sql{query['sql']}```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ‚ö° Performance Optimization for Cloud\n",
    "\n",
    "Learn cloud-specific performance optimization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud Performance Optimization\n",
    "print(\"‚ö° CLOUD PERFORMANCE OPTIMIZATION\")\n",
    "print(\"\\nüöÄ Cloud-specific optimization strategies:\")\n",
    "\n",
    "cloud_optimizations = {\n",
    "    \"üóÑÔ∏è Storage Optimization\": {\n",
    "        \"File Size\": [\n",
    "            \"Target 128MB-1GB files for optimal cloud storage\",\n",
    "            \"Use write.target-file-size-bytes config\",\n",
    "            \"Monitor file count vs. size trade-offs\"\n",
    "        ],\n",
    "        \"Compression\": [\n",
    "            \"Use ZSTD compression for better performance\",\n",
    "            \"Consider GZIP for compatibility\",\n",
    "            \"Test compression ratios for your data\"\n",
    "        ],\n",
    "        \"Partitioning\": [\n",
    "            \"Align partitions with query patterns\",\n",
    "            \"Avoid over-partitioning (< 1GB per partition)\",\n",
    "            \"Use hidden partitioning for flexibility\"\n",
    "        ]\n",
    "    },\n",
    "    \"üåê Network Optimization\": {\n",
    "        \"Data Locality\": [\n",
    "            \"Co-locate compute and storage in same region\",\n",
    "            \"Use regional storage classes\",\n",
    "            \"Consider data transfer costs\"\n",
    "        ],\n",
    "        \"Caching\": [\n",
    "            \"Enable Spark SQL adaptive query execution\",\n",
    "            \"Use broadcast joins for small tables\",\n",
    "            \"Cache frequently accessed metadata\"\n",
    "        ]\n",
    "    },\n",
    "    \"üí∞ Cost Optimization\": {\n",
    "        \"Storage Classes\": [\n",
    "            \"Use appropriate storage tiers (Standard/IA/Archive)\",\n",
    "            \"Implement lifecycle policies\",\n",
    "            \"Monitor storage costs by table\"\n",
    "        ],\n",
    "        \"Compute\": [\n",
    "            \"Right-size compute instances\",\n",
    "            \"Use spot/preemptible instances when possible\",\n",
    "            \"Auto-scaling for variable workloads\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "for category, subcategories in cloud_optimizations.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for subcat, tips in subcategories.items():\n",
    "        print(f\"  üìã {subcat}:\")\n",
    "        for tip in tips:\n",
    "            print(f\"    ‚Ä¢ {tip}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Configuration Examples\n",
    "print(\"üîß PERFORMANCE CONFIGURATION EXAMPLES\")\n",
    "print(\"\\nüìù Spark configurations for cloud optimization:\")\n",
    "\n",
    "performance_configs = {\n",
    "    \"File Size Optimization\": {\n",
    "        \"write.target-file-size-bytes\": \"134217728\",  # 128MB\n",
    "        \"write.delete.target-file-size-bytes\": \"67108864\",  # 64MB\n",
    "        \"write.merge.target-file-size-bytes\": \"134217728\"  # 128MB\n",
    "    },\n",
    "    \"Compression Settings\": {\n",
    "        \"write.parquet.compression-codec\": \"zstd\",\n",
    "        \"write.parquet.compression-level\": \"3\",\n",
    "        \"read.parquet.vectorization.enabled\": \"true\"\n",
    "    },\n",
    "    \"Memory Optimization\": {\n",
    "        \"spark.sql.adaptive.enabled\": \"true\",\n",
    "        \"spark.sql.adaptive.coalescePartitions.enabled\": \"true\",\n",
    "        \"spark.sql.adaptive.skewJoin.enabled\": \"true\",\n",
    "        \"spark.serializer\": \"org.apache.spark.serializer.KryoSerializer\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for category, configs in performance_configs.items():\n",
    "    print(f\"\\nüìã {category}:\")\n",
    "    for key, value in configs.items():\n",
    "        print(f\"  spark.sql.catalog.prod.{key} = {value}\")\n",
    "\n",
    "# Example table properties\n",
    "print(\"\\nüìä Example table properties for performance:\")\n",
    "table_properties_sql = \"\"\"\n",
    "CREATE TABLE prod.analytics.optimized_table (\n",
    "    id bigint,\n",
    "    data string,\n",
    "    timestamp timestamp\n",
    ") USING ICEBERG\n",
    "PARTITIONED BY (days(timestamp))\n",
    "TBLPROPERTIES (\n",
    "    'write.target-file-size-bytes' = '134217728',\n",
    "    'write.parquet.compression-codec' = 'zstd',\n",
    "    'write.metadata.compression-codec' = 'gzip',\n",
    "    'history.expire.max-snapshot-age-ms' = '2592000000'  # 30 days\n",
    ")\n",
    "\"\"\"\n",
    "print(table_properties_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. üìä Monitoring and Observability\n",
    "\n",
    "Learn to monitor Iceberg in cloud environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud Monitoring Strategies\n",
    "print(\"üìä CLOUD MONITORING AND OBSERVABILITY\")\n",
    "print(\"\\nüîç Key metrics to monitor:\")\n",
    "\n",
    "monitoring_metrics = {\n",
    "    \"üìà Performance Metrics\": [\n",
    "        \"Query execution time\",\n",
    "        \"Data scan rates (GB/sec)\",\n",
    "        \"File count per table\",\n",
    "        \"Average file size\",\n",
    "        \"Partition pruning effectiveness\"\n",
    "    ],\n",
    "    \"üíæ Storage Metrics\": [\n",
    "        \"Total storage usage\",\n",
    "        \"Storage growth rate\",\n",
    "        \"Metadata size vs data size ratio\",\n",
    "        \"Snapshot count per table\",\n",
    "        \"Orphaned file count\"\n",
    "    ],\n",
    "    \"üí∞ Cost Metrics\": [\n",
    "        \"Storage costs by table\",\n",
    "        \"Compute costs by workload\",\n",
    "        \"Data transfer costs\",\n",
    "        \"API call costs\",\n",
    "        \"Cost per query\"\n",
    "    ],\n",
    "    \"üîí Security Metrics\": [\n",
    "        \"Access patterns by user\",\n",
    "        \"Failed authentication attempts\",\n",
    "        \"Permission violations\",\n",
    "        \"Data access audit logs\",\n",
    "        \"Encryption status\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, metrics in monitoring_metrics.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for metric in metrics:\n",
    "        print(f\"  ‚Ä¢ {metric}\")\n",
    "\n",
    "# Cloud-specific monitoring tools\n",
    "print(\"\\nüõ†Ô∏è Cloud-specific monitoring tools:\")\n",
    "cloud_tools = {\n",
    "    \"AWS\": [\n",
    "        \"CloudWatch for metrics and logs\",\n",
    "        \"CloudTrail for API audit logs\",\n",
    "        \"Cost Explorer for cost analysis\",\n",
    "        \"X-Ray for distributed tracing\"\n",
    "    ],\n",
    "    \"Azure\": [\n",
    "        \"Azure Monitor for metrics\",\n",
    "        \"Log Analytics for log analysis\",\n",
    "        \"Cost Management for cost tracking\",\n",
    "        \"Application Insights for performance\"\n",
    "    ],\n",
    "    \"GCP\": [\n",
    "        \"Cloud Monitoring for metrics\",\n",
    "        \"Cloud Logging for log management\",\n",
    "        \"Cloud Billing for cost analysis\",\n",
    "        \"Cloud Trace for performance\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for cloud, tools in cloud_tools.items():\n",
    "    print(f\"\\nüåê {cloud}:\")\n",
    "    for tool in tools:\n",
    "        print(f\"  ‚Ä¢ {tool}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitoring Queries Examples\n",
    "print(\"üìä MONITORING QUERIES EXAMPLES\")\n",
    "print(\"\\nüìù SQL queries for monitoring Iceberg tables:\")\n",
    "\n",
    "monitoring_queries = [\n",
    "    {\n",
    "        \"name\": \"Table Storage Analysis\",\n",
    "        \"sql\": \"\"\"\n",
    "-- Analyze storage usage by table\n",
    "SELECT \n",
    "    'your_table' as table_name,\n",
    "    COUNT(*) as file_count,\n",
    "    SUM(file_size_in_bytes) / 1024 / 1024 / 1024 as size_gb,\n",
    "    AVG(file_size_in_bytes) / 1024 / 1024 as avg_file_size_mb,\n",
    "    SUM(record_count) as total_records\n",
    "FROM your_catalog.your_db.your_table.files\n",
    "\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Snapshot Management\",\n",
    "        \"sql\": \"\"\"\n",
    "-- Monitor snapshot accumulation\n",
    "SELECT \n",
    "    COUNT(*) as snapshot_count,\n",
    "    MIN(committed_at) as oldest_snapshot,\n",
    "    MAX(committed_at) as newest_snapshot,\n",
    "    DATEDIFF(day, MIN(committed_at), MAX(committed_at)) as retention_days\n",
    "FROM your_catalog.your_db.your_table.snapshots\n",
    "\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Partition Efficiency\",\n",
    "        \"sql\": \"\"\"\n",
    "-- Analyze partition distribution\n",
    "SELECT \n",
    "    partition,\n",
    "    record_count,\n",
    "    file_count,\n",
    "    ROUND(record_count / file_count, 0) as avg_records_per_file\n",
    "FROM your_catalog.your_db.your_table.partitions\n",
    "ORDER BY record_count DESC\n",
    "\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Performance Metrics\",\n",
    "        \"sql\": \"\"\"\n",
    "-- Query performance analysis\n",
    "SELECT \n",
    "    operation,\n",
    "    COUNT(*) as operation_count,\n",
    "    AVG(DATEDIFF(second, lag(committed_at) OVER (ORDER BY committed_at), committed_at)) as avg_time_between_ops\n",
    "FROM your_catalog.your_db.your_table.snapshots\n",
    "GROUP BY operation\n",
    "\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for query in monitoring_queries:\n",
    "    print(f\"\\nüìã {query['name']}:\")\n",
    "    print(f\"```sql{query['sql']}```\")\n",
    "\n",
    "print(\"\\nüí° Monitoring Best Practices:\")\n",
    "monitoring_best_practices = [\n",
    "    \"Set up automated alerts for storage growth\",\n",
    "    \"Monitor query performance trends\",\n",
    "    \"Track cost metrics regularly\",\n",
    "    \"Implement data quality checks\",\n",
    "    \"Audit access patterns for security\",\n",
    "    \"Regular snapshot cleanup automation\"\n",
    "]\n",
    "\n",
    "for practice in monitoring_best_practices:\n",
    "    print(f\"  ‚úÖ {practice}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. üéâ Summary and Best Practices\n",
    "\n",
    "Cloud integration summary and recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"üéâ CLOUD INTEGRATION TUTORIAL COMPLETE!\")\n",
    "print(\"\\n‚úÖ What You've Learned:\")\n",
    "\n",
    "accomplishments = [\n",
    "    \"Cloud storage integration patterns for AWS, Azure, and GCP\",\n",
    "    \"Security and authentication best practices\",\n",
    "    \"Multi-cloud and hybrid deployment strategies\",\n",
    "    \"Performance optimization for cloud environments\",\n",
    "    \"Cost optimization techniques\",\n",
    "    \"Monitoring and observability patterns\",\n",
    "    \"Cloud-specific service integrations\"\n",
    "]\n",
    "\n",
    "for i, accomplishment in enumerate(accomplishments, 1):\n",
    "    print(f\"   {i}. {accomplishment}\")\n",
    "\n",
    "print(\"\\nüí° CLOUD INTEGRATION BEST PRACTICES:\")\n",
    "\n",
    "best_practices = {\n",
    "    \"üîí Security First\": [\n",
    "        \"Use IAM roles/managed identities instead of keys\",\n",
    "        \"Implement least privilege access\",\n",
    "        \"Enable audit logging and monitoring\",\n",
    "        \"Encrypt data at rest and in transit\"\n",
    "    ],\n",
    "    \"‚ö° Performance\": [\n",
    "        \"Co-locate compute and storage\",\n",
    "        \"Optimize file sizes for cloud storage\",\n",
    "        \"Use appropriate compression algorithms\",\n",
    "        \"Monitor and tune query performance\"\n",
    "    ],\n",
    "    \"üí∞ Cost Management\": [\n",
    "        \"Choose appropriate storage tiers\",\n",
    "        \"Implement lifecycle policies\",\n",
    "        \"Monitor data transfer costs\",\n",
    "        \"Right-size compute resources\"\n",
    "    ],\n",
    "    \"üîß Operations\": [\n",
    "        \"Automate deployment and configuration\",\n",
    "        \"Implement proper backup strategies\",\n",
    "        \"Set up comprehensive monitoring\",\n",
    "        \"Plan for disaster recovery\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, practices in best_practices.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for practice in practices:\n",
    "        print(f\"   ‚Ä¢ {practice}\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "next_steps = [\n",
    "    \"Set up a cloud environment for hands-on practice\",\n",
    "    \"Implement monitoring and alerting\",\n",
    "    \"Explore production pipeline patterns\",\n",
    "    \"Study advanced multi-cloud architectures\"\n",
    "]\n",
    "\n",
    "for step in next_steps:\n",
    "    print(f\"   ‚Üí {step}\")\n",
    "\n",
    "print(\"\\nüéØ Key Takeaway:\")\n",
    "print(\"   Cloud integration with Iceberg provides scalable, cost-effective,\")\n",
    "print(\"   and secure data lake solutions across all major cloud providers!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
