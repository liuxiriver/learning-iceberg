{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# üßä Apache Iceberg Tutorial\n",
    "\n",
    "Welcome to the comprehensive Apache Iceberg tutorial! In this notebook, you'll learn:\n",
    "\n",
    "1. **Setting up Spark with Iceberg**\n",
    "2. **Creating your first Iceberg table**\n",
    "3. **Inserting and querying data**\n",
    "4. **Understanding table metadata and structure**\n",
    "5. **Time travel and snapshots**\n",
    "6. **Schema evolution**\n",
    "\n",
    "## üìã Prerequisites\n",
    "\n",
    "- Docker environment is running\n",
    "- Basic understanding of SQL\n",
    "- Python and PySpark knowledge (helpful but not required)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. üöÄ Initialize Spark with Iceberg\n",
    "\n",
    "First, let's set up Spark with Iceberg extensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è No existing Spark session to stop\n",
      "‚úÖ Spark with Iceberg initialized successfully!\n",
      "Spark version: 3.5.0\n",
      "Python path: /opt/conda/bin/python\n",
      "Configured warehouse location: file:///home/jovyan/work/warehouse\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "# Set Python path for Spark to ensure consistent Python version\n",
    "os.environ['PYSPARK_PYTHON'] = '/opt/conda/bin/python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/opt/conda/bin/python'\n",
    "\n",
    "# Stop existing Spark session if any\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"üõë Stopped existing Spark session\")\n",
    "except:\n",
    "    print(\"‚ÑπÔ∏è No existing Spark session to stop\")\n",
    "\n",
    "# Create Spark session with Iceberg and correct warehouse path\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergTutorial\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.3\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.local.warehouse\", \"file:///home/jovyan/work/warehouse\") \\\n",
    "    .config(\"spark.pyspark.python\", \"/opt/conda/bin/python\") \\\n",
    "    .config(\"spark.pyspark.driver.python\", \"/opt/conda/bin/python\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"‚úÖ Spark with Iceberg initialized successfully!\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Python path: {os.environ.get('PYSPARK_PYTHON', 'Not set')}\")\n",
    "\n",
    "# Verify the warehouse configuration\n",
    "warehouse_path = spark.conf.get(\"spark.sql.catalog.local.warehouse\")\n",
    "print(f\"Configured warehouse location: {warehouse_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. üèóÔ∏è Create Database and Your First Table\n",
    "\n",
    "Let's create a database and a table to store user events data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Database 'local.demo' created!\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create database\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS local.demo\")\n",
    "print(\"‚úÖ Database 'local.demo' created!\")\n",
    "\n",
    "# Show available databases\n",
    "spark.sql(\"SHOW DATABASES\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóëÔ∏è Dropped existing table (if any)\n",
      "üéâ Iceberg table 'user_events' created successfully!\n",
      "‚úÖ Table 'user_events' found in catalog\n",
      "üìç Warehouse location: file:///home/jovyan/work/warehouse\n",
      "+--------------+----------------+-------+\n",
      "|      col_name|       data_type|comment|\n",
      "+--------------+----------------+-------+\n",
      "|       user_id|          bigint|   NULL|\n",
      "|    event_type|          string|   NULL|\n",
      "|    event_time|       timestamp|   NULL|\n",
      "|      page_url|          string|   NULL|\n",
      "|    user_agent|          string|   NULL|\n",
      "|    session_id|          string|   NULL|\n",
      "|              |                |       |\n",
      "|# Partitioning|                |       |\n",
      "|        Part 0|days(event_time)|       |\n",
      "+--------------+----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop the table if it exists to avoid path conflicts\n",
    "try:\n",
    "    spark.sql(\"DROP TABLE IF EXISTS local.demo.user_events\")\n",
    "    print(\"üóëÔ∏è Dropped existing table (if any)\")\n",
    "except:\n",
    "    print(\"‚ÑπÔ∏è No existing table to drop\")\n",
    "\n",
    "# Create Iceberg table with explicit path\n",
    "create_table_sql = \"\"\"\n",
    "CREATE TABLE local.demo.user_events (\n",
    "    user_id bigint,\n",
    "    event_type string,\n",
    "    event_time timestamp,\n",
    "    page_url string,\n",
    "    user_agent string,\n",
    "    session_id string\n",
    ") USING ICEBERG\n",
    "PARTITIONED BY (days(event_time))\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(create_table_sql)\n",
    "print(\"üéâ Iceberg table 'user_events' created successfully!\")\n",
    "\n",
    "# Verify the table was created successfully\n",
    "try:\n",
    "    # Check if we can query the table structure\n",
    "    table_info = spark.sql(\"SHOW TABLES IN local.demo\").collect()\n",
    "    tables = [row.tableName for row in table_info]\n",
    "    if 'user_events' in tables:\n",
    "        print(\"‚úÖ Table 'user_events' found in catalog\")\n",
    "    else:\n",
    "        print(\"‚ùå Table 'user_events' not found in catalog\")\n",
    "\n",
    "    # Try to get table properties (alternative way to check table details)\n",
    "    warehouse_path = spark.conf.get(\"spark.sql.catalog.local.warehouse\")\n",
    "    print(f\"üìç Warehouse location: {warehouse_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not verify table details: {e}\")\n",
    "\n",
    "# Describe the table\n",
    "spark.sql(\"DESCRIBE local.demo.user_events\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. üìù Insert Sample Data\n",
    "\n",
    "Let's insert some sample user events data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Sample data to insert:\n",
      "+-------+----------+-------------------+-------------+-----------+----------+\n",
      "|user_id|event_type|event_time         |page_url     |user_agent |session_id|\n",
      "+-------+----------+-------------------+-------------+-----------+----------+\n",
      "|1001   |page_view |2024-01-15 10:30:00|/home        |Mozilla/5.0|sess_001  |\n",
      "|1001   |click     |2024-01-15 10:35:00|/products    |Mozilla/5.0|sess_001  |\n",
      "|1001   |purchase  |2024-01-15 10:45:00|/checkout    |Mozilla/5.0|sess_001  |\n",
      "|1002   |page_view |2024-01-15 11:00:00|/home        |Chrome/98.0|sess_002  |\n",
      "|1002   |search    |2024-01-15 11:05:00|/search      |Chrome/98.0|sess_002  |\n",
      "|1002   |click     |2024-01-15 11:10:00|/products/123|Chrome/98.0|sess_002  |\n",
      "|1003   |page_view |2024-01-16 09:00:00|/home        |Safari/15.0|sess_003  |\n",
      "|1003   |signup    |2024-01-16 09:15:00|/signup      |Safari/15.0|sess_003  |\n",
      "+-------+----------+-------------------+-------------+-----------+----------+\n",
      "\n",
      "‚úÖ Data inserted successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create sample data\n",
    "from datetime import datetime\n",
    "\n",
    "sample_data = [\n",
    "    (1001, \"page_view\", datetime(2024, 1, 15, 10, 30, 0), \"/home\", \"Mozilla/5.0\", \"sess_001\"),\n",
    "    (1001, \"click\", datetime(2024, 1, 15, 10, 35, 0), \"/products\", \"Mozilla/5.0\", \"sess_001\"),\n",
    "    (1001, \"purchase\", datetime(2024, 1, 15, 10, 45, 0), \"/checkout\", \"Mozilla/5.0\", \"sess_001\"),\n",
    "    (1002, \"page_view\", datetime(2024, 1, 15, 11, 0, 0), \"/home\", \"Chrome/98.0\", \"sess_002\"),\n",
    "    (1002, \"search\", datetime(2024, 1, 15, 11, 5, 0), \"/search\", \"Chrome/98.0\", \"sess_002\"),\n",
    "    (1002, \"click\", datetime(2024, 1, 15, 11, 10, 0), \"/products/123\", \"Chrome/98.0\", \"sess_002\"),\n",
    "    (1003, \"page_view\", datetime(2024, 1, 16, 9, 0, 0), \"/home\", \"Safari/15.0\", \"sess_003\"),\n",
    "    (1003, \"signup\", datetime(2024, 1, 16, 9, 15, 0), \"/signup\", \"Safari/15.0\", \"sess_003\"),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"user_id\", \"event_type\", \"event_time\", \"page_url\", \"user_agent\", \"session_id\"]\n",
    "df = spark.createDataFrame(sample_data, columns)\n",
    "\n",
    "# Show the data we're about to insert\n",
    "print(\"üìä Sample data to insert:\")\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Insert data into Iceberg table\n",
    "df.writeTo(\"local.demo.user_events\").append()\n",
    "print(\"‚úÖ Data inserted successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. üîç Query and Analyze Data\n",
    "\n",
    "Now let's query our Iceberg table and perform some analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìú Table history (fixed):\n",
      "+--------------------+-------------------+---------+-------------------+\n",
      "|     made_current_at|        snapshot_id|parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+---------+-------------------+\n",
      "|2025-06-15 23:33:...|4374133211291202804|     NULL|               true|\n",
      "+--------------------+-------------------+---------+-------------------+\n",
      "\n",
      "\n",
      "‚ú® What each column means:\n",
      "‚Ä¢ made_current_at: When this snapshot became the current table state\n",
      "‚Ä¢ snapshot_id: Unique identifier for each snapshot\n",
      "‚Ä¢ parent_id: The previous snapshot this one builds upon\n",
      "‚Ä¢ is_current_ancestor: Whether this snapshot is in the current lineage\n"
     ]
    }
   ],
   "source": [
    "# üîß Fix table history query\n",
    "print(\"üìú Table history (fixed):\")\n",
    "\n",
    "try:\n",
    "    # Use correct column names for this Iceberg version\n",
    "    spark.sql(\"SELECT made_current_at, snapshot_id, parent_id, is_current_ancestor FROM local.demo.user_events.history\").show()\n",
    "\n",
    "    print(\"\\n‚ú® What each column means:\")\n",
    "    print(\"‚Ä¢ made_current_at: When this snapshot became the current table state\")\n",
    "    print(\"‚Ä¢ snapshot_id: Unique identifier for each snapshot\")\n",
    "    print(\"‚Ä¢ parent_id: The previous snapshot this one builds upon\")\n",
    "    print(\"‚Ä¢ is_current_ancestor: Whether this snapshot is in the current lineage\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"History query error: {e}\")\n",
    "    print(\"üìã Let's see all available columns in history:\")\n",
    "    try:\n",
    "        # Show all columns to understand the schema\n",
    "        history_df = spark.sql(\"SELECT * FROM local.demo.user_events.history\")\n",
    "        print(f\"Columns: {history_df.columns}\")\n",
    "        history_df.show()\n",
    "    except Exception as e2:\n",
    "        print(f\"Could not access history table: {e2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ Congratulations! Your Iceberg table is working perfectly!\n",
      "\n",
      "üìä Summary of what we created:\n",
      "‚Ä¢ Total records: 8\n",
      "‚Ä¢ Partitions created: 2\n",
      "  - Row(event_time_day=datetime.date(2024, 1, 15)): 1 records\n",
      "  - Row(event_time_day=datetime.date(2024, 1, 16)): 1 records\n",
      "‚Ä¢ Unique users: 3\n",
      "‚Ä¢ Event types:\n",
      "  - page_view: 3 events\n",
      "  - click: 2 events\n",
      "  - signup: 1 events\n",
      "  - purchase: 1 events\n",
      "  - search: 1 events\n",
      "\n",
      "üîç Key Iceberg Features Demonstrated:\n",
      "‚úÖ ACID transactions - All data writes are atomic\n",
      "‚úÖ Time partitioning - Data organized by partition\n",
      "‚úÖ Metadata tables - .snapshots, .files, .history for introspection\n",
      "‚úÖ Schema enforcement - Strong typing with bigint, string, timestamp\n",
      "‚úÖ Parquet storage - Efficient columnar format for analytics\n",
      "\n",
      "üìÅ Your data is stored at: file:///home/jovyan/work/warehouse\n",
      "üöÄ Ready to explore time travel, schema evolution, and more!\n"
     ]
    }
   ],
   "source": [
    "# üéâ SUCCESS! Let's explore what we've accomplished\n",
    "print(\"üéâ Congratulations! Your Iceberg table is working perfectly!\")\n",
    "print(\"\\nüìä Summary of what we created:\")\n",
    "\n",
    "# Count total records\n",
    "total_records = spark.sql(\"SELECT COUNT(*) as total FROM local.demo.user_events\").collect()[0]['total']\n",
    "print(f\"‚Ä¢ Total records: {total_records}\")\n",
    "\n",
    "# Show partition information\n",
    "partitions = spark.sql(\"SELECT partition, COUNT(*) as records FROM local.demo.user_events.partitions GROUP BY partition ORDER BY partition\").collect()\n",
    "print(f\"‚Ä¢ Partitions created: {len(partitions)}\")\n",
    "for p in partitions:\n",
    "    print(f\"  - {p['partition']}: {p['records']} records\")\n",
    "\n",
    "# Show unique users and events\n",
    "user_count = spark.sql(\"SELECT COUNT(DISTINCT user_id) as users FROM local.demo.user_events\").collect()[0]['users']\n",
    "event_types = spark.sql(\"SELECT event_type, COUNT(*) as count FROM local.demo.user_events GROUP BY event_type ORDER BY count DESC\").collect()\n",
    "\n",
    "print(f\"‚Ä¢ Unique users: {user_count}\")\n",
    "print(\"‚Ä¢ Event types:\")\n",
    "for et in event_types:\n",
    "    print(f\"  - {et['event_type']}: {et['count']} events\")\n",
    "\n",
    "print(\"\\nüîç Key Iceberg Features Demonstrated:\")\n",
    "print(\"‚úÖ ACID transactions - All data writes are atomic\")\n",
    "print(\"‚úÖ Time partitioning - Data organized by partition\")\n",
    "print(\"‚úÖ Metadata tables - .snapshots, .files, .history for introspection\")\n",
    "print(\"‚úÖ Schema enforcement - Strong typing with bigint, string, timestamp\")\n",
    "print(\"‚úÖ Parquet storage - Efficient columnar format for analytics\")\n",
    "\n",
    "print(f\"\\nüìÅ Your data is stored at: {spark.conf.get('spark.sql.catalog.local.warehouse')}\")\n",
    "print(\"üöÄ Ready to explore time travel, schema evolution, and more!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã All user events:\n",
      "+-------+----------+-------------------+-------------+-----------+----------+\n",
      "|user_id|event_type|event_time         |page_url     |user_agent |session_id|\n",
      "+-------+----------+-------------------+-------------+-----------+----------+\n",
      "|1001   |page_view |2024-01-15 10:30:00|/home        |Mozilla/5.0|sess_001  |\n",
      "|1001   |click     |2024-01-15 10:35:00|/products    |Mozilla/5.0|sess_001  |\n",
      "|1001   |purchase  |2024-01-15 10:45:00|/checkout    |Mozilla/5.0|sess_001  |\n",
      "|1002   |page_view |2024-01-15 11:00:00|/home        |Chrome/98.0|sess_002  |\n",
      "|1002   |search    |2024-01-15 11:05:00|/search      |Chrome/98.0|sess_002  |\n",
      "|1002   |click     |2024-01-15 11:10:00|/products/123|Chrome/98.0|sess_002  |\n",
      "|1003   |page_view |2024-01-16 09:00:00|/home        |Safari/15.0|sess_003  |\n",
      "|1003   |signup    |2024-01-16 09:15:00|/signup      |Safari/15.0|sess_003  |\n",
      "+-------+----------+-------------------+-------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic query - show all data\n",
    "print(\"üìã All user events:\")\n",
    "spark.sql(\"SELECT * FROM local.demo.user_events ORDER BY event_time\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5. üì∏ Explore Iceberg Metadata\n",
    "\n",
    "One of Iceberg's key features is rich metadata. Let's explore it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∏ Table snapshots:\n",
      "+-------------------+-----------------------+---------+\n",
      "|snapshot_id        |committed_at           |operation|\n",
      "+-------------------+-----------------------+---------+\n",
      "|4374133211291202804|2025-06-15 23:33:25.237|append   |\n",
      "+-------------------+-----------------------+---------+\n",
      "\n",
      "\n",
      "üìÅ Table files:\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+-----------+------------+\n",
      "|file_path                                                                                                                                   |file_format|record_count|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+-----------+------------+\n",
      "|file:/home/jovyan/work/warehouse/demo/user_events/data/event_time_day=2024-01-15/00000-28-67900940-a662-4bbe-abe8-7253f253a735-00001.parquet|PARQUET    |6           |\n",
      "|file:/home/jovyan/work/warehouse/demo/user_events/data/event_time_day=2024-01-16/00000-28-67900940-a662-4bbe-abe8-7253f253a735-00002.parquet|PARQUET    |2           |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+-----------+------------+\n",
      "\n",
      "\n",
      "üìú Table history:\n",
      "+--------------------+-------------------+---------+-------------------+\n",
      "|     made_current_at|        snapshot_id|parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+---------+-------------------+\n",
      "|2025-06-15 23:33:...|4374133211291202804|     NULL|               true|\n",
      "+--------------------+-------------------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View table snapshots - This is ONLY possible with Iceberg!\n",
    "print(\"üì∏ Table snapshots:\")\n",
    "spark.sql(\"SELECT snapshot_id, committed_at, operation FROM local.demo.user_events.snapshots\").show(truncate=False)\n",
    "\n",
    "# View table files\n",
    "print(\"\\nüìÅ Table files:\")\n",
    "spark.sql(\"SELECT file_path, file_format, record_count FROM local.demo.user_events.files\").show(truncate=False)\n",
    "\n",
    "# View table history\n",
    "print(\"\\nüìú Table history:\")\n",
    "# spark.sql(\"SELECT made_current_at, snapshot_id FROM local.demo.user_events.history\").show()\n",
    "spark.sql(\"SELECT * FROM local.demo.user_events.history\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìú Table history (corrected):\n",
      "+--------------------+-------------------+---------+-------------------+\n",
      "|     made_current_at|        snapshot_id|parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+---------+-------------------+\n",
      "|2025-06-15 23:33:...|4374133211291202804|     NULL|               true|\n",
      "+--------------------+-------------------+---------+-------------------+\n",
      "\n",
      "\n",
      "‚ú® What each column means:\n",
      "‚Ä¢ made_current_at: Timestamp when this snapshot became the current table state\n",
      "‚Ä¢ snapshot_id: Unique identifier for each table snapshot/version\n",
      "‚Ä¢ parent_id: The ID of the previous snapshot this one builds upon (NULL for first)\n",
      "‚Ä¢ is_current_ancestor: Whether this snapshot is part of the current table lineage\n",
      "\n",
      "üîç Understanding the output:\n",
      "‚Ä¢ You should see one row showing your single data insertion\n",
      "‚Ä¢ parent_id is NULL because this is the first snapshot\n",
      "‚Ä¢ is_current_ancestor is true because this is your current table state\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ CORRECTED: Table history query with proper columns\n",
    "print(\"üìú Table history (corrected):\")\n",
    "\n",
    "try:\n",
    "    spark.sql(\"SELECT made_current_at, snapshot_id, parent_id, is_current_ancestor FROM local.demo.user_events.history\").show()\n",
    "\n",
    "    print(\"\\n‚ú® What each column means:\")\n",
    "    print(\"‚Ä¢ made_current_at: Timestamp when this snapshot became the current table state\")\n",
    "    print(\"‚Ä¢ snapshot_id: Unique identifier for each table snapshot/version\")\n",
    "    print(\"‚Ä¢ parent_id: The ID of the previous snapshot this one builds upon (NULL for first)\")\n",
    "    print(\"‚Ä¢ is_current_ancestor: Whether this snapshot is part of the current table lineage\")\n",
    "\n",
    "    print(\"\\nüîç Understanding the output:\")\n",
    "    print(\"‚Ä¢ You should see one row showing your single data insertion\")\n",
    "    print(\"‚Ä¢ parent_id is NULL because this is the first snapshot\")\n",
    "    print(\"‚Ä¢ is_current_ancestor is true because this is your current table state\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Could not query history: {e}\")\n",
    "    print(\"üìã Let's try showing all columns:\")\n",
    "    try:\n",
    "        spark.sql(\"SELECT * FROM local.demo.user_events.history\").show()\n",
    "    except Exception as e2:\n",
    "        print(f\"History table not accessible: {e2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully created and queried your first Iceberg table using Jupyter Notebook!\n",
    "\n",
    "### ‚úÖ What You've Accomplished\n",
    "\n",
    "1. **Set up Spark with Iceberg** in a Jupyter environment\n",
    "2. **Created an Iceberg table** with partitioning\n",
    "3. **Inserted sample data** using DataFrames\n",
    "4. **Queried the data** with SQL\n",
    "5. **Explored Iceberg metadata** - snapshots, files, and history\n",
    "\n",
    "### üîç Key Differences from Regular Spark Tables\n",
    "\n",
    "- **`USING ICEBERG`** - This makes it an Iceberg table, not a regular Spark table\n",
    "- **Rich metadata** - `.snapshots`, `.files`, `.history` queries only work with Iceberg\n",
    "- **ACID transactions** - All operations are atomic and consistent\n",
    "- **Time travel** - You can query historical versions (try adding more data and exploring!)\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "1. **Add more data** and observe new snapshots\n",
    "2. **Try schema evolution** - add new columns safely\n",
    "3. **Experiment with time travel** queries\n",
    "4. **Explore different partitioning strategies**\n",
    "\n",
    "**Remember**: You're using the power of both Spark (compute engine) and Iceberg (table format) together!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
