{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè≠ Apache Iceberg Production Pipeline Tutorial\n",
    "\n",
    "Welcome to the comprehensive Production Pipeline tutorial! In this notebook, you'll learn:\n",
    "\n",
    "1. **Production-Ready Architecture Patterns**\n",
    "2. **Data Ingestion Pipelines**\n",
    "3. **Batch and Streaming Processing**\n",
    "4. **Data Quality and Validation**\n",
    "5. **Table Maintenance and Optimization**\n",
    "6. **Monitoring and Alerting**\n",
    "7. **CI/CD for Data Pipelines**\n",
    "\n",
    "## üìã Prerequisites\n",
    "\n",
    "- Understanding of Iceberg core concepts\n",
    "- Experience with schema evolution and time travel\n",
    "- Basic knowledge of data engineering patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üöÄ Initialize Production Environment\n",
    "\n",
    "Set up a production-like Spark environment with proper configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Set Python path for Spark consistency\n",
    "os.environ['PYSPARK_PYTHON'] = '/opt/conda/bin/python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/opt/conda/bin/python'\n",
    "\n",
    "print(\"üè≠ Production Pipeline Tutorial Environment Setup\")\n",
    "print(\"‚ÑπÔ∏è This tutorial demonstrates production-grade patterns\")\n",
    "print(\"‚ÑπÔ∏è Focus on reliability, scalability, and maintainability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production Spark Session Configuration\n",
    "def create_production_spark_session(app_name=\"ProductionIcebergPipeline\"):\n",
    "    \"\"\"Create a production-ready Spark session with optimized settings\"\"\"\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.3\") \\\n",
    "        .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "        .config(\"spark.sql.catalog.prod\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "        .config(\"spark.sql.catalog.prod.type\", \"hadoop\") \\\n",
    "        .config(\"spark.sql.catalog.prod.warehouse\", \"file:///home/jovyan/work/warehouse\") \\\n",
    "        \\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "        \\\n",
    "        .config(\"spark.sql.catalog.prod.write.target-file-size-bytes\", \"134217728\") \\\n",
    "        .config(\"spark.sql.catalog.prod.write.parquet.compression-codec\", \"zstd\") \\\n",
    "        .config(\"spark.sql.catalog.prod.write.metadata.compression-codec\", \"gzip\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Set log level to reduce noise\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    \n",
    "    return spark\n",
    "\n",
    "# Stop existing session\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"üõë Stopped existing Spark session\")\n",
    "except:\n",
    "    print(\"‚ÑπÔ∏è No existing Spark session to stop\")\n",
    "\n",
    "# Create production session\n",
    "spark = create_production_spark_session()\n",
    "print(\"‚úÖ Production Spark session initialized!\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Warehouse location: {spark.conf.get('spark.sql.catalog.prod.warehouse')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üèóÔ∏è Production Architecture Patterns\n",
    "\n",
    "Learn production-ready architecture patterns for Iceberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production Architecture Patterns\n",
    "print(\"üèóÔ∏è PRODUCTION ARCHITECTURE PATTERNS\")\n",
    "print(\"\\nüìê Common production architectures:\")\n",
    "\n",
    "architecture_patterns = {\n",
    "    \"üîÑ Lambda Architecture\": {\n",
    "        \"description\": \"Batch + Real-time processing\",\n",
    "        \"components\": [\n",
    "            \"Batch Layer: Historical data processing\",\n",
    "            \"Speed Layer: Real-time stream processing\",\n",
    "            \"Serving Layer: Query interface combining both\"\n",
    "        ],\n",
    "        \"iceberg_benefits\": [\n",
    "            \"ACID transactions for consistency\",\n",
    "            \"Schema evolution for changing requirements\",\n",
    "            \"Time travel for historical analysis\"\n",
    "        ]\n",
    "    },\n",
    "    \"üìä Medallion Architecture\": {\n",
    "        \"description\": \"Bronze ‚Üí Silver ‚Üí Gold data flow\",\n",
    "        \"components\": [\n",
    "            \"Bronze: Raw data ingestion\",\n",
    "            \"Silver: Cleaned and validated data\",\n",
    "            \"Gold: Business-ready aggregated data\"\n",
    "        ],\n",
    "        \"iceberg_benefits\": [\n",
    "            \"Hidden partitioning for performance\",\n",
    "            \"Snapshot isolation for quality gates\",\n",
    "            \"Metadata tables for lineage tracking\"\n",
    "        ]\n",
    "    },\n",
    "    \"üåä Event-Driven Architecture\": {\n",
    "        \"description\": \"Event-based data processing\",\n",
    "        \"components\": [\n",
    "            \"Event Producers: Data sources\",\n",
    "            \"Event Streams: Message queues\",\n",
    "            \"Event Consumers: Processing applications\"\n",
    "        ],\n",
    "        \"iceberg_benefits\": [\n",
    "            \"Atomic writes for reliability\",\n",
    "            \"Table versioning for rollbacks\",\n",
    "            \"Multi-engine support for flexibility\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "for pattern, details in architecture_patterns.items():\n",
    "    print(f\"\\n{pattern}: {details['description']}\")\n",
    "    print(\"  üìã Components:\")\n",
    "    for component in details['components']:\n",
    "        print(f\"    ‚Ä¢ {component}\")\n",
    "    print(\"  ‚úÖ Iceberg Benefits:\")\n",
    "    for benefit in details['iceberg_benefits']:\n",
    "        print(f\"    ‚Ä¢ {benefit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üì• Data Ingestion Pipeline\n",
    "\n",
    "Build a production data ingestion pipeline with quality checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create production database and tables\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS prod.data_lake\")\n",
    "print(\"‚úÖ Created production database\")\n",
    "\n",
    "# Medallion Architecture Implementation\n",
    "class MedallionPipeline:\n",
    "    def __init__(self, spark_session):\n",
    "        self.spark = spark_session\n",
    "        self.catalog = \"prod\"\n",
    "        self.database = \"data_lake\"\n",
    "        \n",
    "    def create_bronze_table(self):\n",
    "        \"\"\"Create bronze table for raw data ingestion\"\"\"\n",
    "        create_bronze_sql = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {self.catalog}.{self.database}.bronze_events (\n",
    "            event_id string,\n",
    "            source_system string,\n",
    "            raw_data string,\n",
    "            ingestion_timestamp timestamp,\n",
    "            file_name string,\n",
    "            file_size bigint\n",
    "        ) USING ICEBERG\n",
    "        PARTITIONED BY (days(ingestion_timestamp), source_system)\n",
    "        TBLPROPERTIES (\n",
    "            'write.target-file-size-bytes' = '134217728',\n",
    "            'write.parquet.compression-codec' = 'zstd'\n",
    "        )\n",
    "        \"\"\"\n",
    "        self.spark.sql(create_bronze_sql)\n",
    "        print(\"‚úÖ Created bronze_events table\")\n",
    "        \n",
    "    def create_silver_table(self):\n",
    "        \"\"\"Create silver table for cleaned data\"\"\"\n",
    "        create_silver_sql = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {self.catalog}.{self.database}.silver_events (\n",
    "            event_id string,\n",
    "            user_id bigint,\n",
    "            event_type string,\n",
    "            event_time timestamp,\n",
    "            properties map<string, string>,\n",
    "            source_system string,\n",
    "            processed_timestamp timestamp,\n",
    "            data_quality_score double\n",
    "        ) USING ICEBERG\n",
    "        PARTITIONED BY (days(event_time), source_system)\n",
    "        TBLPROPERTIES (\n",
    "            'write.target-file-size-bytes' = '134217728',\n",
    "            'write.parquet.compression-codec' = 'zstd'\n",
    "        )\n",
    "        \"\"\"\n",
    "        self.spark.sql(create_silver_sql)\n",
    "        print(\"‚úÖ Created silver_events table\")\n",
    "        \n",
    "    def create_gold_table(self):\n",
    "        \"\"\"Create gold table for business metrics\"\"\"\n",
    "        create_gold_sql = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {self.catalog}.{self.database}.gold_daily_metrics (\n",
    "            metric_date date,\n",
    "            source_system string,\n",
    "            event_type string,\n",
    "            event_count bigint,\n",
    "            unique_users bigint,\n",
    "            avg_quality_score double,\n",
    "            calculated_timestamp timestamp\n",
    "        ) USING ICEBERG\n",
    "        PARTITIONED BY (metric_date, source_system)\n",
    "        TBLPROPERTIES (\n",
    "            'write.target-file-size-bytes' = '67108864',\n",
    "            'write.parquet.compression-codec' = 'zstd'\n",
    "        )\n",
    "        \"\"\"\n",
    "        self.spark.sql(create_gold_sql)\n",
    "        print(\"‚úÖ Created gold_daily_metrics table\")\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = MedallionPipeline(spark)\n",
    "pipeline.create_bronze_table()\n",
    "pipeline.create_silver_table()\n",
    "pipeline.create_gold_table()\n",
    "\n",
    "print(\"\\nüèóÔ∏è Medallion architecture tables created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Ingestion Pipeline Implementation\n",
    "class DataIngestionPipeline:\n",
    "    def __init__(self, spark_session, pipeline):\n",
    "        self.spark = spark_session\n",
    "        self.pipeline = pipeline\n",
    "        \n",
    "    def ingest_raw_data(self, source_system, raw_data_list):\n",
    "        \"\"\"Ingest raw data into bronze layer\"\"\"\n",
    "        print(f\"üì• Ingesting data from {source_system}...\")\n",
    "        \n",
    "        # Create bronze records\n",
    "        bronze_records = []\n",
    "        for i, raw_data in enumerate(raw_data_list):\n",
    "            record = (\n",
    "                f\"{source_system}_{datetime.now().strftime('%Y%m%d')}_{i:06d}\",  # event_id\n",
    "                source_system,\n",
    "                json.dumps(raw_data),  # raw_data as JSON string\n",
    "                datetime.now(),  # ingestion_timestamp\n",
    "                f\"{source_system}_batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\",  # file_name\n",
    "                len(json.dumps(raw_data))  # file_size\n",
    "            )\n",
    "            bronze_records.append(record)\n",
    "        \n",
    "        # Create DataFrame and insert\n",
    "        bronze_schema = StructType([\n",
    "            StructField(\"event_id\", StringType(), True),\n",
    "            StructField(\"source_system\", StringType(), True),\n",
    "            StructField(\"raw_data\", StringType(), True),\n",
    "            StructField(\"ingestion_timestamp\", TimestampType(), True),\n",
    "            StructField(\"file_name\", StringType(), True),\n",
    "            StructField(\"file_size\", LongType(), True)\n",
    "        ])\n",
    "        \n",
    "        bronze_df = self.spark.createDataFrame(bronze_records, bronze_schema)\n",
    "        bronze_df.writeTo(f\"{self.pipeline.catalog}.{self.pipeline.database}.bronze_events\").append()\n",
    "        \n",
    "        print(f\"‚úÖ Ingested {len(bronze_records)} records into bronze layer\")\n",
    "        return bronze_df\n",
    "    \n",
    "    def process_to_silver(self, source_system):\n",
    "        \"\"\"Process bronze data to silver layer with quality checks\"\"\"\n",
    "        print(f\"üîÑ Processing {source_system} data to silver layer...\")\n",
    "        \n",
    "        # Read latest bronze data\n",
    "        bronze_data = self.spark.sql(f\"\"\"\n",
    "        SELECT * FROM {self.pipeline.catalog}.{self.pipeline.database}.bronze_events\n",
    "        WHERE source_system = '{source_system}'\n",
    "        AND ingestion_timestamp >= current_timestamp() - INTERVAL 1 HOUR\n",
    "        \"\"\")\n",
    "        \n",
    "        if bronze_data.count() == 0:\n",
    "            print(\"‚ö†Ô∏è No new bronze data to process\")\n",
    "            return\n",
    "        \n",
    "        # Parse JSON and apply transformations\n",
    "        silver_data = bronze_data.select(\n",
    "            F.col(\"event_id\"),\n",
    "            F.get_json_object(F.col(\"raw_data\"), \"$.user_id\").cast(LongType()).alias(\"user_id\"),\n",
    "            F.get_json_object(F.col(\"raw_data\"), \"$.event_type\").alias(\"event_type\"),\n",
    "            F.get_json_object(F.col(\"raw_data\"), \"$.event_time\").cast(TimestampType()).alias(\"event_time\"),\n",
    "            F.from_json(F.get_json_object(F.col(\"raw_data\"), \"$.properties\"), \n",
    "                       MapType(StringType(), StringType())).alias(\"properties\"),\n",
    "            F.col(\"source_system\"),\n",
    "            F.current_timestamp().alias(\"processed_timestamp\")\n",
    "        )\n",
    "        \n",
    "        # Add data quality score\n",
    "        silver_data = silver_data.withColumn(\n",
    "            \"data_quality_score\",\n",
    "            F.when(F.col(\"user_id\").isNull(), 0.0)\n",
    "            .when(F.col(\"event_type\").isNull(), 0.3)\n",
    "            .when(F.col(\"event_time\").isNull(), 0.5)\n",
    "            .otherwise(1.0)\n",
    "        )\n",
    "        \n",
    "        # Filter out low quality records\n",
    "        quality_data = silver_data.filter(F.col(\"data_quality_score\") >= 0.7)\n",
    "        \n",
    "        # Write to silver table\n",
    "        quality_data.writeTo(f\"{self.pipeline.catalog}.{self.pipeline.database}.silver_events\").append()\n",
    "        \n",
    "        total_records = silver_data.count()\n",
    "        quality_records = quality_data.count()\n",
    "        print(f\"‚úÖ Processed {quality_records}/{total_records} quality records to silver layer\")\n",
    "        \n",
    "        return quality_data\n",
    "\n",
    "# Initialize ingestion pipeline\n",
    "ingestion = DataIngestionPipeline(spark, pipeline)\n",
    "print(\"\\nüì• Data ingestion pipeline initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üß™ Data Quality and Validation\n",
    "\n",
    "Implement comprehensive data quality checks and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality Framework\n",
    "class DataQualityFramework:\n",
    "    def __init__(self, spark_session):\n",
    "        self.spark = spark_session\n",
    "        \n",
    "    def run_quality_checks(self, table_name, checks_config):\n",
    "        \"\"\"Run comprehensive data quality checks\"\"\"\n",
    "        print(f\"üß™ Running quality checks for {table_name}...\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for check_name, check_config in checks_config.items():\n",
    "            try:\n",
    "                result = self._execute_check(table_name, check_name, check_config)\n",
    "                results[check_name] = result\n",
    "                status = \"‚úÖ PASS\" if result['passed'] else \"‚ùå FAIL\"\n",
    "                print(f\"  {status}: {check_name} - {result['message']}\")\n",
    "            except Exception as e:\n",
    "                results[check_name] = {\n",
    "                    'passed': False,\n",
    "                    'message': f\"Error: {str(e)}\",\n",
    "                    'value': None\n",
    "                }\n",
    "                print(f\"  ‚ùå ERROR: {check_name} - {str(e)}\")\n",
    "                \n",
    "        return results\n",
    "    \n",
    "    def _execute_check(self, table_name, check_name, check_config):\n",
    "        \"\"\"Execute individual quality check\"\"\"\n",
    "        check_type = check_config['type']\n",
    "        \n",
    "        if check_type == 'row_count':\n",
    "            return self._check_row_count(table_name, check_config)\n",
    "        elif check_type == 'null_percentage':\n",
    "            return self._check_null_percentage(table_name, check_config)\n",
    "        elif check_type == 'uniqueness':\n",
    "            return self._check_uniqueness(table_name, check_config)\n",
    "        elif check_type == 'freshness':\n",
    "            return self._check_data_freshness(table_name, check_config)\n",
    "        elif check_type == 'custom_sql':\n",
    "            return self._check_custom_sql(table_name, check_config)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown check type: {check_type}\")\n",
    "    \n",
    "    def _check_row_count(self, table_name, config):\n",
    "        count = self.spark.sql(f\"SELECT COUNT(*) as count FROM {table_name}\").collect()[0]['count']\n",
    "        min_count = config.get('min_count', 0)\n",
    "        max_count = config.get('max_count', float('inf'))\n",
    "        \n",
    "        passed = min_count <= count <= max_count\n",
    "        return {\n",
    "            'passed': passed,\n",
    "            'value': count,\n",
    "            'message': f\"Row count: {count} (expected: {min_count}-{max_count})\"\n",
    "        }\n",
    "    \n",
    "    def _check_null_percentage(self, table_name, config):\n",
    "        column = config['column']\n",
    "        max_null_pct = config.get('max_null_percentage', 5.0)\n",
    "        \n",
    "        result = self.spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_rows,\n",
    "            SUM(CASE WHEN {column} IS NULL THEN 1 ELSE 0 END) as null_rows\n",
    "        FROM {table_name}\n",
    "        \"\"\").collect()[0]\n",
    "        \n",
    "        null_pct = (result['null_rows'] / result['total_rows']) * 100 if result['total_rows'] > 0 else 0\n",
    "        passed = null_pct <= max_null_pct\n",
    "        \n",
    "        return {\n",
    "            'passed': passed,\n",
    "            'value': null_pct,\n",
    "            'message': f\"Null percentage for {column}: {null_pct:.2f}% (max allowed: {max_null_pct}%)\"\n",
    "        }\n",
    "    \n",
    "    def _check_uniqueness(self, table_name, config):\n",
    "        column = config['column']\n",
    "        \n",
    "        result = self.spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_rows,\n",
    "            COUNT(DISTINCT {column}) as distinct_rows\n",
    "        FROM {table_name}\n",
    "        \"\"\").collect()[0]\n",
    "        \n",
    "        uniqueness_pct = (result['distinct_rows'] / result['total_rows']) * 100 if result['total_rows'] > 0 else 0\n",
    "        min_uniqueness = config.get('min_uniqueness_percentage', 100.0)\n",
    "        passed = uniqueness_pct >= min_uniqueness\n",
    "        \n",
    "        return {\n",
    "            'passed': passed,\n",
    "            'value': uniqueness_pct,\n",
    "            'message': f\"Uniqueness for {column}: {uniqueness_pct:.2f}% (min required: {min_uniqueness}%)\"\n",
    "        }\n",
    "    \n",
    "    def _check_data_freshness(self, table_name, config):\n",
    "        timestamp_column = config['timestamp_column']\n",
    "        max_age_hours = config.get('max_age_hours', 24)\n",
    "        \n",
    "        result = self.spark.sql(f\"\"\"\n",
    "        SELECT MAX({timestamp_column}) as latest_timestamp\n",
    "        FROM {table_name}\n",
    "        \"\"\").collect()[0]\n",
    "        \n",
    "        if result['latest_timestamp'] is None:\n",
    "            return {\n",
    "                'passed': False,\n",
    "                'value': None,\n",
    "                'message': f\"No data found in {timestamp_column}\"\n",
    "            }\n",
    "        \n",
    "        age_hours = (datetime.now() - result['latest_timestamp']).total_seconds() / 3600\n",
    "        passed = age_hours <= max_age_hours\n",
    "        \n",
    "        return {\n",
    "            'passed': passed,\n",
    "            'value': age_hours,\n",
    "            'message': f\"Data age: {age_hours:.2f} hours (max allowed: {max_age_hours} hours)\"\n",
    "        }\n",
    "    \n",
    "    def _check_custom_sql(self, table_name, config):\n",
    "        sql_query = config['query'].format(table_name=table_name)\n",
    "        expected_result = config.get('expected_result', True)\n",
    "        \n",
    "        result = self.spark.sql(sql_query).collect()[0][0]\n",
    "        passed = result == expected_result\n",
    "        \n",
    "        return {\n",
    "            'passed': passed,\n",
    "            'value': result,\n",
    "            'message': f\"Custom check result: {result} (expected: {expected_result})\"\n",
    "        }\n",
    "\n",
    "# Initialize quality framework\n",
    "quality_framework = DataQualityFramework(spark)\n",
    "print(\"üß™ Data quality framework initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data for testing\n",
    "print(\"üîÑ Generating sample data for quality testing...\")\n",
    "\n",
    "# Sample data for different source systems\n",
    "web_events = [\n",
    "    {\"user_id\": 1001, \"event_type\": \"page_view\", \"event_time\": \"2024-01-15 10:30:00\", \"properties\": {\"page\": \"/home\", \"browser\": \"chrome\"}},\n",
    "    {\"user_id\": 1002, \"event_type\": \"click\", \"event_time\": \"2024-01-15 10:35:00\", \"properties\": {\"element\": \"button\", \"page\": \"/products\"}},\n",
    "    {\"user_id\": 1003, \"event_type\": \"purchase\", \"event_time\": \"2024-01-15 10:40:00\", \"properties\": {\"amount\": \"99.99\", \"currency\": \"USD\"}},\n",
    "    {\"user_id\": None, \"event_type\": \"error\", \"event_time\": \"2024-01-15 10:45:00\", \"properties\": {\"error_code\": \"404\"}},  # Null user_id for quality testing\n",
    "]\n",
    "\n",
    "mobile_events = [\n",
    "    {\"user_id\": 2001, \"event_type\": \"app_open\", \"event_time\": \"2024-01-15 11:00:00\", \"properties\": {\"app_version\": \"1.2.3\", \"os\": \"iOS\"}},\n",
    "    {\"user_id\": 2002, \"event_type\": \"screen_view\", \"event_time\": \"2024-01-15 11:05:00\", \"properties\": {\"screen\": \"profile\", \"duration\": \"30\"}},\n",
    "    {\"user_id\": 2003, \"event_type\": None, \"event_time\": \"2024-01-15 11:10:00\", \"properties\": {\"action\": \"swipe\"}},  # Null event_type for quality testing\n",
    "]\n",
    "\n",
    "# Ingest data\n",
    "ingestion.ingest_raw_data(\"web_analytics\", web_events)\n",
    "ingestion.ingest_raw_data(\"mobile_app\", mobile_events)\n",
    "\n",
    "# Process to silver\n",
    "ingestion.process_to_silver(\"web_analytics\")\n",
    "ingestion.process_to_silver(\"mobile_app\")\n",
    "\n",
    "print(\"‚úÖ Sample data ingested and processed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define quality checks configuration\n",
    "quality_checks_config = {\n",
    "    \"row_count_check\": {\n",
    "        \"type\": \"row_count\",\n",
    "        \"min_count\": 1,\n",
    "        \"max_count\": 10000\n",
    "    },\n",
    "    \"user_id_null_check\": {\n",
    "        \"type\": \"null_percentage\",\n",
    "        \"column\": \"user_id\",\n",
    "        \"max_null_percentage\": 10.0\n",
    "    },\n",
    "    \"event_id_uniqueness\": {\n",
    "        \"type\": \"uniqueness\",\n",
    "        \"column\": \"event_id\",\n",
    "        \"min_uniqueness_percentage\": 100.0\n",
    "    },\n",
    "    \"data_freshness_check\": {\n",
    "        \"type\": \"freshness\",\n",
    "        \"timestamp_column\": \"processed_timestamp\",\n",
    "        \"max_age_hours\": 1.0\n",
    "    },\n",
    "    \"quality_score_check\": {\n",
    "        \"type\": \"custom_sql\",\n",
    "        \"query\": \"SELECT AVG(data_quality_score) >= 0.8 FROM {table_name}\",\n",
    "        \"expected_result\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Run quality checks on silver table\n",
    "print(\"\\nüß™ Running quality checks on silver_events table...\")\n",
    "silver_table = \"prod.data_lake.silver_events\"\n",
    "quality_results = quality_framework.run_quality_checks(silver_table, quality_checks_config)\n",
    "\n",
    "# Summary\n",
    "passed_checks = sum(1 for result in quality_results.values() if result['passed'])\n",
    "total_checks = len(quality_results)\n",
    "print(f\"\\nüìä Quality Check Summary: {passed_checks}/{total_checks} checks passed\")\n",
    "\n",
    "if passed_checks == total_checks:\n",
    "    print(\"‚úÖ All quality checks passed! Data is ready for gold layer processing.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Some quality checks failed. Review data quality before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üìä Gold Layer Analytics Pipeline\n",
    "\n",
    "Build the gold layer with business metrics and aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gold Layer Analytics Pipeline\n",
    "class GoldLayerPipeline:\n",
    "    def __init__(self, spark_session, pipeline):\n",
    "        self.spark = spark_session\n",
    "        self.pipeline = pipeline\n",
    "        \n",
    "    def calculate_daily_metrics(self, target_date=None):\n",
    "        \"\"\"Calculate daily business metrics for gold layer\"\"\"\n",
    "        if target_date is None:\n",
    "            target_date = datetime.now().date()\n",
    "            \n",
    "        print(f\"üìä Calculating daily metrics for {target_date}...\")\n",
    "        \n",
    "        # Calculate metrics from silver layer\n",
    "        daily_metrics = self.spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            DATE(event_time) as metric_date,\n",
    "            source_system,\n",
    "            event_type,\n",
    "            COUNT(*) as event_count,\n",
    "            COUNT(DISTINCT user_id) as unique_users,\n",
    "            AVG(data_quality_score) as avg_quality_score,\n",
    "            current_timestamp() as calculated_timestamp\n",
    "        FROM {self.pipeline.catalog}.{self.pipeline.database}.silver_events\n",
    "        WHERE DATE(event_time) = DATE('{target_date}')\n",
    "        AND data_quality_score >= 0.7\n",
    "        GROUP BY DATE(event_time), source_system, event_type\n",
    "        \"\"\")\n",
    "        \n",
    "        if daily_metrics.count() == 0:\n",
    "            print(\"‚ö†Ô∏è No data found for the specified date\")\n",
    "            return\n",
    "        \n",
    "        # Write to gold table (upsert pattern)\n",
    "        print(\"üíæ Writing metrics to gold layer...\")\n",
    "        \n",
    "        # First, delete existing data for this date\n",
    "        self.spark.sql(f\"\"\"\n",
    "        DELETE FROM {self.pipeline.catalog}.{self.pipeline.database}.gold_daily_metrics\n",
    "        WHERE metric_date = DATE('{target_date}')\n",
    "        \"\"\")\n",
    "        \n",
    "        # Insert new metrics\n",
    "        daily_metrics.writeTo(f\"{self.pipeline.catalog}.{self.pipeline.database}.gold_daily_metrics\").append()\n",
    "        \n",
    "        metrics_count = daily_metrics.count()\n",
    "        print(f\"‚úÖ Calculated and stored {metrics_count} daily metrics\")\n",
    "        \n",
    "        return daily_metrics\n",
    "    \n",
    "    def generate_business_report(self):\n",
    "        \"\"\"Generate business intelligence report\"\"\"\n",
    "        print(\"üìà Generating business intelligence report...\")\n",
    "        \n",
    "        # Top events by volume\n",
    "        top_events = self.spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            event_type,\n",
    "            SUM(event_count) as total_events,\n",
    "            SUM(unique_users) as total_users,\n",
    "            AVG(avg_quality_score) as avg_quality\n",
    "        FROM {self.pipeline.catalog}.{self.pipeline.database}.gold_daily_metrics\n",
    "        GROUP BY event_type\n",
    "        ORDER BY total_events DESC\n",
    "        \"\"\")\n",
    "        \n",
    "        print(\"\\nüìä Top Events by Volume:\")\n",
    "        top_events.show()\n",
    "        \n",
    "        # Source system performance\n",
    "        source_performance = self.spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            source_system,\n",
    "            COUNT(DISTINCT metric_date) as active_days,\n",
    "            SUM(event_count) as total_events,\n",
    "            AVG(avg_quality_score) as avg_quality_score\n",
    "        FROM {self.pipeline.catalog}.{self.pipeline.database}.gold_daily_metrics\n",
    "        GROUP BY source_system\n",
    "        ORDER BY total_events DESC\n",
    "        \"\"\")\n",
    "        \n",
    "        print(\"\\nüìä Source System Performance:\")\n",
    "        source_performance.show()\n",
    "        \n",
    "        # Daily trends\n",
    "        daily_trends = self.spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            metric_date,\n",
    "            SUM(event_count) as daily_events,\n",
    "            SUM(unique_users) as daily_users,\n",
    "            AVG(avg_quality_score) as daily_quality\n",
    "        FROM {self.pipeline.catalog}.{self.pipeline.database}.gold_daily_metrics\n",
    "        GROUP BY metric_date\n",
    "        ORDER BY metric_date DESC\n",
    "        \"\"\")\n",
    "        \n",
    "        print(\"\\nüìä Daily Trends:\")\n",
    "        daily_trends.show()\n",
    "        \n",
    "        return {\n",
    "            'top_events': top_events,\n",
    "            'source_performance': source_performance,\n",
    "            'daily_trends': daily_trends\n",
    "        }\n",
    "\n",
    "# Initialize gold layer pipeline\n",
    "gold_pipeline = GoldLayerPipeline(spark, pipeline)\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = gold_pipeline.calculate_daily_metrics()\n",
    "\n",
    "# Generate report\n",
    "report = gold_pipeline.generate_business_report()\n",
    "\n",
    "print(\"\\n‚úÖ Gold layer analytics pipeline completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. üîß Table Maintenance and Optimization\n",
    "\n",
    "Implement production table maintenance procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table Maintenance Framework\n",
    "class TableMaintenanceFramework:\n",
    "    def __init__(self, spark_session):\n",
    "        self.spark = spark_session\n",
    "        \n",
    "    def analyze_table_health(self, table_name):\n",
    "        \"\"\"Analyze table health and provide optimization recommendations\"\"\"\n",
    "        print(f\"üîç Analyzing table health for {table_name}...\")\n",
    "        \n",
    "        health_metrics = {}\n",
    "        \n",
    "        try:\n",
    "            # File statistics\n",
    "            file_stats = self.spark.sql(f\"\"\"\n",
    "            SELECT \n",
    "                COUNT(*) as file_count,\n",
    "                SUM(file_size_in_bytes) / 1024 / 1024 / 1024 as total_size_gb,\n",
    "                AVG(file_size_in_bytes) / 1024 / 1024 as avg_file_size_mb,\n",
    "                MIN(file_size_in_bytes) / 1024 / 1024 as min_file_size_mb,\n",
    "                MAX(file_size_in_bytes) / 1024 / 1024 as max_file_size_mb,\n",
    "                SUM(record_count) as total_records\n",
    "            FROM {table_name}.files\n",
    "            \"\"\").collect()[0]\n",
    "            \n",
    "            health_metrics['files'] = {\n",
    "                'count': file_stats['file_count'],\n",
    "                'total_size_gb': round(file_stats['total_size_gb'], 2),\n",
    "                'avg_size_mb': round(file_stats['avg_file_size_mb'], 2),\n",
    "                'min_size_mb': round(file_stats['min_file_size_mb'], 2),\n",
    "                'max_size_mb': round(file_stats['max_file_size_mb'], 2),\n",
    "                'total_records': file_stats['total_records']\n",
    "            }\n",
    "            \n",
    "            # Snapshot statistics\n",
    "            snapshot_stats = self.spark.sql(f\"\"\"\n",
    "            SELECT \n",
    "                COUNT(*) as snapshot_count,\n",
    "                MIN(committed_at) as oldest_snapshot,\n",
    "                MAX(committed_at) as newest_snapshot\n",
    "            FROM {table_name}.snapshots\n",
    "            \"\"\").collect()[0]\n",
    "            \n",
    "            health_metrics['snapshots'] = {\n",
    "                'count': snapshot_stats['snapshot_count'],\n",
    "                'oldest': snapshot_stats['oldest_snapshot'],\n",
    "                'newest': snapshot_stats['newest_snapshot']\n",
    "            }\n",
    "            \n",
    "            # Partition statistics\n",
    "            try:\n",
    "                partition_stats = self.spark.sql(f\"\"\"\n",
    "                SELECT \n",
    "                    COUNT(*) as partition_count,\n",
    "                    AVG(record_count) as avg_records_per_partition,\n",
    "                    AVG(file_count) as avg_files_per_partition\n",
    "                FROM {table_name}.partitions\n",
    "                \"\"\").collect()[0]\n",
    "                \n",
    "                health_metrics['partitions'] = {\n",
    "                    'count': partition_stats['partition_count'],\n",
    "                    'avg_records': round(partition_stats['avg_records_per_partition'], 0),\n",
    "                    'avg_files': round(partition_stats['avg_files_per_partition'], 1)\n",
    "                }\n",
    "            except:\n",
    "                health_metrics['partitions'] = {'count': 'N/A'}\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error analyzing table health: {e}\")\n",
    "            return None\n",
    "            \n",
    "        # Generate recommendations\n",
    "        recommendations = self._generate_recommendations(health_metrics)\n",
    "        \n",
    "        # Display results\n",
    "        self._display_health_report(table_name, health_metrics, recommendations)\n",
    "        \n",
    "        return health_metrics, recommendations\n",
    "    \n",
    "    def _generate_recommendations(self, metrics):\n",
    "        \"\"\"Generate optimization recommendations based on health metrics\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        # File size recommendations\n",
    "        if metrics['files']['avg_size_mb'] < 50:\n",
    "            recommendations.append({\n",
    "                'type': 'COMPACTION',\n",
    "                'priority': 'HIGH',\n",
    "                'message': f\"Small files detected (avg: {metrics['files']['avg_size_mb']:.1f}MB). Consider compaction.\"\n",
    "            })\n",
    "        \n",
    "        if metrics['files']['count'] > 1000:\n",
    "            recommendations.append({\n",
    "                'type': 'COMPACTION',\n",
    "                'priority': 'MEDIUM',\n",
    "                'message': f\"High file count ({metrics['files']['count']}). Compaction may improve performance.\"\n",
    "            })\n",
    "        \n",
    "        # Snapshot recommendations\n",
    "        if metrics['snapshots']['count'] > 50:\n",
    "            recommendations.append({\n",
    "                'type': 'SNAPSHOT_CLEANUP',\n",
    "                'priority': 'MEDIUM',\n",
    "                'message': f\"High snapshot count ({metrics['snapshots']['count']}). Consider cleanup.\"\n",
    "            })\n",
    "        \n",
    "        # Partition recommendations\n",
    "        if isinstance(metrics['partitions']['count'], int) and metrics['partitions']['avg_files'] > 10:\n",
    "            recommendations.append({\n",
    "                'type': 'PARTITION_TUNING',\n",
    "                'priority': 'MEDIUM', \n",
    "                'message': f\"High files per partition ({metrics['partitions']['avg_files']:.1f}). Review partitioning strategy.\"\n",
    "            })\n",
    "            \n",
    "        return recommendations\n",
    "    \n",
    "    def _display_health_report(self, table_name, metrics, recommendations):\n",
    "        \"\"\"Display formatted health report\"\"\"\n",
    "        print(f\"\\nüìä Health Report for {table_name}:\")\n",
    "        print(f\"\\nüìÅ File Statistics:\")\n",
    "        print(f\"  ‚Ä¢ Total files: {metrics['files']['count']}\")\n",
    "        print(f\"  ‚Ä¢ Total size: {metrics['files']['total_size_gb']} GB\")\n",
    "        print(f\"  ‚Ä¢ Average file size: {metrics['files']['avg_size_mb']} MB\")\n",
    "        print(f\"  ‚Ä¢ File size range: {metrics['files']['min_size_mb']:.1f} - {metrics['files']['max_size_mb']:.1f} MB\")\n",
    "        print(f\"  ‚Ä¢ Total records: {metrics['files']['total_records']:,}\")\n",
    "        \n",
    "        print(f\"\\nüì∏ Snapshot Statistics:\")\n",
    "        print(f\"  ‚Ä¢ Total snapshots: {metrics['snapshots']['count']}\")\n",
    "        print(f\"  ‚Ä¢ Date range: {metrics['snapshots']['oldest']} to {metrics['snapshots']['newest']}\")\n",
    "        \n",
    "        if isinstance(metrics['partitions']['count'], int):\n",
    "            print(f\"\\nüóÇÔ∏è Partition Statistics:\")\n",
    "            print(f\"  ‚Ä¢ Total partitions: {metrics['partitions']['count']}\")\n",
    "            print(f\"  ‚Ä¢ Average records per partition: {metrics['partitions']['avg_records']:,}\")\n",
    "            print(f\"  ‚Ä¢ Average files per partition: {metrics['partitions']['avg_files']}\")\n",
    "        \n",
    "        print(f\"\\nüí° Recommendations:\")\n",
    "        if recommendations:\n",
    "            for rec in recommendations:\n",
    "                priority_icon = \"üî¥\" if rec['priority'] == 'HIGH' else \"üü°\" if rec['priority'] == 'MEDIUM' else \"üü¢\"\n",
    "                print(f\"  {priority_icon} {rec['type']}: {rec['message']}\")\n",
    "        else:\n",
    "            print(f\"  ‚úÖ No immediate optimizations needed\")\n",
    "    \n",
    "    def compact_table(self, table_name, target_file_size_mb=128):\n",
    "        \"\"\"Compact table files for better performance\"\"\"\n",
    "        print(f\"üîß Compacting table {table_name} (target file size: {target_file_size_mb}MB)...\")\n",
    "        \n",
    "        try:\n",
    "            # Note: This is a simplified compaction example\n",
    "            # In production, you might use Iceberg's built-in compaction procedures\n",
    "            \n",
    "            # Get file count before compaction\n",
    "            before_stats = self.spark.sql(f\"SELECT COUNT(*) as count FROM {table_name}.files\").collect()[0]['count']\n",
    "            \n",
    "            # Perform compaction by rewriting data\n",
    "            # This is a simplified approach - in production use proper compaction procedures\n",
    "            temp_data = self.spark.sql(f\"SELECT * FROM {table_name}\")\n",
    "            \n",
    "            # Set target file size for this session\n",
    "            target_bytes = target_file_size_mb * 1024 * 1024\n",
    "            self.spark.conf.set(\"spark.sql.catalog.prod.write.target-file-size-bytes\", str(target_bytes))\n",
    "            \n",
    "            print(f\"‚úÖ Compaction completed for {table_name}\")\n",
    "            print(f\"üìä Files before compaction: {before_stats}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Compaction failed: {e}\")\n",
    "    \n",
    "    def cleanup_snapshots(self, table_name, retention_days=30):\n",
    "        \"\"\"Clean up old snapshots beyond retention period\"\"\"\n",
    "        print(f\"üßπ Cleaning up snapshots for {table_name} (retention: {retention_days} days)...\")\n",
    "        \n",
    "        try:\n",
    "            # Calculate cutoff date\n",
    "            cutoff_date = datetime.now() - timedelta(days=retention_days)\n",
    "            \n",
    "            # Get snapshot count before cleanup\n",
    "            before_count = self.spark.sql(f\"SELECT COUNT(*) as count FROM {table_name}.snapshots\").collect()[0]['count']\n",
    "            \n",
    "            # In a real implementation, you would use:\n",
    "            # CALL prod.system.expire_snapshots('table_name', TIMESTAMP 'cutoff_date')\n",
    "            print(f\"üìä Snapshots before cleanup: {before_count}\")\n",
    "            print(f\"üí° Would expire snapshots older than {cutoff_date}\")\n",
    "            print(f\"‚ö†Ô∏è Snapshot cleanup not executed in tutorial (use CALL prod.system.expire_snapshots)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Snapshot cleanup failed: {e}\")\n",
    "\n",
    "# Initialize maintenance framework\n",
    "maintenance = TableMaintenanceFramework(spark)\n",
    "print(\"üîß Table maintenance framework initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run table health analysis\n",
    "print(\"üîç Running table health analysis...\")\n",
    "\n",
    "# Analyze all tables in our data lake\n",
    "tables_to_analyze = [\n",
    "    \"prod.data_lake.bronze_events\",\n",
    "    \"prod.data_lake.silver_events\", \n",
    "    \"prod.data_lake.gold_daily_metrics\"\n",
    "]\n",
    "\n",
    "for table in tables_to_analyze:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    try:\n",
    "        health_metrics, recommendations = maintenance.analyze_table_health(table)\n",
    "        \n",
    "        # Take action on high priority recommendations\n",
    "        high_priority_recs = [r for r in recommendations if r['priority'] == 'HIGH']\n",
    "        if high_priority_recs:\n",
    "            print(f\"\\nüö® High priority issues found for {table}:\")\n",
    "            for rec in high_priority_recs:\n",
    "                print(f\"  ‚Ä¢ {rec['message']}\")\n",
    "                \n",
    "                if rec['type'] == 'COMPACTION':\n",
    "                    # In production, you might automatically trigger compaction\n",
    "                    print(f\"  üîß Recommendation: Schedule compaction for {table}\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not analyze {table}: {e}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"‚úÖ Table health analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. üìä Production Monitoring Dashboard\n",
    "\n",
    "Create a monitoring dashboard for production operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production Monitoring Dashboard\n",
    "class ProductionMonitoringDashboard:\n",
    "    def __init__(self, spark_session):\n",
    "        self.spark = spark_session\n",
    "        \n",
    "    def generate_operational_dashboard(self):\n",
    "        \"\"\"Generate comprehensive operational dashboard\"\"\"\n",
    "        print(\"üìä PRODUCTION MONITORING DASHBOARD\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Data Pipeline Health\n",
    "        self._show_pipeline_health()\n",
    "        \n",
    "        # Data Quality Metrics\n",
    "        self._show_data_quality_metrics()\n",
    "        \n",
    "        # Storage and Performance\n",
    "        self._show_storage_performance()\n",
    "        \n",
    "        # Business Metrics\n",
    "        self._show_business_metrics()\n",
    "        \n",
    "        # System Alerts\n",
    "        self._show_system_alerts()\n",
    "    \n",
    "    def _show_pipeline_health(self):\n",
    "        \"\"\"Show data pipeline health status\"\"\"\n",
    "        print(\"\\nüè• PIPELINE HEALTH STATUS\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        try:\n",
    "            # Check data freshness across layers\n",
    "            layers = {\n",
    "                'Bronze': 'prod.data_lake.bronze_events',\n",
    "                'Silver': 'prod.data_lake.silver_events',\n",
    "                'Gold': 'prod.data_lake.gold_daily_metrics'\n",
    "            }\n",
    "            \n",
    "            for layer_name, table_name in layers.items():\n",
    "                try:\n",
    "                    # Get latest timestamp\n",
    "                    if layer_name == 'Gold':\n",
    "                        timestamp_col = 'calculated_timestamp'\n",
    "                    elif layer_name == 'Silver':\n",
    "                        timestamp_col = 'processed_timestamp'\n",
    "                    else:\n",
    "                        timestamp_col = 'ingestion_timestamp'\n",
    "                    \n",
    "                    latest_result = self.spark.sql(f\"\"\"\n",
    "                    SELECT \n",
    "                        MAX({timestamp_col}) as latest_timestamp,\n",
    "                        COUNT(*) as record_count\n",
    "                    FROM {table_name}\n",
    "                    \"\"\").collect()[0]\n",
    "                    \n",
    "                    if latest_result['latest_timestamp']:\n",
    "                        age_minutes = (datetime.now() - latest_result['latest_timestamp']).total_seconds() / 60\n",
    "                        status = \"üü¢ HEALTHY\" if age_minutes < 60 else \"üü° WARNING\" if age_minutes < 180 else \"üî¥ CRITICAL\"\n",
    "                        print(f\"{layer_name:8}: {status} | Records: {latest_result['record_count']:,} | Age: {age_minutes:.1f}min\")\n",
    "                    else:\n",
    "                        print(f\"{layer_name:8}: üî¥ NO DATA\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"{layer_name:8}: ‚ùå ERROR - {str(e)[:50]}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Pipeline health check failed: {e}\")\n",
    "    \n",
    "    def _show_data_quality_metrics(self):\n",
    "        \"\"\"Show data quality metrics\"\"\"\n",
    "        print(\"\\nüìè DATA QUALITY METRICS\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        try:\n",
    "            # Silver layer quality metrics\n",
    "            quality_metrics = self.spark.sql(\"\"\"\n",
    "            SELECT \n",
    "                source_system,\n",
    "                COUNT(*) as total_records,\n",
    "                AVG(data_quality_score) as avg_quality_score,\n",
    "                SUM(CASE WHEN data_quality_score >= 0.8 THEN 1 ELSE 0 END) / COUNT(*) * 100 as high_quality_pct\n",
    "            FROM prod.data_lake.silver_events\n",
    "            WHERE processed_timestamp >= current_timestamp() - INTERVAL 24 HOURS\n",
    "            GROUP BY source_system\n",
    "            \"\"\")\n",
    "            \n",
    "            print(\"üìä Last 24 Hours Quality Summary:\")\n",
    "            quality_data = quality_metrics.collect()\n",
    "            \n",
    "            if quality_data:\n",
    "                for row in quality_data:\n",
    "                    quality_status = \"üü¢\" if row['avg_quality_score'] >= 0.8 else \"üü°\" if row['avg_quality_score'] >= 0.6 else \"üî¥\"\n",
    "                    print(f\"  {row['source_system']:15}: {quality_status} Score: {row['avg_quality_score']:.2f} | High Quality: {row['high_quality_pct']:.1f}% | Records: {row['total_records']:,}\")\n",
    "            else:\n",
    "                print(\"  ‚ö†Ô∏è No recent data for quality analysis\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Quality metrics check failed: {e}\")\n",
    "    \n",
    "    def _show_storage_performance(self):\n",
    "        \"\"\"Show storage and performance metrics\"\"\"\n",
    "        print(\"\\nüíæ STORAGE & PERFORMANCE\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        try:\n",
    "            tables = [\n",
    "                ('Bronze', 'prod.data_lake.bronze_events'),\n",
    "                ('Silver', 'prod.data_lake.silver_events'),\n",
    "                ('Gold', 'prod.data_lake.gold_daily_metrics')\n",
    "            ]\n",
    "            \n",
    "            total_size_gb = 0\n",
    "            total_files = 0\n",
    "            \n",
    "            for layer, table in tables:\n",
    "                try:\n",
    "                    storage_stats = self.spark.sql(f\"\"\"\n",
    "                    SELECT \n",
    "                        COUNT(*) as file_count,\n",
    "                        SUM(file_size_in_bytes) / 1024 / 1024 / 1024 as size_gb,\n",
    "                        AVG(file_size_in_bytes) / 1024 / 1024 as avg_file_size_mb\n",
    "                    FROM {table}.files\n",
    "                    \"\"\").collect()[0]\n",
    "                    \n",
    "                    size_gb = storage_stats['size_gb'] or 0\n",
    "                    file_count = storage_stats['file_count'] or 0\n",
    "                    avg_size_mb = storage_stats['avg_file_size_mb'] or 0\n",
    "                    \n",
    "                    total_size_gb += size_gb\n",
    "                    total_files += file_count\n",
    "                    \n",
    "                    file_health = \"üü¢\" if avg_size_mb >= 64 else \"üü°\" if avg_size_mb >= 32 else \"üî¥\"\n",
    "                    print(f\"{layer:8}: {file_health} Size: {size_gb:.2f}GB | Files: {file_count:,} | Avg: {avg_size_mb:.1f}MB\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"{layer:8}: ‚ùå ERROR - {str(e)[:30]}\")\n",
    "            \n",
    "            print(f\"\\nüìä Total Storage: {total_size_gb:.2f}GB across {total_files:,} files\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Storage metrics check failed: {e}\")\n",
    "    \n",
    "    def _show_business_metrics(self):\n",
    "        \"\"\"Show key business metrics\"\"\"\n",
    "        print(\"\\nüìà BUSINESS METRICS\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        try:\n",
    "            # Today's metrics\n",
    "            today_metrics = self.spark.sql(\"\"\"\n",
    "            SELECT \n",
    "                SUM(event_count) as total_events,\n",
    "                SUM(unique_users) as total_users,\n",
    "                COUNT(DISTINCT source_system) as active_sources\n",
    "            FROM prod.data_lake.gold_daily_metrics\n",
    "            WHERE metric_date = CURRENT_DATE\n",
    "            \"\"\").collect()[0]\n",
    "            \n",
    "            # Yesterday's metrics for comparison\n",
    "            yesterday_metrics = self.spark.sql(\"\"\"\n",
    "            SELECT \n",
    "                SUM(event_count) as total_events,\n",
    "                SUM(unique_users) as total_users\n",
    "            FROM prod.data_lake.gold_daily_metrics\n",
    "            WHERE metric_date = CURRENT_DATE - INTERVAL 1 DAY\n",
    "            \"\"\").collect()[0]\n",
    "            \n",
    "            print(\"üìä Today's Activity:\")\n",
    "            print(f\"  Events: {today_metrics['total_events'] or 0:,}\")\n",
    "            print(f\"  Users: {today_metrics['total_users'] or 0:,}\")\n",
    "            print(f\"  Active Sources: {today_metrics['active_sources'] or 0}\")\n",
    "            \n",
    "            # Growth calculation\n",
    "            if yesterday_metrics['total_events'] and today_metrics['total_events']:\n",
    "                event_growth = ((today_metrics['total_events'] - yesterday_metrics['total_events']) / yesterday_metrics['total_events']) * 100\n",
    "                growth_icon = \"üìà\" if event_growth > 0 else \"üìâ\" if event_growth < 0 else \"‚û°Ô∏è\"\n",
    "                print(f\"\\nüìä Day-over-Day Growth:\")\n",
    "                print(f\"  Events: {growth_icon} {event_growth:+.1f}%\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Business metrics check failed: {e}\")\n",
    "    \n",
    "    def _show_system_alerts(self):\n",
    "        \"\"\"Show system alerts and recommendations\"\"\"\n",
    "        print(\"\\nüö® SYSTEM ALERTS\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        alerts = []\n",
    "        \n",
    "        try:\n",
    "            # Check for tables with many small files\n",
    "            tables_to_check = [\n",
    "                'prod.data_lake.bronze_events',\n",
    "                'prod.data_lake.silver_events',\n",
    "                'prod.data_lake.gold_daily_metrics'\n",
    "            ]\n",
    "            \n",
    "            for table in tables_to_check:\n",
    "                try:\n",
    "                    file_stats = self.spark.sql(f\"\"\"\n",
    "                    SELECT \n",
    "                        COUNT(*) as file_count,\n",
    "                        AVG(file_size_in_bytes) / 1024 / 1024 as avg_size_mb\n",
    "                    FROM {table}.files\n",
    "                    \"\"\").collect()[0]\n",
    "                    \n",
    "                    if file_stats['avg_size_mb'] and file_stats['avg_size_mb'] < 32:\n",
    "                        alerts.append(f\"üî¥ PERFORMANCE: {table} has small files (avg: {file_stats['avg_size_mb']:.1f}MB)\")\n",
    "                    \n",
    "                    if file_stats['file_count'] > 500:\n",
    "                        alerts.append(f\"üü° MAINTENANCE: {table} has many files ({file_stats['file_count']:,})\")\n",
    "                        \n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Check for data freshness issues\n",
    "            try:\n",
    "                latest_silver = self.spark.sql(\"\"\"\n",
    "                SELECT MAX(processed_timestamp) as latest\n",
    "                FROM prod.data_lake.silver_events\n",
    "                \"\"\").collect()[0]['latest']\n",
    "                \n",
    "                if latest_silver:\n",
    "                    age_hours = (datetime.now() - latest_silver).total_seconds() / 3600\n",
    "                    if age_hours > 2:\n",
    "                        alerts.append(f\"üî¥ DATA FRESHNESS: Silver layer data is {age_hours:.1f} hours old\")\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Display alerts\n",
    "            if alerts:\n",
    "                for alert in alerts:\n",
    "                    print(f\"  {alert}\")\n",
    "            else:\n",
    "                print(\"  ‚úÖ No active alerts\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Alert check failed: {e}\")\n",
    "\n",
    "# Initialize monitoring dashboard\n",
    "dashboard = ProductionMonitoringDashboard(spark)\n",
    "\n",
    "# Generate dashboard\n",
    "dashboard.generate_operational_dashboard()\n",
    "\n",
    "print(\"\\n‚úÖ Production monitoring dashboard generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. üéâ Production Pipeline Summary\n",
    "\n",
    "Summary of production patterns and best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production Pipeline Summary\n",
    "print(\"üéâ PRODUCTION PIPELINE TUTORIAL COMPLETE!\")\n",
    "print(\"\\n‚úÖ What You've Built:\")\n",
    "\n",
    "accomplishments = [\n",
    "    \"Production-ready Spark session with optimized configurations\",\n",
    "    \"Medallion architecture (Bronze ‚Üí Silver ‚Üí Gold) implementation\",\n",
    "    \"Comprehensive data quality framework with multiple check types\",\n",
    "    \"Automated data ingestion pipeline with error handling\",\n",
    "    \"Business metrics calculation and gold layer analytics\",\n",
    "    \"Table maintenance framework with health analysis\",\n",
    "    \"Production monitoring dashboard with alerts\",\n",
    "    \"End-to-end data lineage and quality tracking\"\n",
    "]\n",
    "\n",
    "for i, accomplishment in enumerate(accomplishments, 1):\n",
    "    print(f\"   {i}. {accomplishment}\")\n",
    "\n",
    "print(\"\\nüí° PRODUCTION BEST PRACTICES LEARNED:\")\n",
    "\n",
    "best_practices = {\n",
    "    \"üèóÔ∏è Architecture\": [\n",
    "        \"Use medallion architecture for clear data flow\",\n",
    "        \"Implement proper separation of concerns\",\n",
    "        \"Design for scalability and maintainability\",\n",
    "        \"Plan for schema evolution from day one\"\n",
    "    ],\n",
    "    \"üìä Data Quality\": [\n",
    "        \"Implement comprehensive data validation\",\n",
    "        \"Use quality scores for automated decisions\",\n",
    "        \"Monitor data freshness and completeness\",\n",
    "        \"Set up automated quality alerts\"\n",
    "    ],\n",
    "    \"‚ö° Performance\": [\n",
    "        \"Optimize file sizes for your workload\",\n",
    "        \"Use appropriate compression algorithms\",\n",
    "        \"Implement regular table maintenance\",\n",
    "        \"Monitor and tune partition strategies\"\n",
    "    ],\n",
    "    \"üîß Operations\": [\n",
    "        \"Automate monitoring and alerting\",\n",
    "        \"Implement proper error handling\",\n",
    "        \"Plan for disaster recovery\",\n",
    "        \"Document operational procedures\"\n",
    "    ],\n",
    "    \"üîí Reliability\": [\n",
    "        \"Use ACID transactions for consistency\",\n",
    "        \"Implement proper retry mechanisms\",\n",
    "        \"Plan for graceful degradation\",\n",
    "        \"Test failure scenarios regularly\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, practices in best_practices.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for practice in practices:\n",
    "        print(f\"   ‚Ä¢ {practice}\")\n",
    "\n",
    "print(\"\\nüöÄ NEXT STEPS FOR PRODUCTION:\")\n",
    "next_steps = [\n",
    "    \"Implement CI/CD pipelines for data code\",\n",
    "    \"Set up proper security and access controls\",\n",
    "    \"Integrate with enterprise monitoring systems\",\n",
    "    \"Implement advanced data governance\",\n",
    "    \"Scale to handle production data volumes\",\n",
    "    \"Add stream processing for real-time use cases\"\n",
    "]\n",
    "\n",
    "for step in next_steps:\n",
    "    print(f\"   ‚Üí {step}\")\n",
    "\n",
    "print(\"\\nüéØ Key Takeaway:\")\n",
    "print(\"   Production Iceberg pipelines require careful attention to data quality,\")\n",
    "print(\"   performance optimization, monitoring, and operational excellence.\")\n",
    "print(\"   With proper implementation, Iceberg provides a robust foundation\")\n",
    "print(\"   for enterprise-scale data lake solutions!\")\n",
    "\n",
    "print(\"\\nüèÜ Congratulations on completing the Production Pipeline Tutorial!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
