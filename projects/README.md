# Projects Overview

This directory contains all hands-on projects for learning Apache Iceberg. Each project corresponds to a key learning phase and is designed to reinforce both theoretical knowledge and practical skills.

## Table of Contents

- [Phase 1: Core Concepts Mastery](#phase-1-core-concepts-mastery)
- [Phase 2: Hands-on Practice](#phase-2-hands-on-practice)
- [Phase 3: Architecture Deep Dive](#phase-3-architecture-deep-dive)
- [Phase 4: Production Applications](#phase-4-production-applications)
- [Project Workflow & Best Practices](#project-workflow--best-practices)

---

## Phase 1: Core Concepts Mastery
**Goal:** Understand the value, architecture, and basic features of Apache Iceberg.

**Project:** `01-core-concepts/`
- Explore open table format concepts
- Compare Iceberg with traditional Hive tables
- Study the evolution from data lakes to data warehouses
- Deep dive into core features:
  - Schema Evolution
  - Hidden Partitioning
  - Time Travel
  - ACID Transactions
  - Multi-engine Support

**Deliverables:**
- Summary notes (notebook or markdown)
- Key concept mindmap or diagram
- Q&A or FAQ for this phase

**Self-Check:**
- Can you explain Iceberg's core value and use cases?
- Can you describe the differences between Iceberg and Hive tables?

---

## Phase 2: Hands-on Practice
**Goal:** Set up and run Iceberg in a local environment, and master basic operations.

**Project:** `02-hands-on-practice/`
- Environment setup (Docker, Java, Spark)
- Spark + Iceberg integration
- Create your first Iceberg table
- Insert and query test data
- Practice schema evolution (add/modify/remove columns)
- Explore time travel features (snapshots, rollback)

**Deliverables:**
- Jupyter notebooks with step-by-step operations
- Screenshots or outputs of successful table creation and queries
- Troubleshooting notes for any issues encountered

**Self-Check:**
- Can you create and query an Iceberg table from scratch?
- Can you perform schema evolution and time travel queries?

---

## Phase 3: Architecture Deep Dive
**Goal:** Gain a deep understanding of Iceberg's internal architecture and advanced features.

**Project:** `03-architecture-deep-dive/`
- Analyze core modules: iceberg-api, iceberg-core, iceberg-data
- Study storage format support: Parquet, ORC
- Explore Java API for table operations
- Implement custom data read/write logic
- Investigate metadata management mechanisms
- Experiment with performance optimization:
  - File size tuning
  - Partition design
  - Compression algorithms
  - Query optimization

**Deliverables:**
- Technical analysis notes (notebook or markdown)
- Example code for Java API usage
- Performance test results and summary

**Self-Check:**
- Can you explain the role of each core module?
- Can you use the Java API for advanced table operations?
- Can you identify and apply performance optimization strategies?

---

## Phase 4: Production Applications
**Goal:** Master deployment, integration, and operations in production environments.

**Project:** `04-production-applications/`
- Cloud platform integration (AWS, Azure, GCP)
- S3/Glue/ADLS configuration
- Multi-engine support (Spark, Flink, Trino)
- Table maintenance (compaction, snapshot cleanup, stats update)
- Monitoring and debugging (metrics, logs, troubleshooting)

**Deliverables:**
- Cloud deployment scripts/configs
- Maintenance and monitoring guides
- Case studies or real-world integration examples

**Self-Check:**
- Can you deploy and operate Iceberg in a cloud environment?
- Can you perform table maintenance and monitor performance?
- Can you troubleshoot and resolve common production issues?

---

## Project Workflow & Best Practices

1. **Read the project README** to understand objectives and requirements.
2. **Prepare the environment** using provided scripts/configs.
3. **Follow the step-by-step notebooks or scripts** for each task.
4. **Document problems and solutions** in each project.
5. **Summarize key learnings** at the end of each phase.
6. **Review self-check questions** to ensure mastery before moving to the next phase.

---

This structure ensures a progressive, project-driven learning experience. Each phase builds on the previous one, helping you grow from beginner to production-ready Iceberg practitioner.

## ğŸ“‚ Project Structure

```
projects/
â”œâ”€â”€ 01-first-iceberg-table/         # âœ… Project 1: First Iceberg Table (Completed)
â”‚   â”œâ”€â”€ docker-compose.yml          # Docker environment configuration
â”‚   â”œâ”€â”€ manage.sh                   # Environment management script
â”‚   â”œâ”€â”€ notebooks/                  # Jupyter tutorials and examples
â”‚   â”‚   â””â”€â”€ iceberg-tutorial.ipynb  # Complete Iceberg tutorial
â”‚   â”œâ”€â”€ scripts/                    # Test scripts
â”‚   â”œâ”€â”€ warehouse/                  # Iceberg table storage location
â”‚   â”œâ”€â”€ README.md                   # Detailed project documentation
â”‚   â””â”€â”€ SPARK_VS_ICEBERG.md        # Spark vs Iceberg relationship documentation
â”œâ”€â”€ 02-schema-evolution/            # âœ… Project 2: Schema Evolution Experiments (Completed)
â”‚   â”œâ”€â”€ docker-compose.yml          # Docker environment configuration
â”‚   â”œâ”€â”€ manage.sh                   # Environment management script
â”‚   â”œâ”€â”€ notebooks/                  # Schema evolution tutorials
â”‚   â”‚   â””â”€â”€ schema-evolution-tutorial.ipynb  # Complete schema evolution tutorial
â”‚   â”œâ”€â”€ scripts/                    # Demo and validation scripts
â”‚   â”‚   â”œâ”€â”€ schema-evolution-demo.py         # Interactive demo script
â”‚   â”‚   â””â”€â”€ validation-tests.py             # Comprehensive validation tests
â”‚   â”œâ”€â”€ test-data/                  # Sample data for testing
â”‚   â”‚   â”œâ”€â”€ users-v1.json           # Initial user data
â”‚   â”‚   â””â”€â”€ users-v2.json           # Evolved user data
â”‚   â”œâ”€â”€ warehouse/                  # Iceberg table storage location
â”‚   â””â”€â”€ README.md                   # Detailed project documentation
â”œâ”€â”€ 03-time-travel/                 # Project 3: Time Travel Features (Planned)
â”‚   â”œâ”€â”€ demo/                       # Demo code
â”‚   â””â”€â”€ README.md                   # Project documentation
â”œâ”€â”€ 04-cloud-integration/           # Project 4: Cloud Platform Integration (Planned)
â”‚   â”œâ”€â”€ aws/                        # AWS configuration
â”‚   â”œâ”€â”€ azure/                      # Azure configuration
â”‚   â””â”€â”€ README.md                   # Project documentation
â””â”€â”€ 05-production-pipeline/         # Project 5: Production Data Pipeline (Planned)
    â”œâ”€â”€ spark-jobs/                 # Spark jobs
    â”œâ”€â”€ monitoring/                 # Monitoring configuration
    â””â”€â”€ README.md                   # Project documentation
```

## ğŸ¯ Project Completion Status

### âœ… Project 1: First Iceberg Table (Completed)
- **Learning Goal**: Master basic Iceberg table creation and operations âœ…
- **Tech Stack**: Docker, Spark, Iceberg, Jupyter Notebook âœ…
- **Completion Time**: Completed
- **Main Achievements**:
  - âœ… Built complete Docker development environment
  - âœ… Successfully integrated Spark 3.5.0 + Iceberg 1.4.3
  - âœ… Created interactive Jupyter tutorial
  - âœ… Implemented data insertion, querying and metadata exploration
  - âœ… Resolved Python version conflicts and permission issues
  - âœ… Verified partitioning and ACID transaction features

### âœ… Project 2: Schema Evolution Experiments (Completed)
- **Learning Goal**: Master Iceberg's schema evolution capabilities âœ…
- **Tech Stack**: Docker, Spark SQL, Schema management, Jupyter Notebook âœ…
- **Completion Time**: Completed
- **Main Achievements**:
  - âœ… Built comprehensive schema evolution environment
  - âœ… Demonstrated safe column addition and type promotions
  - âœ… Implemented complex nested data structures (maps, arrays, structs)
  - âœ… Created interactive Jupyter tutorial with real-world scenarios
  - âœ… Developed validation framework for schema evolution testing
  - âœ… Verified backward and forward compatibility features
  - âœ… Performance analysis of schema changes

### ğŸ”„ Project 3: Time Travel Features (Planned)
- **Learning Goal**: Master snapshot management and time travel queries
- **Tech Stack**: Spark SQL, Snapshot API
- **Estimated Time**: 2 days

### ğŸ”„ Project 4: Cloud Platform Integration (Planned)
- **Learning Goal**: Deploy and use Iceberg in cloud environments
- **Tech Stack**: AWS S3/Glue, or Azure ADLS
- **Estimated Time**: 3-5 days

### ğŸ”„ Project 5: Production Data Pipeline (Planned)
- **Learning Goal**: Build complete production-grade data pipeline
- **Tech Stack**: Spark, Cloud services, Monitoring
- **Estimated Time**: 1-2 weeks

## ğŸ“‹ Project Completion Results

### Project 1: First Iceberg Table âœ…

### ğŸ› ï¸ Technical Environment
- **Dockerized Deployment**: One-click Spark + Iceberg environment
- **Jupyter Integration**: Interactive learning and experimentation platform
- **Permission Management**: Resolved container permissions and Python version issues

### ğŸ“Š Implemented Features
- **Table Creation**: Create partitioned tables using `USING ICEBERG` syntax
- **Data Operations**: Successfully inserted 8 user event records
- **Partitioning Strategy**: Automatic partitioning by date (`days(event_time)`)
- **Metadata Queries**:
  - `.snapshots` - View table snapshot history
  - `.files` - View physical file distribution
  - `.history` - View table change history

### ğŸ¯ Verification Results
- âœ… Created `local.demo.user_events` table
- âœ… Data correctly partitioned to 2024-01-15 and 2024-01-16
- âœ… Generated 2 Parquet files (6+2 records)
- âœ… All metadata queries working normally
- âœ… ACID transaction features verified

### Project 2: Schema Evolution Experiments âœ…

### ğŸ› ï¸ Technical Environment
- **Dockerized Deployment**: Dedicated schema evolution environment (port 8889)
- **Jupyter Integration**: Interactive schema evolution tutorial and experiments
- **Validation Framework**: Comprehensive testing and validation scripts

### ğŸ“Š Implemented Features
- **Column Operations**: Safe addition, removal, and renaming of columns
- **Type Promotions**: intâ†’bigint, floatâ†’double, decimal precision increases
- **Complex Types**: Maps, arrays, structs with nested data structures
- **Real-World Scenarios**: E-commerce, IoT, analytics table evolution patterns
- **Compatibility Testing**: Backward and forward compatibility validation

### ğŸ¯ Verification Results
- âœ… Created multiple evolved tables (users, metrics, products)
- âœ… Successfully added nullable columns without data rewriting
- âœ… Performed safe type promotions maintaining data integrity
- âœ… Implemented complex nested structures with maps and arrays
- âœ… Validated historical data accessibility after schema changes
- âœ… Performance analysis showing minimal impact of schema evolution

## ğŸ“‹ Project Completion Checklist

### Project 1 Checklist âœ…
- [x] Code runs successfully
- [x] Detailed README documentation
- [x] Includes execution steps and result screenshots
- [x] Records problems encountered and solutions
- [x] Summarizes key concepts learned
- [x] Complete Docker environment configuration
- [x] Reproducible Jupyter tutorial execution

### Project 2 Checklist âœ…
- [x] Code runs successfully
- [x] Detailed README documentation
- [x] Includes execution steps and result screenshots
- [x] Records problems encountered and solutions
- [x] Summarizes key concepts learned
- [x] Complete Docker environment configuration
- [x] Reproducible Jupyter tutorial execution
- [x] Comprehensive validation tests
- [x] Real-world scenario demonstrations

### Future Projects Checklist
- [ ] Code runs successfully
- [ ] Detailed README documentation
- [ ] Includes execution steps and result screenshots
- [ ] Records problems encountered and solutions
- [ ] Summarizes key concepts learned

## ğŸ”§ Development Environment Requirements

### Basic Environment âœ…
- Java 11/17/21
- Apache Spark 3.5.0 âœ…
- Docker (for local testing) âœ…
- Iceberg 1.4.3 âœ…

### Cloud Environment (Optional)
- AWS Account (S3, Glue, EMR)
- Azure Account (ADLS, Synapse)
- GCP Account (GCS, Dataproc)

## ğŸ“– Learning Process Summary

### Project 1 Learning Gains âœ…
1. **Environment Setup** - Mastered Docker-based Iceberg development environment âœ…
2. **Basic Concepts** - Understood Spark engine + Iceberg table format architecture âœ…
3. **Practical Operations** - Completed table creation, data insertion, query analysis âœ…
4. **Problem Solving** - Handled Python version conflicts, permission configuration issues âœ…
5. **Feature Verification** - Verified partitioning, transactions, metadata core features âœ…

### Future Project Workflow
1. **Read Project Documentation** - Understand goals and requirements
2. **Environment Preparation** - Set up necessary development environment
3. **Code Implementation** - Complete project code step by step
4. **Testing and Verification** - Run and verify results
5. **Documentation** - Update README and learning notes
6. **Problem Summary** - Record problems encountered and solutions

## ğŸ¯ Current Learning Achievements

After completing Project 1, you can now:

- âœ… Independently set up Iceberg development environment
- âœ… Create and manage basic Iceberg tables
- âœ… Use partitioning strategies to optimize data storage
- âœ… Query and analyze Iceberg table metadata
- âœ… Understand Spark and Iceberg integration mechanisms
- âœ… Troubleshoot and resolve common environment issues

After completing Project 2, you can now:

- âœ… Safely evolve table schemas without breaking existing applications
- âœ… Add, remove, and rename columns while maintaining data integrity
- âœ… Perform safe data type promotions for growing data requirements
- âœ… Design complex nested data structures for flexible data modeling
- âœ… Validate schema compatibility across different time periods
- âœ… Analyze performance impact of schema evolution operations

## ğŸš€ Next Steps Recommendations

Based on Projects 1 & 2 successful completion, suggested continuation order:

1. **âœ… Project 1 - Core Concepts**: Master basic Iceberg table operations âœ…
2. **âœ… Project 2 - Schema Evolution**: Master schema evolution capabilities âœ… 
3. **Project 3 - Time Travel**: Demonstrate time travel queries and snapshot management
4. **Project 4 - Cloud Integration**: Migrate environment to cloud platforms
5. **Project 5 - Production Pipeline**: Build complete production data flows

---

**Completed Projects**:
- [01-core-concepts - First Iceberg Table](01-core-concepts/README.md) âœ…
- [02-schema-evolution - Schema Evolution Experiments](02-schema-evolution/README.md) âœ…

**Start Next Project**: Time Travel Features (Coming Soon)